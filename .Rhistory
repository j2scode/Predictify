createTestNGrams(directories)
# Initialize MKN language mkn
mknInit(mkn, training$nGrams, regex)
# Create absolute counts of each nGram
features <- parallelizeTask(mknAbsCount, mkn, training$nGrams)
# Create continuation counts of each nGram
parallelizeTask(mknCKN, mkn, mkn$mOrder)
# Count nGram histories
parallelizeTask(mknHistories, mkn, mkn$mOrder)
# Calculate discounts
discounts <- mknDiscount(mkn)
# Calculate pseudo probability alpha
parallelizeTask(mknAlpha, mkn)
# Compute weighting factor lambda
parallelizeTask(mknLambda, mkn)
# Compute probabilities
parallelizeTask(mknEstimate, mkn)
load("~/Data Science/Data Science Projects/PredictifyR-1.0/lm/mkn/epsilon/counts/mkn-bigrams.Rdata")
load("~/Data Science/Data Science Projects/PredictifyR-1.0/lm/mkn/epsilon/counts/mkn-quadgrams.Rdata")
load("~/Data Science/Data Science Projects/PredictifyR-1.0/lm/mkn/epsilon/counts/mkn-trigrams.Rdata")
load("~/Data Science/Data Science Projects/PredictifyR-1.0/lm/mkn/epsilon/counts/mkn-unigrams.Rdata")
source('~/Data Science/Data Science Projects/PredictifyR-1.0/lab/write2Excel.R')
x
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM08.mknEstimate.R')
createTestNGrams(directories)
# Initialize MKN language mkn
mknInit(mkn, training$nGrams, regex)
# Create absolute counts of each nGram
features <- parallelizeTask(mknAbsCount, mkn, training$nGrams)
# Create continuation counts of each nGram
parallelizeTask(mknCKN, mkn, mkn$mOrder)
# Count nGram histories
parallelizeTask(mknHistories, mkn, mkn$mOrder)
# Calculate discounts
discounts <- mknDiscount(mkn)
# Calculate pseudo probability alpha
parallelizeTask(mknAlpha, mkn)
# Compute weighting factor lambda
parallelizeTask(mknLambda, mkn)
# Compute probabilities
parallelizeTask(mknEstimate, mkn)
load("~/Data Science/Data Science Projects/PredictifyR-1.0/lm/mkn/epsilon/counts/mkn-bigrams.Rdata")
load("~/Data Science/Data Science Projects/PredictifyR-1.0/lm/mkn/epsilon/counts/mkn-quadgrams.Rdata")
load("~/Data Science/Data Science Projects/PredictifyR-1.0/lm/mkn/epsilon/counts/mkn-trigrams.Rdata")
load("~/Data Science/Data Science Projects/PredictifyR-1.0/lm/mkn/epsilon/counts/mkn-unigrams.Rdata")
source('~/Data Science/Data Science Projects/PredictifyR-1.0/lab/write2Excel.R')
sent <- '<S> this is sentence </s>'
ng <- quanteda::dfm(sent, ngrams = 3, remove_punct = FALSE,
concatenator = ' ', tolower = FALSE)
sent <- '<S> this is sentence </s>'
ng <- quanteda::dfm(sent, ngrams = 3, remove_punct = FALSE,
concatenator = ' ', tolower = FALSE, what = 'fasterword')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/E2.createNGrams.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/E1.processCorpora.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM01.mknInit.R')
x
nGrams <- corpora$training$epsilon$nGrams
nGram <- loadObject(nGrams[[x]])
counts <- data.table(nGram = featnames(nGram), key = 'nGram')
if (x == 1) {
counts <- rbindlist(list(counts, list("<s>")))
}
View(counts)
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM01.mknInit.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/lab/createTestNGrams.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/lab/createTestNGrams.R')
createTestNGrams(directories)
# Initialize MKN language mkn
mknInit(mkn, training$nGrams, regex)
# Create absolute counts of each nGram
features <- parallelizeTask(mknAbsCount, mkn, training$nGrams)
# Create continuation counts of each nGram
parallelizeTask(mknCKN, mkn, mkn$mOrder)
# Count nGram histories
parallelizeTask(mknHistories, mkn, mkn$mOrder)
# Calculate discounts
discounts <- mknDiscount(mkn)
# Calculate pseudo probability alpha
parallelizeTask(mknAlpha, mkn)
# Compute weighting factor lambda
parallelizeTask(mknLambda, mkn)
# Compute probabilities
parallelizeTask(mknEstimate, mkn)
# Publish language mkn
parallelizeTask(mknPublish, mkn, directories)
# Evaluate Model
pp <- mknEvaluate(lm$mkn$epsilon, corpora$training$epsilon,  corpora$validation$epsilon, sents = NULL, directories)
load("~/Data Science/Data Science Projects/PredictifyR-1.0/lm/mkn/epsilon/counts/mkn-bigrams.Rdata")
load("~/Data Science/Data Science Projects/PredictifyR-1.0/lm/mkn/epsilon/counts/mkn-quadgrams.Rdata")
load("~/Data Science/Data Science Projects/PredictifyR-1.0/lm/mkn/epsilon/counts/mkn-trigrams.Rdata")
load("~/Data Science/Data Science Projects/PredictifyR-1.0/lm/mkn/epsilon/counts/mkn-unigrams.Rdata")
source('~/Data Science/Data Science Projects/PredictifyR-1.0/lab/write2Excel.R')
load("~/Data Science/Data Science Projects/PredictifyR-1.0/lm/mkn/epsilon/counts/mkn-quadgrams.Rdata")
View(mknQuadgrams)
load("~/Data Science/Data Science Projects/PredictifyR-1.0/lm/mkn/epsilon/counts/mkn-trigrams.Rdata")
View(mknTrigrams)
View(mknQuadgrams)
source('~/Data Science/Data Science Projects/PredictifyR-1.0/lab/createTestNGrams.R')
createTestNGrams(directories)
load("~/Data Science/Data Science Projects/PredictifyR-1.0/test/mkn/nGrams/ltcorpus4.RData")
training
mknInit(mkn, training$nGrams, regex)
load("~/Data Science/Data Science Projects/PredictifyR-1.0/lm/mkn/epsilon/counts/mkn-quadgrams.Rdata")
lapply(seq_along(nGrams), function(x) {
message(paste('...initializing', mkn$counts[[x]]$fileDesc))
# Initialize with NGrams
nGram <- loadObject(nGrams[[x]])
counts <- data.table(nGram = featnames(nGram), key = 'nGram')
# Add a start-of-sentence token to unigram to store unigram backoff weight
if (x == 1) {
counts <- rbindlist(list(counts, list("<s>")))
}
# Add Context and suffix if n > 1
if (x > 1) {
context <- gsub(regex$context[[x-1]], "\\1", counts$nGram, perl = TRUE)
suffix  <- gsub(regex$suffix[[x-1]], "\\1", counts$nGram, perl = TRUE)
counts[, c('context', 'suffix') := list(context, suffix)]
}
mkn$counts[[x]]$data <- counts
saveObject(mkn$counts[[x]])
})
load("~/Data Science/Data Science Projects/PredictifyR-1.0/lm/mkn/epsilon/counts/mkn-quadgrams.Rdata")
View(mknQuadgrams)
load("~/Data Science/Data Science Projects/PredictifyR-1.0/lm/mkn/epsilon/counts/mkn-bigrams.Rdata")
load("~/Data Science/Data Science Projects/PredictifyR-1.0/lm/mkn/epsilon/counts/mkn-quadgrams.Rdata")
load("~/Data Science/Data Science Projects/PredictifyR-1.0/lm/mkn/epsilon/counts/mkn-trigrams.Rdata")
load("~/Data Science/Data Science Projects/PredictifyR-1.0/lm/mkn/epsilon/counts/mkn-unigrams.Rdata")
source('~/Data Science/Data Science Projects/PredictifyR-1.0/lab/write2Excel.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/lab/write2Excel.R')
View(mknQuadgrams)
features <- parallelizeTask(mknAbsCount, mkn, training$nGrams)
View(mknQuadgrams)
# Create continuation counts of each nGram
parallelizeTask(mknCKN, mkn, mkn$mOrder)
# Count nGram histories
parallelizeTask(mknHistories, mkn, mkn$mOrder)
# Calculate discounts
discounts <- mknDiscount(mkn)
# Calculate pseudo probability alpha
parallelizeTask(mknAlpha, mkn)
# Compute weighting factor lambda
parallelizeTask(mknLambda, mkn)
# Compute probabilities
parallelizeTask(mknEstimate, mkn)
load("~/Data Science/Data Science Projects/PredictifyR-1.0/lm/mkn/epsilon/counts/mkn-bigrams.Rdata")
load("~/Data Science/Data Science Projects/PredictifyR-1.0/lm/mkn/epsilon/counts/mkn-quadgrams.Rdata")
load("~/Data Science/Data Science Projects/PredictifyR-1.0/lm/mkn/epsilon/counts/mkn-trigrams.Rdata")
load("~/Data Science/Data Science Projects/PredictifyR-1.0/lm/mkn/epsilon/counts/mkn-unigrams.Rdata")
createTestNGrams(directories)
# Initialize MKN language mkn
mknInit(mkn, training$nGrams, regex)
# Create absolute counts of each nGram
features <- parallelizeTask(mknAbsCount, mkn, training$nGrams)
load("~/Data Science/Data Science Projects/PredictifyR-1.0/lm/mkn/epsilon/counts/mkn-bigrams.Rdata")
load("~/Data Science/Data Science Projects/PredictifyR-1.0/lm/mkn/epsilon/counts/mkn-quadgrams.Rdata")
load("~/Data Science/Data Science Projects/PredictifyR-1.0/lm/mkn/epsilon/counts/mkn-trigrams.Rdata")
load("~/Data Science/Data Science Projects/PredictifyR-1.0/lm/mkn/epsilon/counts/mkn-unigrams.Rdata")
parallelizeTask(mknCKN, mkn, mkn$mOrder)
parallelizeTask(mknHistories, mkn, mkn$mOrder)
discounts <- mknDiscount(mkn)
parallelizeTask(mknAlpha, mkn)
parallelizeTask(mknLambda, mkn)
load("~/Data Science/Data Science Projects/PredictifyR-1.0/lm/mkn/epsilon/counts/mkn-quadgrams.Rdata")
createTestNGrams(directories)
# Initialize MKN language mkn
mknInit(mkn, training$nGrams, regex)
# Create absolute counts of each nGram
features <- parallelizeTask(mknAbsCount, mkn, training$nGrams)
load("~/Data Science/Data Science Projects/PredictifyR-1.0/lm/mkn/epsilon/counts/mkn-bigrams.Rdata")
load("~/Data Science/Data Science Projects/PredictifyR-1.0/lm/mkn/epsilon/counts/mkn-quadgrams.Rdata")
load("~/Data Science/Data Science Projects/PredictifyR-1.0/lm/mkn/epsilon/counts/mkn-trigrams.Rdata")
load("~/Data Science/Data Science Projects/PredictifyR-1.0/lm/mkn/epsilon/counts/mkn-unigrams.Rdata")
parallelizeTask(mknCKN, mkn, mkn$mOrder)
load("~/Data Science/Data Science Projects/PredictifyR-1.0/lm/mkn/epsilon/counts/mkn-quadgrams.Rdata")
parallelizeTask(mknHistories, mkn, mkn$mOrder)
load("~/Data Science/Data Science Projects/PredictifyR-1.0/lm/mkn/epsilon/counts/mkn-quadgrams.Rdata")
discounts <- mknDiscount(mkn)
load("~/Data Science/Data Science Projects/PredictifyR-1.0/lm/mkn/epsilon/counts/mkn-quadgrams.Rdata")
parallelizeTask(mknAlpha, mkn)
load("~/Data Science/Data Science Projects/PredictifyR-1.0/lm/mkn/epsilon/counts/mkn-quadgrams.Rdata")
x <- 4
contextCounts <- current[, sum(count), by = context]
current <- model[[x]]
View(current)
model <- lapply(seq_along(mkn$counts), function(x) {
loadObject(mkn$counts[[x]])
})
discounts <- loadObject(mkn$discounts)
summary <- loadObject(mkn$summary)
current <- model[[x]]
# Select discounts for nGram Order
dCounts <- t(subset(discounts, nGramOrder == x, select = c(-(1:2))))
# Compute N1count groups based upon continution counts
current[, group := min(3, (cKN)), by=cKN]
# Determine the discount based upon the count group
current[, D := dCounts[group+1]]
# Subtract discount from continuation count
current[, alphaCount := (cKN - D)]
current[alphaCount < 0, alphaCount := 0]
contextCounts <- current[, sum(count), by = context]
View(contextCounts)
contextCounts <- current[, counts := sum(count), by = context]
contextCounts <- current[,. counts := sum(count), by = context]
contextCounts <- current[., counts := sum(count), by = context]
contextCounts <- current[ counts := sum(count), by = context]
contextCounts <- current[ ,counts := sum(count), by = context]
contextCounts <- current[ ,sum(count), by = context]
contextCounts <- current[,.(counts = sum(count)), by = context]
context <- current[,.(contextCount = sum(count)), by = context]
setkey(context, context)
setkey(current, context)
current <- model[[x]]
createTestNGrams(directories)
# Initialize MKN language mkn
mknInit(mkn, training$nGrams, regex)
# Create absolute counts of each nGram
features <- parallelizeTask(mknAbsCount, mkn, training$nGrams)
# Create continuation counts of each nGram
parallelizeTask(mknCKN, mkn, mkn$mOrder)
# Count nGram histories
parallelizeTask(mknHistories, mkn, mkn$mOrder)
# Calculate discounts
discounts <- mknDiscount(mkn)
current <- model[[x]]
message('...loading models and discounts')
model <- lapply(seq_along(mkn$counts), function(x) {
loadObject(mkn$counts[[x]])
})
discounts <- loadObject(mkn$discounts)
summary <- loadObject(mkn$summary)
current <- model[[x]]
dCounts <- t(subset(discounts, nGramOrder == x, select = c(-(1:2))))
# Compute N1count groups based upon continution counts
current[, group := min(3, (cKN)), by=cKN]
# Determine the discount based upon the count group
current[, D := dCounts[group+1]]
# Subtract discount from continuation count
current[, alphaCount := (cKN - D)]
current[alphaCount < 0, alphaCount := 0]
context <- current[,.(contextCount = sum(count)), by = context]
setkey(context, context)
setkey(current, context)
current <- merge(current, context, by = "context")
View(current)
current[, alpha := alphaCount / contextCount]
View(current)
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM06.mknAlpha.R')
createTestNGrams(directories)
# Initialize MKN language mkn
mknInit(mkn, training$nGrams, regex)
# Create absolute counts of each nGram
features <- parallelizeTask(mknAbsCount, mkn, training$nGrams)
# Create continuation counts of each nGram
parallelizeTask(mknCKN, mkn, mkn$mOrder)
# Count nGram histories
parallelizeTask(mknHistories, mkn, mkn$mOrder)
# Calculate discounts
discounts <- mknDiscount(mkn)
# Calculate pseudo probability alpha
parallelizeTask(mknAlpha, mkn)
# Compute weighting factor lambda
parallelizeTask(mknLambda, mkn)
# Compute probabilities
parallelizeTask(mknEstimate, mkn)
source('~/Data Science/Data Science Projects/PredictifyR-1.0/lab/write2Excel.R')
loadObject(lm$mkn$epsilon$counts[[1]])
mknUnigram <- loadObject(lm$mkn$epsilon$counts[[1]])
source('~/Data Science/Data Science Projects/PredictifyR-1.0/lab/write2Excel.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/lab/write2Excel.R')
View(mknUnigram)
source('~/Data Science/Data Science Projects/PredictifyR-1.0/lab/write2Excel.R')
x <- 1
summary <- loadObject(mkn$summary)
current <- loadObject(mkn$counts[[x]])
uniformDistribution <- 1 / summary[1,2]
uniformDistribution <- 1 / summary[1,2]$Count
current[, Pmkn := alpha + D * uniformDistribution]
current[, Pmkn := alpha + (D * uniformDistribution)]
View(current)
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM08.mknEstimate.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/config/initEnvironment.R')
training <- corpora$training$epsilon
test <- corpora$validation$epsilon
mkn <- lm$mkn
regex <- regexPatterns
# Get Test NGrams
createTestNGrams(directories)
# Initialize MKN language mkn
mknInit(mkn, training$nGrams, regex)
# Create absolute counts of each nGram
features <- parallelizeTask(mknAbsCount, mkn, training$nGrams)
# Create continuation counts of each nGram
parallelizeTask(mknCKN, mkn, mkn$mOrder)
# Count nGram histories
parallelizeTask(mknHistories, mkn, mkn$mOrder)
# Calculate discounts
discounts <- mknDiscount(mkn)
# Calculate pseudo probability alpha
parallelizeTask(mknAlpha, mkn)
# Compute weighting factor lambda
parallelizeTask(mknLambda, mkn)
# Compute probabilities
parallelizeTask(mknEstimate, mkn)
createTestNGrams(directories)
source('~/Data Science/Data Science Projects/PredictifyR-1.0/lab/createTestNGrams.R')
createTestNGrams(directories)
# Initialize MKN language mkn
mknInit(mkn, training$nGrams, regex)
# Create absolute counts of each nGram
features <- parallelizeTask(mknAbsCount, mkn, training$nGrams)
# Create continuation counts of each nGram
parallelizeTask(mknCKN, mkn, mkn$mOrder)
# Count nGram histories
parallelizeTask(mknHistories, mkn, mkn$mOrder)
# Calculate discounts
discounts <- mknDiscount(mkn)
# Calculate pseudo probability alpha
parallelizeTask(mknAlpha, mkn)
# Compute weighting factor lambda
parallelizeTask(mknLambda, mkn)
# Compute probabilities
parallelizeTask(mknEstimate, mkn)
createTestNGrams(directories)
createTestNGrams(directories)
warnings()
features <- parallelizeTask(mknAbsCount, mkn, training$nGrams)
training <- corpora$training$epsilon
features <- parallelizeTask(mknAbsCount, mkn, training$nGrams)
View(analyzeLexicalFeatures)
message(paste('\nExecuting', mkn$mDesc, 'on',
training$corpusName, 'at', startTime))
startTime <- Sys.time()
message(paste('\nExecuting', mkn$mDesc, 'on',
training$corpusName, 'at', startTime))
mkn <- lm$mkn$epsilon
createTestNGrams(directories)
# Initialize MKN language mkn
mknInit(mkn, training$nGrams, regex)
# Create absolute counts of each nGram
features <- parallelizeTask(mknAbsCount, mkn, training$nGrams)
# Create continuation counts of each nGram
parallelizeTask(mknCKN, mkn, mkn$mOrder)
# Count nGram histories
parallelizeTask(mknHistories, mkn, mkn$mOrder)
# Calculate discounts
discounts <- mknDiscount(mkn)
# Calculate pseudo probability alpha
parallelizeTask(mknAlpha, mkn)
# Compute weighting factor lambda
parallelizeTask(mknLambda, mkn)
# Compute probabilities
parallelizeTask(mknEstimate, mkn)
source('~/Data Science/Data Science Projects/PredictifyR-1.0/lab/write2Excel.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM08.mknEstimate.R')
createTestNGrams(directories)
# Initialize MKN language mkn
mknInit(mkn, training$nGrams, regex)
# Create absolute counts of each nGram
features <- parallelizeTask(mknAbsCount, mkn, training$nGrams)
# Create continuation counts of each nGram
parallelizeTask(mknCKN, mkn, mkn$mOrder)
# Count nGram histories
parallelizeTask(mknHistories, mkn, mkn$mOrder)
# Calculate discounts
discounts <- mknDiscount(mkn)
# Calculate pseudo probability alpha
parallelizeTask(mknAlpha, mkn)
# Compute weighting factor lambda
parallelizeTask(mknLambda, mkn)
# Compute probabilities
parallelizeTask(mknEstimate, mkn)
source('~/Data Science/Data Science Projects/PredictifyR-1.0/lab/write2Excel.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM08.mknEstimate.R')
createTestNGrams(directories)
# Initialize MKN language mkn
mknInit(mkn, training$nGrams, regex)
# Create absolute counts of each nGram
features <- parallelizeTask(mknAbsCount, mkn, training$nGrams)
# Create continuation counts of each nGram
parallelizeTask(mknCKN, mkn, mkn$mOrder)
# Count nGram histories
parallelizeTask(mknHistories, mkn, mkn$mOrder)
# Calculate discounts
discounts <- mknDiscount(mkn)
# Calculate pseudo probability alpha
parallelizeTask(mknAlpha, mkn)
# Compute weighting factor lambda
parallelizeTask(mknLambda, mkn)
# Compute probabilities
parallelizeTask(mknEstimate, mkn)
source('~/Data Science/Data Science Projects/PredictifyR-1.0/lab/write2Excel.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM07.mknLambda.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM08.mknEstimate.R')
x <- 2
current <- model[[x]]
model <- lapply(seq_along(mkn$counts), function(x) {
loadObject(mkn$counts[[x]])
})
current <- model[[x]]
histories1  <- current[count == 1,.(context)]
histories2  <- current[count == 2,.(context)]
histories3  <- current[count > 2,.(context)]
counts1 <- histories1[,.(n1wdot = .N), by = context]
counts2 <- histories2[,.(n2wdot = .N), by = context]
counts3 <- histories3[,.(n3pwdot = .N), by = context]
createTestNGrams(directories)
# Initialize MKN language mkn
mknInit(mkn, training$nGrams, regex)
# Create absolute counts of each nGram
features <- parallelizeTask(mknAbsCount, mkn, training$nGrams)
# Create continuation counts of each nGram
parallelizeTask(mknCKN, mkn, mkn$mOrder)
model <- lapply(seq_along(mkn$counts), function(x) {
loadObject(mkn$counts[[x]])
})
current <- model[[x]]
# Extract histories
histories1  <- current[count == 1,.(context)]
histories2  <- current[count == 2,.(context)]
histories3  <- current[count > 2,.(context)]
# Count number of rows for each group
counts1 <- histories1[,.(n1wdot = .N), by = context]
counts2 <- histories2[,.(n2wdot = .N), by = context]
counts3 <- histories3[,.(n3pwdot = .N), by = context]
# Merge counts into current ngram table
current <- merge(current, counts1, by.x = 'context', by.y = 'context', all.x = TRUE)
current <- merge(current, counts2, by.x = 'context', by.y = 'context', all.x = TRUE)
current <- merge(current, counts3, by.x = 'context', by.y = 'context', all.x = TRUE)
View(current)
current[context == '555']
current[context == '<s>']
for (i in seq_along(current)) set(current, i=which(is.na(current[[i]])), j=i, value=0)
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM04.mknHistories.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM06.mknAlpha.R')
x <- 2
lower <- loadObject(mkn$counts[[x-1]])
# Extract lower level probabilities
lowerProbs <- lower[,.(nGram, Pmkn)]
x <- 1
current <- loadObject(mkn$counts[[x]])
current[, Pmkn := cKN / summary[2,2]$Count]
summary <- loadObject(mkn$summary)
current[, Pmkn := cKN / summary[2,2]$Count]
# Save nGram
mkn$counts[[x]]$data <- current
saveObject(mkn$counts[[x]])
x <- 2
lower <- loadObject(mkn$counts[[x-1]])
lowerProbs <- lower[,.(nGram, Pmkn)]
lambda <- current[,.(nGram, suffix, lambda)]
createTestNGrams(directories)
# Initialize MKN language mkn
mknInit(mkn, training$nGrams, regex)
# Create absolute counts of each nGram
features <- parallelizeTask(mknAbsCount, mkn, training$nGrams)
# Create continuation counts of each nGram
parallelizeTask(mknCKN, mkn, mkn$mOrder)
# Count nGram histories
parallelizeTask(mknHistories, mkn, mkn$mOrder)
# Calculate discounts
discounts <- mknDiscount(mkn)
# Calculate pseudo probability alpha
parallelizeTask(mknAlpha, mkn)
# Compute weighting factor lambda
parallelizeTask(mknLambda, mkn)
createTestNGrams(directories)
# Initialize MKN language mkn
mknInit(mkn, training$nGrams, regex)
# Create absolute counts of each nGram
features <- parallelizeTask(mknAbsCount, mkn, training$nGrams)
# Create continuation counts of each nGram
parallelizeTask(mknCKN, mkn, mkn$mOrder)
# Count nGram histories
parallelizeTask(mknHistories, mkn, mkn$mOrder)
# Calculate discounts
discounts <- mknDiscount(mkn)
# Calculate pseudo probability alpha
parallelizeTask(mknAlpha, mkn)
load("~/Data Science/Data Science Projects/PredictifyR-1.0/lm/mkn/epsilon/counts/mkn-bigrams.Rdata")
load("~/Data Science/Data Science Projects/PredictifyR-1.0/lm/mkn/epsilon/counts/mkn-quadgrams.Rdata")
load("~/Data Science/Data Science Projects/PredictifyR-1.0/lm/mkn/epsilon/counts/mkn-trigrams.Rdata")
load("~/Data Science/Data Science Projects/PredictifyR-1.0/lm/mkn/epsilon/counts/mkn-unigrams.Rdata")
load("~/Data Science/Data Science Projects/PredictifyR-1.0/lm/mkn/epsilon/counts/mkn-bigrams.Rdata")
load("~/Data Science/Data Science Projects/PredictifyR-1.0/lm/mkn/epsilon/counts/mkn-quadgrams.Rdata")
load("~/Data Science/Data Science Projects/PredictifyR-1.0/lm/mkn/epsilon/counts/mkn-trigrams.Rdata")
load("~/Data Science/Data Science Projects/PredictifyR-1.0/lm/mkn/epsilon/counts/mkn-unigrams.Rdata")
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM07.mknLambda.R')
load("~/Data Science/Data Science Projects/PredictifyR-1.0/lm/mkn/epsilon/counts/mkn-bigrams.Rdata")
load("~/Data Science/Data Science Projects/PredictifyR-1.0/lm/mkn/epsilon/counts/mkn-quadgrams.Rdata")
load("~/Data Science/Data Science Projects/PredictifyR-1.0/lm/mkn/epsilon/counts/mkn-trigrams.Rdata")
load("~/Data Science/Data Science Projects/PredictifyR-1.0/lm/mkn/epsilon/counts/mkn-unigrams.Rdata")
createTestNGrams(directories)
# Initialize MKN language mkn
mknInit(mkn, training$nGrams, regex)
# Create absolute counts of each nGram
features <- parallelizeTask(mknAbsCount, mkn, training$nGrams)
# Create continuation counts of each nGram
parallelizeTask(mknCKN, mkn, mkn$mOrder)
# Count nGram histories
parallelizeTask(mknHistories, mkn, mkn$mOrder)
# Calculate discounts
discounts <- mknDiscount(mkn)
# Calculate pseudo probability alpha
parallelizeTask(mknAlpha, mkn)
load("~/Data Science/Data Science Projects/PredictifyR-1.0/lm/mkn/epsilon/counts/mkn-bigrams.Rdata")
load("~/Data Science/Data Science Projects/PredictifyR-1.0/lm/mkn/epsilon/counts/mkn-quadgrams.Rdata")
load("~/Data Science/Data Science Projects/PredictifyR-1.0/lm/mkn/epsilon/counts/mkn-trigrams.Rdata")
load("~/Data Science/Data Science Projects/PredictifyR-1.0/lm/mkn/epsilon/counts/mkn-unigrams.Rdata")
View(mknBigrams)
View(mknQuadgrams)
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM07.mknLambda.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM08.mknEstimate.R')

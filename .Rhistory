# Compute training, validation and test set sizes
load("~/Data Science/Data Science Projects/PredictifyR-1.0/analysis/fast-analysis-of-clean-corpus_20170517_160253.Rdata")
#---------------------------------------------------------------------------#
#                  Compute Estimated Pilot Corpus Size                      #
#---------------------------------------------------------------------------#
register <- c(as.character(registerSizeEstimate$Register[1:3]), 'Corpus')
hcCorpus <- analysis$featureMatrix$tokens
diversity <- as.vector(t(sampleSizeEstimate[1:4,3]))
syntactic <- round(registerSizeEstimate$'Sample Size'[1:4], 0)
tokens <- pmax(diversity, syntactic, na.rm = TRUE)
sentences <- floor(tokens[1:3] / analysis$featureMatrix$wordsPerSent[1:3])
sentences <- c(sentences, sum(sentences))
pctTotal <- round(tokens / analysis$featureMatrix$tokens[1:4] * 100, 0)
proportion <- round(tokens[1:3] / sum(tokens[1:3]) * 100, 0)
proportion <- c(proportion, 100)
comparison <- data.frame(register = register, hcCorpus = hcCorpus,
diversity = diversity,
syntactic = syntactic, tokens = tokens,
sentences = sentences, pctTotal = pctTotal,
proportion = proportion)
names(comparison) <- c('Register', 'HC Corpus', 'Diversity-Based Estimate',
'Lexical Feature-Based Estimate',
'Tokens', 'Sentences', '% Total',
'Proportion')
#---------------------------------------------------------------------------#
#                  Compute Extrapolated Pilot Corpus Size                   #
#---------------------------------------------------------------------------#
register <- c(as.character(registerSizeEstimate$Register[1:3]), 'Corpus')
hcTokens <- analysis$featureMatrix$tokens
extrapolated <- pctTotal / pctTotal[4] * 5
extrapolatedTokens <- hcTokens * extrapolated / 100
extrapolatedSents <- floor(extrapolatedTokens / analysis$featureMatrix$wordsPerSent)
chunkSize <- rep(samplingUnit[[length(samplingUnit)]]$size, 4)
sentsPerChunk <- floor(samplingUnit[[length(samplingUnit)]]$size /
analysis$featureMatrix$wordsPerSent)
chunks <- floor(extrapolatedSents / sentsPerChunk)
sampleSize <- chunks * sentsPerChunk
pilot <- data.frame(register = register, hcTokens = hcTokens,
extrapolated = extrapolated,
extrapolatedTokens = extrapolatedTokens,
extrapolatedSents = extrapolatedSents,
chunkSize = chunkSize,
sentsPerChunk = sentsPerChunk,
chunks = chunks,
sampleSize = sampleSize)
names(pilot) <- c('Register', 'HC Corpus (Tokens)',
'% Total', 'Tokens', 'Sentences',
'Chunk Size (Tokens)',
'Sentences per Chunk', '# Chunks',
'Sample Size (Sentences)')
#---------------------------------------------------------------------------#
#           Compute Model Corpora (Train, Validation, Test) Sizes           #
#---------------------------------------------------------------------------#
# Compute training, validation and test set sizes
sampleSizeEstimate <- sampleSizeEstimate$sampleSize
#---------------------------------------------------------------------------#
#                  Compute Estimated Pilot Corpus Size                      #
#---------------------------------------------------------------------------#
register <- c(as.character(registerSizeEstimate$Register[1:3]), 'Corpus')
hcCorpus <- analysis$featureMatrix$tokens
diversity <- as.vector(t(sampleSizeEstimate[1:4,3]))
syntactic <- round(registerSizeEstimate$'Sample Size'[1:4], 0)
tokens <- pmax(diversity, syntactic, na.rm = TRUE)
sentences <- floor(tokens[1:3] / analysis$featureMatrix$wordsPerSent[1:3])
sentences <- c(sentences, sum(sentences))
pctTotal <- round(tokens / analysis$featureMatrix$tokens[1:4] * 100, 0)
proportion <- round(tokens[1:3] / sum(tokens[1:3]) * 100, 0)
proportion <- c(proportion, 100)
comparison <- data.frame(register = register, hcCorpus = hcCorpus,
diversity = diversity,
syntactic = syntactic, tokens = tokens,
sentences = sentences, pctTotal = pctTotal,
proportion = proportion)
names(comparison) <- c('Register', 'HC Corpus', 'Diversity-Based Estimate',
'Lexical Feature-Based Estimate',
'Tokens', 'Sentences', '% Total',
'Proportion')
#---------------------------------------------------------------------------#
#                  Compute Extrapolated Pilot Corpus Size                   #
#---------------------------------------------------------------------------#
register <- c(as.character(registerSizeEstimate$Register[1:3]), 'Corpus')
hcTokens <- analysis$featureMatrix$tokens
extrapolated <- pctTotal / pctTotal[4] * 5
extrapolatedTokens <- hcTokens * extrapolated / 100
extrapolatedSents <- floor(extrapolatedTokens / analysis$featureMatrix$wordsPerSent)
chunkSize <- rep(samplingUnit[[length(samplingUnit)]]$size, 4)
sentsPerChunk <- floor(samplingUnit[[length(samplingUnit)]]$size /
analysis$featureMatrix$wordsPerSent)
chunks <- floor(extrapolatedSents / sentsPerChunk)
sampleSize <- chunks * sentsPerChunk
pilot <- data.frame(register = register, hcTokens = hcTokens,
extrapolated = extrapolated,
extrapolatedTokens = extrapolatedTokens,
extrapolatedSents = extrapolatedSents,
chunkSize = chunkSize,
sentsPerChunk = sentsPerChunk,
chunks = chunks,
sampleSize = sampleSize)
names(pilot) <- c('Register', 'HC Corpus (Tokens)',
'% Total', 'Tokens', 'Sentences',
'Chunk Size (Tokens)',
'Sentences per Chunk', '# Chunks',
'Sample Size (Sentences)')
#---------------------------------------------------------------------------#
#           Compute Model Corpora (Train, Validation, Test) Sizes           #
#---------------------------------------------------------------------------#
# Compute training, validation and test set sizes
Registers <- c('Blogs', 'News', 'Twitter', 'Corpus')
Validation <- pilot$`Sample Size (Sentences)`[1:3]
Test <- pilot$`Sample Size (Sentences)`[1:3]
Alpha <-  pilot$`Sample Size (Sentences)`[1:3] * 2
Beta <-  pilot$`Sample Size (Sentences)`[1:3] * c(5,2,5)
Gamma <-  pilot$`Sample Size (Sentences)`[1:3] * c(10,2,10)
Delta <-  pilot$`Sample Size (Sentences)`[1:3] * c(14,2,14)
corpusDesign <- cbind(Alpha, Beta, Gamma, Delta, Validation, Test)
ttl <- colSums(corpusDesign)
corpusDesign <- rbind(corpusDesign, ttl)
class(corpusDesign)
str(corpusDesign)
corpusDesign <- data.frame(Alpha, Beta, Gamma, Delta, Validation, Test)
View(corpusDesign)
str(corpusDesign)
ttl <- colSums(corpusDesign)
corpusDesign <- rbind(corpusDesign, ttl)
corpusDesign <- as.data.frame(cbind(Registers, corpusDesign), stringsAsFactors=FALSE)
str(corpusDesign)
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/B06.designPilot.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/E1.exploreTraining.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/E0.buildCorpora.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/E1.exploreTraining.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/E1.exploreTraining.R')
load("~/Data Science/Data Science Projects/PredictifyR-1.0/analysis/pilot-corpus-design_20170518_101618.Rdata")
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/B02.estimateSampleSize.R')
load("~/Data Science/Data Science Projects/PredictifyR-1.0/analysis/nGrams-for-_20170518_103602.Rdata")
load("~/Data Science/Data Science Projects/PredictifyR-1.0/analysis/nGrams-for-_20170518_103602.Rdata")
rm(nGramCorpus)
rm(nGrams)
load("~/Data Science/Data Science Projects/PredictifyR-1.0/analysis/nGrams-for-_20170518_103602.Rdata")
load("~/Data Science/Data Science Projects/PredictifyR-1.0/analysis/verify-vocabulary_20170518_120236.Rdata")
View(verifyVocabulary)
load("~/Data Science/Data Science Projects/PredictifyR-1.0/analysis/verify-vocabulary_20170518_135406.Rdata")
load("~/Data Science/Data Science Projects/PredictifyR-1.0/analysis/verify-vocabulary_20170518_135406.Rdata")
View(verifyVocabulary)
load("~/Data Science/Data Science Projects/PredictifyR-1.0/analysis/verify-vocabulary_20170518_135406.Rdata")
View(verifyVocabulary)
load("~/Data Science/Data Science Projects/PredictifyR-1.0/analysis/verify-vocabulary_20170518_130021.Rdata")
View(verifyVocabulary)
load("~/Data Science/Data Science Projects/PredictifyR-1.0/analysis/verify-vocabulary_20170518_122119.Rdata")
load("~/Data Science/Data Science Projects/PredictifyR-1.0/analysis/verify-vocabulary_20170518_120236.Rdata")
source('~/Data Science/Data Science Projects/PredictifyR-1.0/config/schema.R')
n <- 1
message(paste('\n...analyzing', validation$nGrams[[n]]$fileDesc))
training <- corpora$training
validation <- corpora$validation
message(paste('\n...analyzing', validation$nGrams[[n]]$fileDesc))
t <-1
validationNGram <- loadObject(validation$nGrams[[n]])
validationNGrams <- parallelizeTask(nGramCorpus, validation, directories)
validationNGrams <- parallelizeTask(nGramCorpus, validation, directories)
source('~/Data Science/Data Science Projects/PredictifyR-1.0/config/initEnvironment.R')
training[[t]]$nGrams[[n]]
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/D4.analyzeNGramCoverageTraining.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/D4.analyzeNGramCoverageTraining.R')
ntoken(tSetDfm)
tSetDfm <- loadObject(training[[t]]$nGrams[[n]])
validationNGram <- loadObject(validation$nGrams[[n]])
tSetNGrams <- featnames(tSetDfm)
oov <- dfm_select(validationNGram, features = tSetNGrams,
selection = 'remove', valuetype = 'fixed
oov <- dfm_select(validationNGram, features = tSetNGrams,
selection = 'remove', valuetype = 'fixed')
oovData <- list()
oovData$tSet <- training[[t]]$corpusName
oovData$nGram <- training[[t]]$nGrams[[n]]$fileDesc
oovData$tSetNGrams <- ntoken(tSetDfm)
oovData$tSet <- training[[t]]$corpusName
oovData$nGram <- training[[t]]$nGrams[[n]]$fileDesc
oovData$tSetNGrams <- sum(ntoken(tSetDfm))
oovData$validationNGrams <- sum(ntoken(validationNGram))
oovData$vOOV <- length(featnames(oov))
oovData$nOOV <- sum(ntoken(oov))
oovData$vOOVRate <- length(featnames(oov)) / length(featnames(validationNGram))
oovData$nOOVRate <- oovData$vOOV / oovData$validationNGrams
oovData$coverage <- 1 - oovData$nOOVRate
oovData$coverage <- 1 - oovData$nOOVRate * 100
oovData$coverage <- (1 - oovData$nOOVRate) * 100
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/D4.analyzeNGramCoverageTraining.R')
training <- corpora$training
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/D4.analyzeNGramCoverageTraining.R')
training <- training[[1]]
source('~/Data Science/Data Science Projects/PredictifyR-1.0/config/schema.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/config/schema.R')
training <- training[[1]]
paste0(training$objName, 'CoverageAnalysis')
training <- corpora$training[[1]]
paste0(training$objName, 'CoverageAnalysis')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/D4.analyzeNGramCoverageTraining.R')
load("~/Data Science/Data Science Projects/PredictifyR-1.0/analysis/coverage-analysis-training-set-alpha_20170519_125939.Rdata")
View(trainingSetAlphaCoverageAnalysis)
View(trainingSetAlphaCoverageAnalysis)
training
n <- 2
validationNGram <- loadObject(validation$nGrams[[n]])
message(paste('\n......loading', training$corpusName))
tSetDfm <- loadObject(training$nGrams[[n]])
tSetNGrams <- featnames(tSetDfm)
oov <- dfm_select(validationNGram, features = tSetNGrams,
selection = 'remove', valuetype = 'fixed')
validationV <- featnames(validationNGram)
testV <- featnames(tSetDfm)
oovData <- list()
oovData$tSet  <- training$corpusName
oovData$nGram <- training$nGrams[[n]]$fileDesc
oovData$tSetNGrams <- sum(ntoken(tSetDfm))
oovData$validationNGrams <- sum(ntoken(validationNGram))
oovData$vOOV <- validationV - validationV %in% testV
oovData$vOOV <- length(validationV) - length(validationV %in% testV)
length(validationV)
validationV %in% testV
length(validationV %in% testV)
length(validationV)
length((validationV %in% testV) == TRUE)
sum((validationV %in% testV) == TRUE)
length(validationV) - sum((validationV %in% testV) == TRUE)
validationN <- tokens(validationNGram)
testN <- tokens(tSetNGrams)
validationN <- tokenize(validationNGram)
validationN <- quanteda::tokenize(validationNGram)
validationN <- quanteda::tokenize(readFile(validation$documents[[n]]),
what = 'word', ngrams = n)
source('~/Data Science/Data Science Projects/PredictifyR-1.0/config/schema.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/config/schema.R')
?unique
validationNGrams <- quanteda::tokenize(readFile(validation$documents[[n]]),
what = 'word', ngrams = n)
message(paste('......tokenizing', training$corpusName))
trainingNGrams <- quanteda::tokenize(readFile(training$documents[[n]]),
what = 'word', ngrams = n)
validationV <- unique(validationNGrams)
trainingV <- unique(trainingNGrams)
sum(validationV %in% trainingV)
length(validationV)
nOOV <- length(validaetionNGrams) - sum(validationNGram %in% trainingNGrams)
nOOV <- length(validationNGrams) - sum(validationNGrams %in% trainingNGrams)
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/D4.analyzeNGramCoverageTraining.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/D4.analyzeNGramCoverageTraining.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/D4.analyzeNGramCoverageTraining.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/D4.analyzeNGramCoverageTraining.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/D4.analyzeNGramCoverageTraining.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/D4.analyzeNGramCoverageTraining.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/config/schema.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/config/schema.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/config/schema.R')
korpus <- corpora$training$alpha
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/C1.tagCorpus.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/config/schema.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/C1.tagCorpus.R')
training <- corpora$training
model = lapply(seq_along(nGrams), function(n) {
nGram = list()
nGram$directory = file.path(directories$lm, 'mkn', 'model')
nGram$objName  = paste0('mkn', nGrams[[n]]$fileDesc)
nGram$fileName = paste0('mkn-', nGrams[[n]]$fileName)
nGram$fileDesc = paste0('MKN ', nGrams[[n]]$fileDesc)
nGram
})
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM03.mknCKN.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM00.mknPipeline.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM04.mknHistories.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM06.mknNorm.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM08.mknLambda.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM00.mknPipeline.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/RQ01.pipeline.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM00.mknPipeline.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/config/schema.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/config/schema.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/config/schema.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/config/schema.R')
training <- corpora$training
training <- corpora$training$alpha
source('~/Data Science/Data Science Projects/PredictifyR-1.0/config/schema.R')
training <- corpora$training$alpha
source('~/Data Science/Data Science Projects/PredictifyR-1.0/config/schema.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/config/schema.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/config/schema.R')
training <- corpora$training
source('~/Data Science/Data Science Projects/PredictifyR-1.0/config/schema.R')
training <- corpora$training
test <- corpora$validation
training <- corpora$training$alpha
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/E0.preprocessCorpora.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/E0.preprocessCorpora.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/config/schema.R')
korpus <- corpora$training$alpha
training <- corpora$training$alpha
korpus <- training$processed$words
validation <- corpora$validation
path
korpus <- corpora$training
message(paste("\nAnnotating", korpus$corpusName, "at",  startTime))
startTime <- Sys.time()
message(paste("\nAnnotating", korpus$corpusName, "at",  startTime))
korpus <- corpora$training
korpus <- corpora$training$alpha
startTime <- Sys.time()
message(paste("\nAnnotating", korpus$corpusName, "at",  startTime))
stopifnot(type == 'word' | type == 'pos')
type <- 'word'
stopifnot(type == 'word' | type == 'pos')
if (type == 'word') {
path <- korpus$processed$words
} else if (type == 'pos') {
path <- korpus$processed$pos
}
korpus <- corpora$training$alpha
message(paste('......creating', korpus[[x]]$fileDesc, 'sentence boundaries'))
x < -1
x < 1
x <- 1
message(paste('......creating', korpus[[x]]$fileDesc, 'sentence boundaries'))
korpus <- corpora$training$alpha$processed$words
message(paste('......creating', korpus[[x]]$fileDesc, 'sentence boundaries'))
korpus <- corpora$training$alpha
path <- corpora$training$alpha$processed$words
document <- readFile(path[[1]])
source('~/Data Science/Data Science Projects/PredictifyR-1.0/config/initEnvironment.R')
document <- readFile(path[[1]])
message(paste('......creating', path[[x]]$fileDesc, 'sentence boundaries'))
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/E1.processCorpora.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/E1.processCorpus.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/C5.buildCorpora.R')
training <- corpora$training
training$alpha$processed$pos
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/C5.buildCorpora.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/E0.preprocessCorpora.R')
training <- corpora$training$alpha
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM00.mknPipeline.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/config/schema.R')
korpus <- corpora$training$alpha
message(paste('......creating nGrams for', korpus$processed$words[[x]]$fileDesc))
message(paste('......creating', korpus$processed$words[[x]]$fileDesc))
if ('training' == strsplit(korpus$corpusName, '-')[[1]][2])
)
if ('training' == strsplit(korpus$corpusName, '-')[[1]][2])
if ('Training' %in% korpus$corpusName)
)
('Training' %in% korpus$corpusName)
korpus$corpusName
('Training' %in% strsplit(korpus$corpusName, ' '))
strsplit(korpus$corpusName, ' ')
'Training' %in% strsplit(korpus$corpusName, ' ')[[1]]
message(paste('......creating', korpus$processed$pos[[x]]$fileDesc))
paste0('nGramSummary', korpus$objName)
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/E2.createNGrams.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/D4.analyzeNGramCoverageTraining.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/E2.createNGrams.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/config/schema.R')
model <- lm$mkn4
message(paste('\nExecuting', model$args$mName, 'at', startTime))
message(paste('\nExecuting', model$args$mDesc, 'at', startTime))
source('~/Data Science/Data Science Projects/PredictifyR-1.0/config/schema.R')
model <- lm$mkn4
test <- corpora$validation
load("~/Data Science/Data Science Projects/PredictifyR-1.0/analysis/coverage-analysis-training-set-delta_20170520_024623.Rdata")
load("~/Data Science/Data Science Projects/PredictifyR-1.0/analysis/fast-analysis-of-_20170520_015831.Rdata")
View(trainingSetDeltaCoverageAnalysis)
View(trainingSetDeltaCoverageAnalysis)
load("~/Data Science/Data Science Projects/PredictifyR-1.0/analysis/coverage-analysis-training-set-gamma_20170520_023025.Rdata")
source('~/Data Science/Data Science Projects/PredictifyR-1.0/config/schema.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/config/schema.R')
training <- corpora$training$alpha
metaData <- corpora$training$alpha
metaData <- corpora$clean
korpus <- lapply(seq_along(metaData$documents), function(d) {
k <- list()
k$directory <- metaData$documents[[d]]$directory
k$fileName <- metaData$documents[[d]]$fileName
k$fileDesc <- metaData$documents[[d]]$fileDesc
k$data <- readFile(metaData$documents[[d]])
k
})
document <- unlist(korpus$data)
document <- unlist(lapply(seq_along(korpus), function(d) {
korpus[[d]]$data
}))
Corpus <- metaData$corpusName
Corpus <- metaData$corpusName
Words <- tokens(document)
Words <- ntoken(document)
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/D4.analyzeNGramCoverageTraining.R')
coverageSummary <- list(
alpha <- coverageTrainingAlpha,
beta <- coverageTrainingBeta,
gamma <- coverageTrainingGamma,
delta <- coverageTrainingDelta
)
source('~/Data Science/Data Science Projects/PredictifyR-1.0/config/schema.R')
training <- corpora$training$gamma
message(paste("\nPreprocessing", training$corpusName, 'at', startTime))
training <- corpora$training
s <- 1
test$processed[[model$args$mOrder-2]]
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM00.mknPipeline.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/E0.preprocessCorpora.R')
test <- corpora$test
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/E0.preprocessCorpora.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/E0.preprocessCorpora.R')
load("~/Data Science/Data Science Projects/PredictifyR-1.0/analysis/preprocess-corpora_20170520_043537.Rdata")
View(preprocessCorpora)
training <- corpora$training$alpha
validation <- corpora$validation
test <- corpora$test
oov <- oovCorpora(training, validation, test)
View(oov)
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/E0.preprocessCorpora.R')
training <- corpora$training
c <- 1
corpora <- list(
alpha <- training$alpha,
beta <- training$beta,
gamma <- training $gamma,
delta <- training$delta,
validation <- validation,
test <- test
)
document <- unlist(lapply(seq_along(corpora[[c]]$documents), function(d) {
readFile(corpora[[c]]$documents[[d]])
}))
names(document) <- corpora[[c]]$corpusName
document[1]
document[2]
document[3]
document <- unlist(lapply(seq_along(corpora[[c]]$documents), function(d) {
readFile(corpora[[c]]$documents[[d]])
}))
korpus <- quanteda::corpus(document)
stats <- summary(korpus)
View(stats)
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/C6.summarizeCorpora.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/C6.summarizeCorpora.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM00.mknPipeline.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/C6.summarizeCorpora.R')
training[[s]]
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/E0.preprocessCorpora.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/E0.preprocessCorpora.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/config/initEnvironment.R')
korpora <- corpora$training$alpha
korpus <- corpora$training$alpha
path <- corpora$training$alpha$processed$words
document <- readFile(path[[1]])
load("~/Data Science/Data Science Projects/PredictifyR-1.0/analysis/model-corpora-descriptive-statistics_20170520_133109.Rdata")
View(corporaSummary)
View(corporaSummary)
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM00.mknPipeline.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/E2.createNGrams.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM00.mknPipeline.R')
document <- readFile(korpus$processed$words[[1]])
x <- 1
message(paste('......creating', korpus$processed$words[[x]]$fileDesc, 'sentence boundaries'))
annotatedDoc <- unlist(lapply(seq_along(document), function(s) {
paste(paste0(rep("BOS", times = s-1), collapse = " "), document[s], "EOS", collapse = " ")
}))
s <- 1
paste(paste0(rep("BOS", times = s-1), collapse = " "), document[s], "EOS", collapse = " ")
annotatedDoc <- lapply(seq_along(document), function(s) {
paste(paste0(rep("BOS", times = s-1), collapse = " "), document[s], "EOS", collapse = " ")
})
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/E1.processCorpus.R')
Q
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/E1.processCorpus.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/E1.processCorpus.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/E2.createNGrams.R')
mkn <- lm$mkn4
nGrams <- corpora$training$alpha$nGrams$words
regex <- regexPatterns
x <- 1
message(paste('...initializing', mkn$args$counts[[x]]$fileDesc))
nGram <- loadObject(nGrams[[x]])
counts <- data.table(nGram = featnames(nGram), key = 'nGram')
bosGram <- paste0(rep("BOS", x), collapse = ' ')
counts <- rbindlist(list(counts, list(bosGram)))
mkn$args$counts[[x]]$data <- counts
saveObject(mkn$args$counts[[x]])
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM01.mknInit.R')
x <- 2
message(paste('...initializing', mkn$args$counts[[x]]$fileDesc))
nGram <- loadObject(nGrams[[x]])
counts <- data.table(nGram = featnames(nGram), key = 'nGram')
# Add n BOSs where n is the order of the nGram.
bosGram <- paste0(rep("BOS", x), collapse = ' ')
counts <- rbindlist(list(counts, list(bosGram)))
if (x > 1) {
context <- gsub(regex$context[[x-1]], "\\1", counts$nGram, perl = TRUE)
suffix  <- gsub(regex$suffix[[x-1]], "\\1", counts$nGram, perl = TRUE)
counts[, c('context', 'suffix') := list(context, suffix)]
}
mkn$args$counts[[x]]$data <- counts
saveObject(mkn$args$counts[[x]])
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM00.mknPipeline.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/config/schema.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/config/schema.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/config/schema.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/E0.preprocessCorpora.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/D4.analyzeNGramCoverageTraining.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/config/schema.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/D4.analyzeNGramCoverageTraining.R')
load("~/Data Science/Data Science Projects/PredictifyR-1.0/logs/log.Rdata")
View(log)
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM00.mknPipeline.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM00.mknPipeline.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM00.mknPipeline.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM00.mknPipeline.R')

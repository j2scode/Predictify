set.seed(0505)
chunks <- design$corporaDesign[t+1,r] /
design$pilot$`Sentences per Chunk`[r]
trainingIndices <- sample(pool, chunks)
training[[t]]$documents[[r]]$data <- unlist(korpus[[r]][trainingIndices])
saveFile(training[[t]]$documents[[r]])
})
t <- 4
design$corporaDesign[t+1,r] /
design$pilot$`Sentences per Chunk`[r]
design$corporaDesign[r, t+1] /
design$pilot$`Sentences per Chunk`[r]
message(paste('......processing', hcCorpus$documents[[r]]$fileDesc))
message('......creating validation set')
pool <- 1:length(korpus[[r]])
chunks <- design$corporaDesign$Validation[r] /
design$pilot$`Sentences per Chunk`[r]
set.seed(0505)
valIndices <- sample(pool, chunks)
validation$documents[[r]]$data <- unlist(korpus[[r]][valIndices])
saveFile(validation$documents[[r]])
message('......creating test set')
pool <- pool[!pool %in% valIndices]
chunks <- design$corporaDesign$Test[r] /
design$pilot$`Sentences per Chunk`[r]
set.seed(0505)
testIndices <- sample(pool, chunks)
test$documents[[r]]$data <- unlist(korpus[[r]][testIndices])
saveFile(test$documents[[r]])
message('......creating training sets')
pool <- pool[!pool %in% testIndices]
lapply(seq_along(training), function(t) {
set.seed(0505)
chunks <- design$corporaDesign[r, t+1] /
design$pilot$`Sentences per Chunk`[r]
trainingIndices <- sample(pool, chunks)
training[[t]]$documents[[r]]$data <- unlist(korpus[[r]][trainingIndices])
saveFile(training[[t]]$documents[[r]])
})
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/E0.buildCorpora.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/B06.designPilot.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/E1.exploreTraining.R')
training <- corpora$training
validation <- corpora$validation
message(paste("\nCreating nGrams for training sets", startTime))
nGrams <- rbindlist(lapply(seq_along(training), function(t) {
message(paste('...processing', training[[t]]$corpusName))
nGramCorpus(training[[t]], directories)
}))
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/E1.exploreTraining.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/E0.buildCorpora.R')
load("~/Data Science/Data Science Projects/PredictifyR-1.0/analysis/pilot-corpus-design_20170518_025441.Rdata")
load("~/Data Science/Data Science Projects/PredictifyR-1.0/analysis/pilot-corpus-design_20170518_025441.Rdata")
load("~/Data Science/Data Science Projects/PredictifyR-1.0/analysis/pilot-corpus-design_20170518_033416.Rdata")
load("~/Data Science/Data Science Projects/PredictifyR-1.0/analysis/pilot-corpus-design_20170518_033416.Rdata")
#---------------------------------------------------------------------------#
#                  Compute Estimated Pilot Corpus Size                      #
#---------------------------------------------------------------------------#
register <- c(as.character(registerSizeEstimate$Register[1:3]), 'Corpus')
hcCorpus <- analysis$featureMatrix$tokens
diversity <- as.vector(t(sampleSizeEstimate[1:4,3]))
syntactic <- round(registerSizeEstimate$'Sample Size'[1:4], 0)
tokens <- pmax(diversity, syntactic, na.rm = TRUE)
sentences <- floor(tokens[1:3] / analysis$featureMatrix$wordsPerSent[1:3])
sentences <- c(sentences, sum(sentences))
pctTotal <- round(tokens / analysis$featureMatrix$tokens[1:4] * 100, 0)
proportion <- round(tokens[1:3] / sum(tokens[1:3]) * 100, 0)
proportion <- c(proportion, 100)
comparison <- data.frame(register = register, hcCorpus = hcCorpus,
diversity = diversity,
syntactic = syntactic, tokens = tokens,
sentences = sentences, pctTotal = pctTotal,
proportion = proportion)
names(comparison) <- c('Register', 'HC Corpus', 'Diversity-Based Estimate',
'Lexical Feature-Based Estimate',
'Tokens', 'Sentences', '% Total',
'Proportion')
#---------------------------------------------------------------------------#
#                  Compute Extrapolated Pilot Corpus Size                   #
#---------------------------------------------------------------------------#
register <- c(as.character(registerSizeEstimate$Register[1:3]), 'Corpus')
hcTokens <- analysis$featureMatrix$tokens
extrapolated <- pctTotal / pctTotal[4] * 5
extrapolatedTokens <- hcTokens * extrapolated / 100
extrapolatedSents <- floor(extrapolatedTokens / analysis$featureMatrix$wordsPerSent)
chunkSize <- rep(samplingUnit[[length(samplingUnit)]]$size, 4)
sentsPerChunk <- floor(samplingUnit[[length(samplingUnit)]]$size /
analysis$featureMatrix$wordsPerSent)
chunks <- floor(extrapolatedSents / sentsPerChunk)
sampleSize <- chunks * sentsPerChunk
pilot <- data.frame(register = register, hcTokens = hcTokens,
extrapolated = extrapolated,
extrapolatedTokens = extrapolatedTokens,
extrapolatedSents = extrapolatedSents,
chunkSize = chunkSize,
sentsPerChunk = sentsPerChunk,
chunks = chunks,
sampleSize = sampleSize)
names(pilot) <- c('Register', 'HC Corpus (Tokens)',
'% Total', 'Tokens', 'Sentences',
'Chunk Size (Tokens)',
'Sentences per Chunk', '# Chunks',
'Sample Size (Sentences)')
# Compute training, validation and test set sizes
Registers <- c('Blogs', 'News', 'Twitter', 'Corpus')
Validation <- pilot$`Sample Size (Sentences)`[1:3]
Test <- pilot$`Sample Size (Sentences)`[1:3]
Alpha <-  pilot$`Sample Size (Sentences)`[1:3] * 2
Beta <-  pilot$`Sample Size (Sentences)`[1:3] * c(5,2,5)
Gamma <-  pilot$`Sample Size (Sentences)`[1:3] * c(10,2,10)
Delta <-  pilot$`Sample Size (Sentences)`[1:3] * c(14,2,14)
corpusDesign <- cbind(Alpha, Beta, Gamma, Delta, Validation, Test)
ttl <- colSums(corpusDesign)
corpusDesign <- rbind(corpusDesign, ttl)
corpusDesign <- cbind(Registers, corpusDesign)
View(corpusDesign)
# Format results
design <- list(
comparison = comparison,
pilot = pilot,
corpusDesign = corpusDesign
)
View(corpusDesign)
# Compute training, validation and test set sizes
Registers <- c('Blogs', 'News', 'Twitter', 'Corpus')
Validation <- pilot$`Sample Size (Sentences)`[1:3]
Test <- pilot$`Sample Size (Sentences)`[1:3]
Alpha <-  pilot$`Sample Size (Sentences)`[1:3] * 2
Beta <-  pilot$`Sample Size (Sentences)`[1:3] * c(5,2,5)
Gamma <-  pilot$`Sample Size (Sentences)`[1:3] * c(10,2,10)
Delta <-  pilot$`Sample Size (Sentences)`[1:3] * c(14,2,14)
corpusDesign <- cbind(Alpha, Beta, Gamma, Delta, Validation, Test)
ttl <- colSums(corpusDesign)
corpusDesign <- rbind(corpusDesign, ttl)
corpusDesign <- as.data.frame(cbind(Registers, corpusDesign))
View(corpusDesign)
# Format results
design <- list(
comparison = comparison,
pilot = pilot,
corpusDesign = corpusDesign
)
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/B06.designPilot.R')
load("~/Data Science/Data Science Projects/PredictifyR-1.0/analysis/pilot-corpus-design_20170518_034031.Rdata")
# Compute training, validation and test set sizes
Registers <- c('Blogs', 'News', 'Twitter', 'Corpus')
Validation <- pilot$`Sample Size (Sentences)`[1:3]
Test <- pilot$`Sample Size (Sentences)`[1:3]
Alpha <-  pilot$`Sample Size (Sentences)`[1:3] * 2
Beta <-  pilot$`Sample Size (Sentences)`[1:3] * c(5,2,5)
Gamma <-  pilot$`Sample Size (Sentences)`[1:3] * c(10,2,10)
Delta <-  pilot$`Sample Size (Sentences)`[1:3] * c(14,2,14)
corpusDesign <- cbind(Alpha, Beta, Gamma, Delta, Validation, Test)
ttl <- colSums(corpusDesign)
corpusDesign <- rbind(corpusDesign, ttl)
corpusDesign <- data.frame(cbind(Registers, corpusDesign), stringsAsFactors=FALSE)
Registers <- c('Blogs', 'News', 'Twitter', 'Corpus')
Validation <- pilot$`Sample Size (Sentences)`[1:3]
Test <- pilot$`Sample Size (Sentences)`[1:3]
Alpha <-  pilot$`Sample Size (Sentences)`[1:3] * 2
Beta <-  pilot$`Sample Size (Sentences)`[1:3] * c(5,2,5)
Gamma <-  pilot$`Sample Size (Sentences)`[1:3] * c(10,2,10)
Delta <-  pilot$`Sample Size (Sentences)`[1:3] * c(14,2,14)
corpusDesign <- cbind(Alpha, Beta, Gamma, Delta, Validation, Test)
ttl <- colSums(corpusDesign)
corpusDesign <- rbind(corpusDesign, ttl)
corpusDesign <- as.data.frame(cbind(Registers, corpusDesign), stringsAsFactors=FALSE)
View(corpusDesign)
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/B06.designPilot.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/E0.buildCorpora.R')
load("~/Data Science/Data Science Projects/PredictifyR-1.0/analysis/pilot-corpus-design_20170518_034348.Rdata")
design$corpusDesign$Validation[r] /
design$pilot$`Sentences per Chunk`[r]
design$corpusDesign$Validation[r]
design$corpusDesign$Validation[[r]]
as.numeric(design$corpusDesign$Validation[[r]]) /
design$pilot$`Sentences per Chunk`[r]
design$pilot$`Sentences per Chunk`[r]
as.numeric(design$corpusDesign$Validation[r])
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/E0.buildCorpora.R')
load("~/Data Science/Data Science Projects/PredictifyR-1.0/analysis/pilot-corpus-design_20170518_034348.Rdata")
source('~/Data Science/Data Science Projects/PredictifyR-1.0/config/initEnvironment.R')
design <- pilotDesign
hcCorpus <- corpora$clean
training <- corpora$training
validation <- corpora$validation
test <- corpora$test
startTime <- Sys.time()
message(paste('\nBuilding Model Corpora at', startTime))
message('...reading HC Corpus and break into chunks')
korpus <- lapply(seq_along(hcCorpus$documents), function(d) {
document <- readFile(hcCorpus$documents[[d]])
chunkDocument(document, design$pilot$`Sentences per Chunk`[d])
})
r <- 1
message(paste('......processing', hcCorpus$documents[[r]]$fileDesc))
message('......creating validation set')
pool <- 1:length(korpus[[r]])
chunks <- as.numeric(design$corpusDesign$Validation[r]) /
design$pilot$`Sentences per Chunk`[r]
set.seed(0505)
valIndices <- sample(pool, chunks)
validation$documents[[r]]$data <- unlist(korpus[[r]][valIndices])
saveFile(validation$documents[[r]])
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/E0.buildCorpora.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/E0.buildCorpora.R')
design$corpusDesign$Validation[r]
load("~/Data Science/Data Science Projects/PredictifyR-1.0/analysis/sample-size-estimate_20170517_183321.Rdata")
load("~/Data Science/Data Science Projects/PredictifyR-1.0/analysis/register-sample-size-estimate_20170517_032135.Rdata")
load("~/Data Science/Data Science Projects/PredictifyR-1.0/analysis/estimate-sampling-unit_20170517_030817.Rdata")
registerSizeEstimate <- registerSampleSize
#---------------------------------------------------------------------------#
#                  Compute Estimated Pilot Corpus Size                      #
#---------------------------------------------------------------------------#
register <- c(as.character(registerSizeEstimate$Register[1:3]), 'Corpus')
hcCorpus <- analysis$featureMatrix$tokens
diversity <- as.vector(t(sampleSizeEstimate[1:4,3]))
syntactic <- round(registerSizeEstimate$'Sample Size'[1:4], 0)
tokens <- pmax(diversity, syntactic, na.rm = TRUE)
sentences <- floor(tokens[1:3] / analysis$featureMatrix$wordsPerSent[1:3])
sentences <- c(sentences, sum(sentences))
pctTotal <- round(tokens / analysis$featureMatrix$tokens[1:4] * 100, 0)
proportion <- round(tokens[1:3] / sum(tokens[1:3]) * 100, 0)
proportion <- c(proportion, 100)
comparison <- data.frame(register = register, hcCorpus = hcCorpus,
diversity = diversity,
syntactic = syntactic, tokens = tokens,
sentences = sentences, pctTotal = pctTotal,
proportion = proportion)
names(comparison) <- c('Register', 'HC Corpus', 'Diversity-Based Estimate',
'Lexical Feature-Based Estimate',
'Tokens', 'Sentences', '% Total',
'Proportion')
#---------------------------------------------------------------------------#
#                  Compute Extrapolated Pilot Corpus Size                   #
#---------------------------------------------------------------------------#
register <- c(as.character(registerSizeEstimate$Register[1:3]), 'Corpus')
hcTokens <- analysis$featureMatrix$tokens
extrapolated <- pctTotal / pctTotal[4] * 5
extrapolatedTokens <- hcTokens * extrapolated / 100
extrapolatedSents <- floor(extrapolatedTokens / analysis$featureMatrix$wordsPerSent)
chunkSize <- rep(samplingUnit[[length(samplingUnit)]]$size, 4)
sentsPerChunk <- floor(samplingUnit[[length(samplingUnit)]]$size /
analysis$featureMatrix$wordsPerSent)
chunks <- floor(extrapolatedSents / sentsPerChunk)
sampleSize <- chunks * sentsPerChunk
pilot <- data.frame(register = register, hcTokens = hcTokens,
extrapolated = extrapolated,
extrapolatedTokens = extrapolatedTokens,
extrapolatedSents = extrapolatedSents,
chunkSize = chunkSize,
sentsPerChunk = sentsPerChunk,
chunks = chunks,
sampleSize = sampleSize)
names(pilot) <- c('Register', 'HC Corpus (Tokens)',
'% Total', 'Tokens', 'Sentences',
'Chunk Size (Tokens)',
'Sentences per Chunk', '# Chunks',
'Sample Size (Sentences)')
#---------------------------------------------------------------------------#
#           Compute Model Corpora (Train, Validation, Test) Sizes           #
#---------------------------------------------------------------------------#
# Compute training, validation and test set sizes
load("~/Data Science/Data Science Projects/PredictifyR-1.0/analysis/fast-analysis-of-clean-corpus_20170517_160253.Rdata")
#---------------------------------------------------------------------------#
#                  Compute Estimated Pilot Corpus Size                      #
#---------------------------------------------------------------------------#
register <- c(as.character(registerSizeEstimate$Register[1:3]), 'Corpus')
hcCorpus <- analysis$featureMatrix$tokens
diversity <- as.vector(t(sampleSizeEstimate[1:4,3]))
syntactic <- round(registerSizeEstimate$'Sample Size'[1:4], 0)
tokens <- pmax(diversity, syntactic, na.rm = TRUE)
sentences <- floor(tokens[1:3] / analysis$featureMatrix$wordsPerSent[1:3])
sentences <- c(sentences, sum(sentences))
pctTotal <- round(tokens / analysis$featureMatrix$tokens[1:4] * 100, 0)
proportion <- round(tokens[1:3] / sum(tokens[1:3]) * 100, 0)
proportion <- c(proportion, 100)
comparison <- data.frame(register = register, hcCorpus = hcCorpus,
diversity = diversity,
syntactic = syntactic, tokens = tokens,
sentences = sentences, pctTotal = pctTotal,
proportion = proportion)
names(comparison) <- c('Register', 'HC Corpus', 'Diversity-Based Estimate',
'Lexical Feature-Based Estimate',
'Tokens', 'Sentences', '% Total',
'Proportion')
#---------------------------------------------------------------------------#
#                  Compute Extrapolated Pilot Corpus Size                   #
#---------------------------------------------------------------------------#
register <- c(as.character(registerSizeEstimate$Register[1:3]), 'Corpus')
hcTokens <- analysis$featureMatrix$tokens
extrapolated <- pctTotal / pctTotal[4] * 5
extrapolatedTokens <- hcTokens * extrapolated / 100
extrapolatedSents <- floor(extrapolatedTokens / analysis$featureMatrix$wordsPerSent)
chunkSize <- rep(samplingUnit[[length(samplingUnit)]]$size, 4)
sentsPerChunk <- floor(samplingUnit[[length(samplingUnit)]]$size /
analysis$featureMatrix$wordsPerSent)
chunks <- floor(extrapolatedSents / sentsPerChunk)
sampleSize <- chunks * sentsPerChunk
pilot <- data.frame(register = register, hcTokens = hcTokens,
extrapolated = extrapolated,
extrapolatedTokens = extrapolatedTokens,
extrapolatedSents = extrapolatedSents,
chunkSize = chunkSize,
sentsPerChunk = sentsPerChunk,
chunks = chunks,
sampleSize = sampleSize)
names(pilot) <- c('Register', 'HC Corpus (Tokens)',
'% Total', 'Tokens', 'Sentences',
'Chunk Size (Tokens)',
'Sentences per Chunk', '# Chunks',
'Sample Size (Sentences)')
#---------------------------------------------------------------------------#
#           Compute Model Corpora (Train, Validation, Test) Sizes           #
#---------------------------------------------------------------------------#
# Compute training, validation and test set sizes
sampleSizeEstimate <- sampleSizeEstimate$sampleSize
#---------------------------------------------------------------------------#
#                  Compute Estimated Pilot Corpus Size                      #
#---------------------------------------------------------------------------#
register <- c(as.character(registerSizeEstimate$Register[1:3]), 'Corpus')
hcCorpus <- analysis$featureMatrix$tokens
diversity <- as.vector(t(sampleSizeEstimate[1:4,3]))
syntactic <- round(registerSizeEstimate$'Sample Size'[1:4], 0)
tokens <- pmax(diversity, syntactic, na.rm = TRUE)
sentences <- floor(tokens[1:3] / analysis$featureMatrix$wordsPerSent[1:3])
sentences <- c(sentences, sum(sentences))
pctTotal <- round(tokens / analysis$featureMatrix$tokens[1:4] * 100, 0)
proportion <- round(tokens[1:3] / sum(tokens[1:3]) * 100, 0)
proportion <- c(proportion, 100)
comparison <- data.frame(register = register, hcCorpus = hcCorpus,
diversity = diversity,
syntactic = syntactic, tokens = tokens,
sentences = sentences, pctTotal = pctTotal,
proportion = proportion)
names(comparison) <- c('Register', 'HC Corpus', 'Diversity-Based Estimate',
'Lexical Feature-Based Estimate',
'Tokens', 'Sentences', '% Total',
'Proportion')
#---------------------------------------------------------------------------#
#                  Compute Extrapolated Pilot Corpus Size                   #
#---------------------------------------------------------------------------#
register <- c(as.character(registerSizeEstimate$Register[1:3]), 'Corpus')
hcTokens <- analysis$featureMatrix$tokens
extrapolated <- pctTotal / pctTotal[4] * 5
extrapolatedTokens <- hcTokens * extrapolated / 100
extrapolatedSents <- floor(extrapolatedTokens / analysis$featureMatrix$wordsPerSent)
chunkSize <- rep(samplingUnit[[length(samplingUnit)]]$size, 4)
sentsPerChunk <- floor(samplingUnit[[length(samplingUnit)]]$size /
analysis$featureMatrix$wordsPerSent)
chunks <- floor(extrapolatedSents / sentsPerChunk)
sampleSize <- chunks * sentsPerChunk
pilot <- data.frame(register = register, hcTokens = hcTokens,
extrapolated = extrapolated,
extrapolatedTokens = extrapolatedTokens,
extrapolatedSents = extrapolatedSents,
chunkSize = chunkSize,
sentsPerChunk = sentsPerChunk,
chunks = chunks,
sampleSize = sampleSize)
names(pilot) <- c('Register', 'HC Corpus (Tokens)',
'% Total', 'Tokens', 'Sentences',
'Chunk Size (Tokens)',
'Sentences per Chunk', '# Chunks',
'Sample Size (Sentences)')
#---------------------------------------------------------------------------#
#           Compute Model Corpora (Train, Validation, Test) Sizes           #
#---------------------------------------------------------------------------#
# Compute training, validation and test set sizes
Registers <- c('Blogs', 'News', 'Twitter', 'Corpus')
Validation <- pilot$`Sample Size (Sentences)`[1:3]
Test <- pilot$`Sample Size (Sentences)`[1:3]
Alpha <-  pilot$`Sample Size (Sentences)`[1:3] * 2
Beta <-  pilot$`Sample Size (Sentences)`[1:3] * c(5,2,5)
Gamma <-  pilot$`Sample Size (Sentences)`[1:3] * c(10,2,10)
Delta <-  pilot$`Sample Size (Sentences)`[1:3] * c(14,2,14)
corpusDesign <- cbind(Alpha, Beta, Gamma, Delta, Validation, Test)
ttl <- colSums(corpusDesign)
corpusDesign <- rbind(corpusDesign, ttl)
class(corpusDesign)
str(corpusDesign)
corpusDesign <- data.frame(Alpha, Beta, Gamma, Delta, Validation, Test)
View(corpusDesign)
str(corpusDesign)
ttl <- colSums(corpusDesign)
corpusDesign <- rbind(corpusDesign, ttl)
corpusDesign <- as.data.frame(cbind(Registers, corpusDesign), stringsAsFactors=FALSE)
str(corpusDesign)
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/B06.designPilot.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/E1.exploreTraining.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/E0.buildCorpora.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/E1.exploreTraining.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/E1.exploreTraining.R')
load("~/Data Science/Data Science Projects/PredictifyR-1.0/analysis/pilot-corpus-design_20170518_101618.Rdata")
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/B02.estimateSampleSize.R')
load("~/Data Science/Data Science Projects/PredictifyR-1.0/analysis/nGrams-for-_20170518_103602.Rdata")
load("~/Data Science/Data Science Projects/PredictifyR-1.0/analysis/nGrams-for-_20170518_103602.Rdata")
rm(nGramCorpus)
rm(nGrams)
load("~/Data Science/Data Science Projects/PredictifyR-1.0/analysis/nGrams-for-_20170518_103602.Rdata")
load("~/Data Science/Data Science Projects/PredictifyR-1.0/analysis/verify-vocabulary_20170518_120236.Rdata")
View(verifyVocabulary)
load("~/Data Science/Data Science Projects/PredictifyR-1.0/analysis/verify-vocabulary_20170518_135406.Rdata")
load("~/Data Science/Data Science Projects/PredictifyR-1.0/analysis/verify-vocabulary_20170518_135406.Rdata")
View(verifyVocabulary)
load("~/Data Science/Data Science Projects/PredictifyR-1.0/analysis/verify-vocabulary_20170518_135406.Rdata")
View(verifyVocabulary)
load("~/Data Science/Data Science Projects/PredictifyR-1.0/analysis/verify-vocabulary_20170518_130021.Rdata")
View(verifyVocabulary)
load("~/Data Science/Data Science Projects/PredictifyR-1.0/analysis/verify-vocabulary_20170518_122119.Rdata")
load("~/Data Science/Data Science Projects/PredictifyR-1.0/analysis/verify-vocabulary_20170518_120236.Rdata")
source('~/Data Science/Data Science Projects/PredictifyR-1.0/config/schema.R')
n <- 1
message(paste('\n...analyzing', validation$nGrams[[n]]$fileDesc))
training <- corpora$training
validation <- corpora$validation
message(paste('\n...analyzing', validation$nGrams[[n]]$fileDesc))
t <-1
validationNGram <- loadObject(validation$nGrams[[n]])
validationNGrams <- parallelizeTask(nGramCorpus, validation, directories)
validationNGrams <- parallelizeTask(nGramCorpus, validation, directories)
source('~/Data Science/Data Science Projects/PredictifyR-1.0/config/initEnvironment.R')
training[[t]]$nGrams[[n]]
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/D4.analyzeNGramCoverageTraining.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/D4.analyzeNGramCoverageTraining.R')
ntoken(tSetDfm)
tSetDfm <- loadObject(training[[t]]$nGrams[[n]])
validationNGram <- loadObject(validation$nGrams[[n]])
tSetNGrams <- featnames(tSetDfm)
oov <- dfm_select(validationNGram, features = tSetNGrams,
selection = 'remove', valuetype = 'fixed
oov <- dfm_select(validationNGram, features = tSetNGrams,
selection = 'remove', valuetype = 'fixed')
oovData <- list()
oovData$tSet <- training[[t]]$corpusName
oovData$nGram <- training[[t]]$nGrams[[n]]$fileDesc
oovData$tSetNGrams <- ntoken(tSetDfm)
oovData$tSet <- training[[t]]$corpusName
oovData$nGram <- training[[t]]$nGrams[[n]]$fileDesc
oovData$tSetNGrams <- sum(ntoken(tSetDfm))
oovData$validationNGrams <- sum(ntoken(validationNGram))
oovData$vOOV <- length(featnames(oov))
oovData$nOOV <- sum(ntoken(oov))
oovData$vOOVRate <- length(featnames(oov)) / length(featnames(validationNGram))
oovData$nOOVRate <- oovData$vOOV / oovData$validationNGrams
oovData$coverage <- 1 - oovData$nOOVRate
oovData$coverage <- 1 - oovData$nOOVRate * 100
oovData$coverage <- (1 - oovData$nOOVRate) * 100
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/D4.analyzeNGramCoverageTraining.R')
training <- corpora$training
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/D4.analyzeNGramCoverageTraining.R')
training <- training[[1]]
source('~/Data Science/Data Science Projects/PredictifyR-1.0/config/schema.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/config/schema.R')
training <- training[[1]]
paste0(training$objName, 'CoverageAnalysis')
training <- corpora$training[[1]]
paste0(training$objName, 'CoverageAnalysis')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/D4.analyzeNGramCoverageTraining.R')
load("~/Data Science/Data Science Projects/PredictifyR-1.0/analysis/coverage-analysis-training-set-alpha_20170519_125939.Rdata")
View(trainingSetAlphaCoverageAnalysis)
View(trainingSetAlphaCoverageAnalysis)
training
n <- 2
validationNGram <- loadObject(validation$nGrams[[n]])
message(paste('\n......loading', training$corpusName))
tSetDfm <- loadObject(training$nGrams[[n]])
tSetNGrams <- featnames(tSetDfm)
oov <- dfm_select(validationNGram, features = tSetNGrams,
selection = 'remove', valuetype = 'fixed')
validationV <- featnames(validationNGram)
testV <- featnames(tSetDfm)
oovData <- list()
oovData$tSet  <- training$corpusName
oovData$nGram <- training$nGrams[[n]]$fileDesc
oovData$tSetNGrams <- sum(ntoken(tSetDfm))
oovData$validationNGrams <- sum(ntoken(validationNGram))
oovData$vOOV <- validationV - validationV %in% testV
oovData$vOOV <- length(validationV) - length(validationV %in% testV)
length(validationV)
validationV %in% testV
length(validationV %in% testV)
length(validationV)
length((validationV %in% testV) == TRUE)
sum((validationV %in% testV) == TRUE)
length(validationV) - sum((validationV %in% testV) == TRUE)
validationN <- tokens(validationNGram)
testN <- tokens(tSetNGrams)
validationN <- tokenize(validationNGram)
validationN <- quanteda::tokenize(validationNGram)
validationN <- quanteda::tokenize(readFile(validation$documents[[n]]),
what = 'word', ngrams = n)
source('~/Data Science/Data Science Projects/PredictifyR-1.0/config/schema.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/config/schema.R')
?unique
validationNGrams <- quanteda::tokenize(readFile(validation$documents[[n]]),
what = 'word', ngrams = n)
message(paste('......tokenizing', training$corpusName))
trainingNGrams <- quanteda::tokenize(readFile(training$documents[[n]]),
what = 'word', ngrams = n)
validationV <- unique(validationNGrams)
trainingV <- unique(trainingNGrams)
sum(validationV %in% trainingV)
length(validationV)
nOOV <- length(validaetionNGrams) - sum(validationNGram %in% trainingNGrams)
nOOV <- length(validationNGrams) - sum(validationNGrams %in% trainingNGrams)
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/D4.analyzeNGramCoverageTraining.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/D4.analyzeNGramCoverageTraining.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/D4.analyzeNGramCoverageTraining.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/D4.analyzeNGramCoverageTraining.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/D4.analyzeNGramCoverageTraining.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/D4.analyzeNGramCoverageTraining.R')

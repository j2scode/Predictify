load("~/Data Science/Data Science Projects/PredictifyR-1.0/analysis/pilot-corpus-design_20170515_184645.Rdata")
design <- pilotDesign$pilot
chunks <- design$`# Chunks`[d]
d <- 1
chunks <- design$`# Chunks`[d]
chunkSize <- design$`Sentences per Chunk`[d]
filePath <- list()
filePath$directory <- clean$directory
korpus <- lapply(seq_along(registers), function(r) {
filePath$fileName <- registers[[r]]$fileName
readFile(filePath)
})
source('~/Data Science/Data Science Projects/PredictifyR-1.0/config/schema.R')
clean <- corpora$clean
filePath <- list()
filePath$directory <- clean$directory
korpus <- lapply(seq_along(registers), function(r) {
filePath$fileName <- registers[[r]]$fileName
readFile(filePath)
})
source('~/Data Science/Data Science Projects/PredictifyR-1.0/config/initEnvironment.R')
filePath <- list()
filePath$directory <- clean$directory
korpus <- lapply(seq_along(registers), function(r) {
filePath$fileName <- registers[[r]]$fileName
readFile(filePath)
})
filePath <- list()
filePath$directory <- clean$directory
korpus <- lapply(seq_along(registers), function(r) {
filePath$fileName <- registers[[r]]$fileName
readFile(filePath)
})
chunks <- design$`# Chunks`[d]
chunkSize <- design$`Sentences per Chunk`[d]
chunks <- sampleData(korpus[[d]], chunks, chunkSize, format = 'v')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/C1.buildPilot.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/C1.buildPilot.R')
filePath <- list()
filePath$directory <- file.path(pilot$directory, 'documents')
pilot$directory
pilot <- corpora$pilot
pilot$directory
filePath$directory <- file.path(pilot$directory, 'documents')
r <- 1
filePath$fileName <- registers[[r]]$fileName
document <- readFile(filePath)
posData <- tagDocument(document)
filePath$fileName <- registers[[r]]$fileName
document <- list()
document$meta$fileName <- registers[[r]]$fileName
document$meta$fileDesc <- registers[[r]]$fileDesc
document$content <- readFile(filePath)
posData <- tagDocument(document)
source('~/Data Science/Data Science Projects/PredictifyR-1.0/lib/tagDocument.R')
filePath$fileName <- registers[[r]]$fileName
document <- list()
document$meta$fileName <- registers[[r]]$fileName
document$meta$fileDesc <- registers[[r]]$fileDesc
document$content <- readFile(filePath)
posData <- tagDocument(document)
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/C2.tagPilot.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/lib/tagDocument.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/C2.tagPilot.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/C2.tagPilot.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/C2.tagPilot.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/C2.tagPilot.R')
pos$data <- posData$tags
saveFile(pos$data)
pos <- list()
pos$fileName <- registers[[r]]$fileName
saveFile(pos$data)
pos <- list()
pos$directory <- file.path(pilot$directory, 'pos')
pos$fileName <- registers[[r]]$fileName
saveFile(pos$data)
pos$data <- posData$tags
saveFile(pos)
pos$data <- unlist(posData$tags)
saveFile(pos)
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/C2.tagPilot.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/C3.verifyPilotVocabulary.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/C3.verifyPilotVocabulary.R')
hcCorpus <- corpora$clean
hc <- list(
corpusName = hcCorpus$fileDesc,
document = lapply(seq_along(registers), function(r) {
d <- list()
d$directory <- hcCorpus$directory
d$fileName <- registers[[r]]$fileName
d
})
)
hcCorpus$fileDesc
hcCorpus$corpusName
hc <- list(
corpusName = hcCorpus$corpusName,
document = lapply(seq_along(registers), function(r) {
d <- list()
d$directory <- hcCorpus$directory
d$fileName <- registers[[r]]$fileName
d
})
)
pilot$corpusName,
pilot <- corpora$pilot
pilot$corpusName,
pilot$corpusName
pc <- list(
corpusName = pilot$corpusName,
document = lapply(seq_along(registers), function(r) {
d <- list()
d$directory <- pilot$directory
d$fileName <- registers[[r]]$fileName
d
})
)
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/C4.verifyPilotLinguistics.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/C4.verifyPilotLinguistics.R')
pilotData <- list(
corpusName = pilot$corpusName,
documents = lapply(seq_along(registers), function(r) {
d <- list()
d$directory <- file.path(pilot$directory, 'documents')
d$fileName <- registers[[r]]$fileName
d
})
)
pilotDataAnalysis <- parallelizeTask(analyzeData, 'fast', pilotData, registers, referenceFiles, directories, regexPatterns)
korpus$documents <- lapply(seq_along(registers), function(r) {
registers[[r]]$fileName
})
korpus <- corpora$pilot
korpus$directory <- file.path(korpus$directory, 'documents')
korpus$documents <- lapply(seq_along(registers), function(r) {
korpus$documents <- lapply(seq_along(registers), function(r) {
registers[[r]]$fileName
})
korpus <- corpora$pilot
korpus$documents <- lapply(seq_along(registers), function(r) {
d <- list()
d$directory <- file.path(korpus$directory, 'documents')
d$fileName <- registers[[r]]$fileName
d
})
korpus$documents <- lapply(seq_along(registers), function(r) {
d <- list()
d$directory <- file.path(korpus$directory, 'documents')
d$fileName <- registers[[r]]$fileName
d$fileDesc <- registers[[r]]$fileDesc
d
})
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/D0.analyzeDiversity.R')
load("~/Data Science/Data Science Projects/PredictifyR-1.0/analysis/pilot-corpus-design_20170515_184645.Rdata")
pd <- pilotDesign$pilot
View(pd)
pc <- pilotDesign$comparison
View(pc)
register <- c(as.character(featureEstimate$Register[1:3]), 'Corpus')
hcCorpus <- analysis$featureMatrix$tokens
diversity <- as.vector(t(vocabularyEstimate[1:4,3]))
syntactic <- round(featureEstimate$'Sample Size'[1:4], 0)
tokens <- pmax(diversity, syntactic, na.rm = TRUE)
sentences <-round(tokens[1:3] / analysis$featureMatrix$wordsPerSent[1:3], 0)
sentences <- c(sentences, sum(sentences))
pctTotal <- round(tokens / analysis$featureMatrix$tokens[1:4] * 100, 0)
proportion <- round(tokens[1:3] / sum(tokens[1:3]) * 100, 0)
proportion <- c(proportion, 100)
comparison <- data.frame(register = register, hcCorpus = hcCorpus,
diversity = diversity,
syntactic = syntactic, tokens = tokens,
sentences = sentences, pctTotal = pctTotal,
proportion = proportion)
names(comparison) <- c('Register', 'HC Corpus', 'Diversity-Based Estimate',
'Lexical Feature-Based Estimate',
'Tokens', 'Sentences', '% Total',
'Proportion')
register <- c(as.character(featureEstimate$Register[1:3]), 'Corpus')
hcTokens <- analysis$featureMatrix$tokens
extrapolated <- pctTotal / pctTotal[4] * 5
extrapolatedTokens <- hcTokens * extrapolated / 100
extrapolatedSents <- ceiling(extrapolatedTokens / analysis$featureMatrix$wordsPerSent)
chunkSize <- rep(samplingUnit[[length(samplingUnit)]]$size, 4)
sentsPerChunk <- ceiling(samplingUnit[[length(samplingUnit)]]$size /
analysis$featureMatrix$wordsPerSent)
chunks <- ceiling(extrapolatedSents / sentsPerChunk)
sampleSize <- chunks * sentsPerChunk
pilot <- data.frame(register = register, hcTokens = hcTokens,
extrapolated = extrapolated,
extrapolatedTokens = extrapolatedTokens,
extrapolatedSents = extrapolatedSents,
chunkSize = chunkSize,
sentsPerChunk = sentsPerChunk,
chunks = chunks,
sampleSize = sampleSize)
names(pilot) <- c('Register', 'HC Corpus (Tokens)',
'% Total', 'Tokens', 'Sentences',
'Chunk Size (Tokens)',
'Sentences per Chunk', '# Chunks',
'Sample Size (Sentences)')
baseSize <- pilot$`Sample Size (Sentences)`
trainingSetSizes <- baseSize * mutipliers
multipliers <- c(2,4,7,10)
trainingSetSizes <- baseSize * multipliers
trainingSetSizes <- apply(multipliers, baseSize)
trainingSetSizes <- crossprod(baseSize, multipliers)
View(trainingSetSizes)
trainingSetSizes <- tcrossprod(baseSize, multipliers)
View(trainingSetSizes)
multipliers <- c(1, 2,4,7,10)
trainingSetSizes <- tcrossprod(baseSize, multipliers)
View(trainingSetSizes)
baseSize <- pilot$`Sample Size (Sentences)`
validationSet <- baseSize
testSet <- baseSize
multipliers <- c(2,4,7,10)
trainingSets <- tcrossprod(baseSize, multipliers)
corporaDesign <- cbind(trainingSets, validationSet, testSet)
View(corporaDesign)
colSums(corporaDesign)
rowSums(corporaDesign)
corporaDesign <- cbind(corporaDesign, total = rowSums(corporaDesign)
)
View(comparison)
View(corporaDesign)
baseSize <- pilot$`Sample Size (Sentences)`
validationSet <- baseSize
testSet <- baseSize
multipliers <- c(2,4,7,10)
trainingSets <- tcrossprod(baseSize, multipliers)
corporaDesign <- cbind(trainingSets, validationSet, testSet)
View(corporaDesign)
length(corpusDesign)
length(corporaDesign)
ncol(corporaDesign)
blogsTwitterRatio <- comparison$Proportion[1] / comparison$Proportion[3]
blogTwitterTotal <- comparison$Proportion[1]  + comparison$Proportion[3]
blogProportion <- comparison$Proportion[1] / blogTwitterTotal
names(corpusDesign) <- c('Alpha', 'Beta', 'Gamma', 'Delta', 'Validation', 'Test')
names(corporaDesign) <- c('Alpha', 'Beta', 'Gamma', 'Delta', 'Validation', 'Test')
View(corporaDesign)
corporaDesign <- as.data.frame(cbind(trainingSets, validationSet, testSet))
View(corporaDesign)
names(corporaDesign) <- c('Alpha', 'Beta', 'Gamma', 'Delta', 'Validation', 'Test')
View(corporaDesign)
hcCorpus <- comparison$`HC Corpus`
blogTwitterTotal <- comparison$Proportion[1]  + comparison$Proportion[3]
blogProportion <- comparison$Proportion[1] / blogTwitterTotal
hcCorpus <- comparison$`HC Corpus`
for (i in 1:ncol(corporaDesign)) {
for (j in 1:nrow(corporaDesign)) {
if (corpusDesign[i,j] >= hcCorpus[j]) {
corpusDesign[i,j] = floor(hcCorpus[j] / chunkSize[j] * sentsPerChunk[j])
}
}
}
for (i in 1:ncol(corporaDesign)) {
for (j in 1:nrow(corporaDesign)) {
if (corporaDesign[i,j] >= hcCorpus[j]) {
corporaDesign[i,j] = floor(hcCorpus[j] / chunkSize[j] * sentsPerChunk[j])
}
}
}
hcCorpus <- comparison$`HC Corpus`
hcCorpus[j]
for (i in 1:ncol(corporaDesign)) {
for (j in 1:(nrow(corporaDesign)-1)) {
if (corporaDesign[i,j] * chunkSize[j] * sentsPerChunk[j] >= hcCorpus[j]) {
corporaDesign[i,j] = floor(hcCorpus[j] / chunkSize[j] * sentsPerChunk[j])
}
}
}
nrow(corporaDesign)-1)
nrow(corporaDesign)-1
for (i in 1:ncol(corporaDesign)) {
for (j in 1:(nrow(corporaDesign)-1)) {
if (corporaDesign[i,j] * chunkSize[j] * sentsPerChunk[j] >= hcCorpus[j]) {
corporaDesign[i,j] = floor(hcCorpus[j] / chunkSize[j] * sentsPerChunk[j])
}
}
}
for (i in 1:ncol(corporaDesign)) {
for (j in 1:(nrow(corporaDesign)-1)) {
if ((corporaDesign[i,j] * chunkSize[j] * sentsPerChunk[j]) >= hcCorpus[j]) {
corporaDesign[i,j] = floor(hcCorpus[j] / chunkSize[j] * sentsPerChunk[j])
}
}
}
i <- 1
j <- 1
tokens <- corporaDesign[i,j] * chunkSize[j] * sentsPerChunk[j]
tokens <- corporaDesign[i,j] * analysis$featureMatrix$wordsPerSent[j]
analysis$featureMatrix$wordsPerSent[j]
corporaDesign[i,j]
validationSet <- pilot$`Sample Size (Sentences)`
testSet <- pilot$`Sample Size (Sentences)`
multipliers <- c(2,4,7,10)
trainingSets <- tcrossprod(pilot$`Sample Size (Sentences)`, multipliers)
# Format Corpus Design
corporaDesign <- as.data.frame(cbind(trainingSets, validationSet, testSet))
names(corporaDesign) <- c('Alpha', 'Beta', 'Gamma', 'Delta', 'Validation', 'Test')
View(corporaDesign)
View(corporaDesign)
corporaDesign[i,j]
round(analysis$featureMatrix$wordsPerSent[j],0)
tokens <- corporaDesign[i,j] * round(analysis$featureMatrix$wordsPerSent[j],0)
for (i in 1:ncol(corporaDesign)) {
for (j in 1:(nrow(corporaDesign)-1)) {
tokens <- corporaDesign[i,j] * round(analysis$featureMatrix$wordsPerSent[j],0)
if (tokens >= hcCorpus[j]) {
corporaDesign[i,j] = floor(hcCorpus[j] / chunkSize[j] * sentsPerChunk[j])
}
}
}
for (i in 1:(nrow(corporaDesign)-1)) {
for (j in 1:ncol(corporaDesign)) {
tokens <- corporaDesign[i,j] * round(analysis$featureMatrix$wordsPerSent[i],0)
if (tokens >= hcCorpus[j]) {
corporaDesign[i,j] = floor(hcCorpus[i] / chunkSize[i] * sentsPerChunk[i])
}
}
}
validationSet <- pilot$`Sample Size (Sentences)`
testSet <- pilot$`Sample Size (Sentences)`
multipliers <- c(2,4,7,10)
trainingSets <- tcrossprod(pilot$`Sample Size (Sentences)`, multipliers)
# Format Corpus Design
corporaDesign <- as.data.frame(cbind(trainingSets, validationSet, testSet))
names(corporaDesign) <- c('Alpha', 'Beta', 'Gamma', 'Delta', 'Validation', 'Test')
# Correct for over allocation
blogTwitterTotal <- comparison$Proportion[1]  + comparison$Proportion[3]
blogProportion <- comparison$Proportion[1] / blogTwitterTotal
hcCorpus <- comparison$`HC Corpus`
for (i in 1:(nrow(corporaDesign)-1)) {
for (j in 1:ncol(corporaDesign)) {
tokens <- corporaDesign[i,j] * round(analysis$featureMatrix$wordsPerSent[i],0)
if (tokens >= hcCorpus[i]) {
corporaDesign[i,j] = floor(hcCorpus[i] / chunkSize[i] * sentsPerChunk[i])
}
}
}
View(corporaDesign)
analysis$featureMatrix$wordsPerSent
i <- 2
j <- 3
validationSet <- pilot$`Sample Size (Sentences)`
testSet <- pilot$`Sample Size (Sentences)`
multipliers <- c(2,4,7,10)
trainingSets <- tcrossprod(pilot$`Sample Size (Sentences)`, multipliers)
# Format Corpus Design
corporaDesign <- as.data.frame(cbind(trainingSets, validationSet, testSet))
names(corporaDesign) <- c('Alpha', 'Beta', 'Gamma', 'Delta', 'Validation', 'Test')
# Correct for over allocation
blogTwitterTotal <- comparison$Proportion[1]  + comparison$Proportion[3]
blogProportion <- comparison$Proportion[1] / blogTwitterTotal
hcCorpus <- comparison$`HC Corpus`
corporaDesign[i,j]
tokens <- corporaDesign[i,j] * round(analysis$featureMatrix$wordsPerSent[i],0)
tokens >= hcCorpus[i]
hcCorpus[i]
chunkSize[i]
sentsPerChunk[i]
chunkSize[i] * sentsPerChunk[i]
analysis$featureMatrix$wordsPerSent[i]
floor(hcCorpus[i] / chunkSize[i] * sentsPerChunk[i])
validationSet <- pilot$`Sample Size (Sentences)`[1:3]
testSet <- pilot$`Sample Size (Sentences)`[1:3]
multipliers <- c(2,4,7,10)
trainingSets <- tcrossprod(pilot$`Sample Size (Sentences)`[1:3], multipliers)
# Format Corpus Design
corporaDesign <- as.data.frame(cbind(trainingSets, validationSet, testSet))
View(corporaDesign)
corporaDesign <- rbind(corporaDesign, colSums(corporaDesign))
View(corporaDesign)
names(corporaDesign) <- c('Alpha', 'Beta', 'Gamma', 'Delta', 'Validation', 'Test')
# Correct for over allocation
blogTwitterTotal <- comparison$Proportion[1]  + comparison$Proportion[3]
blogProportion <- comparison$Proportion[1] / blogTwitterTotal
hcCorpus <- comparison$`HC Corpus`
for (i in 1:(nrow(corporaDesign)-1)) {
for (j in 1:ncol(corporaDesign)) {
tokens <- corporaDesign[i,j] * round(analysis$featureMatrix$wordsPerSent[i],0)
if (tokens >= hcCorpus[i]) {
corporaDesign[i,j] = floor(hcCorpus[i] / chunkSize[i] * sentsPerChunk[i])
}
}
}
View(corporaDesign)
View(corporaDesign)
totals <- colSums(corporaDesign[1:3,])
shortfall <- corporaDesign[4,] - totals
View(shortfall)
shortfallBlogs <- shortfall * blogProportion
shortfallTwitter <- shortfall -shortfallBlogs
View(shortfallBlogs)
View(shortfallTwitter)
blogAdjustment <- floor(shortfallBlogs /  sentsPerChunk[i])
shortfall <- as.vector(corporaDesign[4,] - totals)
totals <- as.vector(colSums(corporaDesign[1:3,]))
shortfall <- as.vector((corporaDesign[4,] - totals))
shortfall <- as.vector(corporaDesign[4,]) - totals))
shortfall <- as.vector(corporaDesign[4,]) - totals)
shortfall <- (as.vector(corporaDesign[4,]) - totals)
shortfall <- as.vector(corporaDesign[4,]) - totals
shortfall <- as.vector(corporaDesign[4,]) - as.vector(totals)
blogAdjustment <- floor(shortfallBlogs /  sentsPerChunk[1])
twitAdjustment <- floor(shortfallTwitter /  sentsPerChunk[3])
View(twitAdjustment)
View(twitAdjustment)
shortfall <- corporaDesign[4,] - totals
shortfallBlogs <- shortfall * blogProportion
shortfallTwitter <- shortfall - shortfallBlogs
blogAdjustment <- floor(shortfallBlogs /  chunkSize[1] * sentsPerChunk[1])
twitAdjustment <- floor(shortfallTwitter /  chunkSize[3] * sentsPerChunk[3])
View(twitAdjustment)
blogAdjustment <- floor(shortfallBlogs /  sentsPerChunk[1] * sentsPerChunk[1]
twitAdjustment <- floor(shortfallTwitter / sentsPerChunk[3]) * sentsPerChunk[3]
blogAdjustment <- floor(shortfallBlogs /  sentsPerChunk[1] * sentsPerChunk[1])
twitAdjustment <- floor(shortfallTwitter / sentsPerChunk[3]) * sentsPerChunk[3])
shortfall * blogProportion
blogAdjustment <- floor(shortfallBlogs /  sentsPerChunk[1] * sentsPerChunk[1])
floor(shortfallBlogs /  sentsPerChunk[1] * sentsPerChunk[1])
shortfall - shortfallBlogs
floor(shortfallTwitter / sentsPerChunk[3] * sentsPerChunk[3])
corporaDesign[4,] - totals
blogTwitterTotal <- comparison$Proportion[1]  + comparison$Proportion[3]
blogProportion <- comparison$Proportion[1] / blogTwitterTotal
totals <- colSums(corporaDesign[1:3,])
shortfall <- corporaDesign[4,] - totals
shortfallBlogs <- shortfall * blogProportion
shortfallTwitter <- shortfall - shortfallBlogs
blogAdjustment <- floor(shortfallBlogs /  sentsPerChunk[1] * sentsPerChunk[1])
twitAdjustment <- floor(shortfallTwitter / sentsPerChunk[3] * sentsPerChunk[3])
corporaDesign[1,3] <- corporaDesign[1,3] + blogAdjustment$Gamma
corporaDesign[1,3] <- corporaDesign[1,3] + blogAdjustment$Delta
corporaDesign[3,3] <- corporaDesign[3,3] + twitAdjustment$Gamma
corporaDesign[3,3] <- corporaDesign[3,3] + blogAdjustment$Delta
View(corporaDesign)
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/C0.designPilot.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/C0.designPilot.R')
titles <- c('Blogs Register', 'News Register', 'Twitter Register', 'Corpus')
corporaDesign <- cbind(titles, corporaDesign)
# Compute training, validation and test set sizes
validationSet <- pilot$`Sample Size (Sentences)`[1:3]
testSet <- pilot$`Sample Size (Sentences)`[1:3]
multipliers <- c(2,4,7,10)
trainingSets <- tcrossprod(pilot$`Sample Size (Sentences)`[1:3], multipliers)
# Format Corpus Design
corporaDesign <- as.data.frame(cbind(trainingSets, validationSet, testSet))
corporaDesign <- rbind(corporaDesign, colSums(corporaDesign))
names(corporaDesign) <- c('Alpha', 'Beta', 'Gamma', 'Delta', 'Validation', 'Test')
# Correct for over allocation
hcCorpus <- comparison$`HC Corpus`
for (i in 1:(nrow(corporaDesign)-1)) {
for (j in 1:ncol(corporaDesign)) {
tokens <- corporaDesign[i,j] * round(analysis$featureMatrix$wordsPerSent[i],0)
if (tokens >= hcCorpus[i]) {
corporaDesign[i,j] = floor(hcCorpus[i] / chunkSize[i] * sentsPerChunk[i])
}
}
}
# Reallocate shortfalls
blogTwitterTotal <- comparison$Proportion[1]  + comparison$Proportion[3]
blogProportion <- comparison$Proportion[1] / blogTwitterTotal
totals <- colSums(corporaDesign[1:3,])
shortfall <- corporaDesign[4,] - totals
shortfallBlogs <- shortfall * blogProportion
shortfallTwitter <- shortfall - shortfallBlogs
blogAdjustment <- floor(shortfallBlogs /  sentsPerChunk[1] * sentsPerChunk[1])
twitAdjustment <- floor(shortfallTwitter / sentsPerChunk[3] * sentsPerChunk[3])
corporaDesign[1,3] <- corporaDesign[1,3] + blogAdjustment$Gamma
corporaDesign[1,3] <- corporaDesign[1,3] + blogAdjustment$Delta
corporaDesign[3,3] <- corporaDesign[3,3] + twitAdjustment$Gamma
corporaDesign[3,3] <- corporaDesign[3,3] + blogAdjustment$Delta
Registers <- c('Blogs Register', 'News Register', 'Twitter Register', 'Corpus')
corporaDesign <- cbind(titles, corporaDesign)
View(corporaDesign)
# Compute training, validation and test set sizes
validationSet <- pilot$`Sample Size (Sentences)`[1:3]
testSet <- pilot$`Sample Size (Sentences)`[1:3]
multipliers <- c(2,4,7,10)
trainingSets <- tcrossprod(pilot$`Sample Size (Sentences)`[1:3], multipliers)
# Format Corpus Design
corporaDesign <- as.data.frame(cbind(trainingSets, validationSet, testSet))
corporaDesign <- rbind(corporaDesign, colSums(corporaDesign))
names(corporaDesign) <- c('Alpha', 'Beta', 'Gamma', 'Delta', 'Validation', 'Test')
# Correct for over allocation
hcCorpus <- comparison$`HC Corpus`
for (i in 1:(nrow(corporaDesign)-1)) {
for (j in 1:ncol(corporaDesign)) {
tokens <- corporaDesign[i,j] * round(analysis$featureMatrix$wordsPerSent[i],0)
if (tokens >= hcCorpus[i]) {
corporaDesign[i,j] = floor(hcCorpus[i] / chunkSize[i] * sentsPerChunk[i])
}
}
}
# Reallocate shortfalls
blogTwitterTotal <- comparison$Proportion[1]  + comparison$Proportion[3]
blogProportion <- comparison$Proportion[1] / blogTwitterTotal
totals <- colSums(corporaDesign[1:3,])
shortfall <- corporaDesign[4,] - totals
shortfallBlogs <- shortfall * blogProportion
shortfallTwitter <- shortfall - shortfallBlogs
blogAdjustment <- floor(shortfallBlogs /  sentsPerChunk[1] * sentsPerChunk[1])
twitAdjustment <- floor(shortfallTwitter / sentsPerChunk[3] * sentsPerChunk[3])
corporaDesign[1,3] <- corporaDesign[1,3] + blogAdjustment$Gamma
corporaDesign[1,3] <- corporaDesign[1,3] + blogAdjustment$Delta
corporaDesign[3,3] <- corporaDesign[3,3] + twitAdjustment$Gamma
corporaDesign[3,3] <- corporaDesign[3,3] + blogAdjustment$Delta
Registers <- c('Blogs Register', 'News Register', 'Twitter Register', 'Corpus')
corporaDesign <- cbind(Registers, corporaDesign)
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/C0.designPilot.R')

strsplit(korpus$corpusName, ' ')
'Training' %in% strsplit(korpus$corpusName, ' ')[[1]]
message(paste('......creating', korpus$processed$pos[[x]]$fileDesc))
paste0('nGramSummary', korpus$objName)
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/E2.createNGrams.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/D4.analyzeNGramCoverageTraining.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/E2.createNGrams.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/config/schema.R')
model <- lm$mkn4
message(paste('\nExecuting', model$args$mName, 'at', startTime))
message(paste('\nExecuting', model$args$mDesc, 'at', startTime))
source('~/Data Science/Data Science Projects/PredictifyR-1.0/config/schema.R')
model <- lm$mkn4
test <- corpora$validation
load("~/Data Science/Data Science Projects/PredictifyR-1.0/analysis/coverage-analysis-training-set-delta_20170520_024623.Rdata")
load("~/Data Science/Data Science Projects/PredictifyR-1.0/analysis/fast-analysis-of-_20170520_015831.Rdata")
View(trainingSetDeltaCoverageAnalysis)
View(trainingSetDeltaCoverageAnalysis)
load("~/Data Science/Data Science Projects/PredictifyR-1.0/analysis/coverage-analysis-training-set-gamma_20170520_023025.Rdata")
source('~/Data Science/Data Science Projects/PredictifyR-1.0/config/schema.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/config/schema.R')
training <- corpora$training$alpha
metaData <- corpora$training$alpha
metaData <- corpora$clean
korpus <- lapply(seq_along(metaData$documents), function(d) {
k <- list()
k$directory <- metaData$documents[[d]]$directory
k$fileName <- metaData$documents[[d]]$fileName
k$fileDesc <- metaData$documents[[d]]$fileDesc
k$data <- readFile(metaData$documents[[d]])
k
})
document <- unlist(korpus$data)
document <- unlist(lapply(seq_along(korpus), function(d) {
korpus[[d]]$data
}))
Corpus <- metaData$corpusName
Corpus <- metaData$corpusName
Words <- tokens(document)
Words <- ntoken(document)
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/D4.analyzeNGramCoverageTraining.R')
coverageSummary <- list(
alpha <- coverageTrainingAlpha,
beta <- coverageTrainingBeta,
gamma <- coverageTrainingGamma,
delta <- coverageTrainingDelta
)
source('~/Data Science/Data Science Projects/PredictifyR-1.0/config/schema.R')
training <- corpora$training$gamma
message(paste("\nPreprocessing", training$corpusName, 'at', startTime))
training <- corpora$training
s <- 1
test$processed[[model$args$mOrder-2]]
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM00.mknPipeline.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/E0.preprocessCorpora.R')
test <- corpora$test
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/E0.preprocessCorpora.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/E0.preprocessCorpora.R')
load("~/Data Science/Data Science Projects/PredictifyR-1.0/analysis/preprocess-corpora_20170520_043537.Rdata")
View(preprocessCorpora)
training <- corpora$training$alpha
validation <- corpora$validation
test <- corpora$test
oov <- oovCorpora(training, validation, test)
View(oov)
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/E0.preprocessCorpora.R')
training <- corpora$training
c <- 1
corpora <- list(
alpha <- training$alpha,
beta <- training$beta,
gamma <- training $gamma,
delta <- training$delta,
validation <- validation,
test <- test
)
document <- unlist(lapply(seq_along(corpora[[c]]$documents), function(d) {
readFile(corpora[[c]]$documents[[d]])
}))
names(document) <- corpora[[c]]$corpusName
document[1]
document[2]
document[3]
document <- unlist(lapply(seq_along(corpora[[c]]$documents), function(d) {
readFile(corpora[[c]]$documents[[d]])
}))
korpus <- quanteda::corpus(document)
stats <- summary(korpus)
View(stats)
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/C6.summarizeCorpora.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/C6.summarizeCorpora.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM00.mknPipeline.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/C6.summarizeCorpora.R')
training[[s]]
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/E0.preprocessCorpora.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/E0.preprocessCorpora.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/config/initEnvironment.R')
korpora <- corpora$training$alpha
korpus <- corpora$training$alpha
path <- corpora$training$alpha$processed$words
document <- readFile(path[[1]])
load("~/Data Science/Data Science Projects/PredictifyR-1.0/analysis/model-corpora-descriptive-statistics_20170520_133109.Rdata")
View(corporaSummary)
View(corporaSummary)
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM00.mknPipeline.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/E2.createNGrams.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM00.mknPipeline.R')
document <- readFile(korpus$processed$words[[1]])
x <- 1
message(paste('......creating', korpus$processed$words[[x]]$fileDesc, 'sentence boundaries'))
annotatedDoc <- unlist(lapply(seq_along(document), function(s) {
paste(paste0(rep("BOS", times = s-1), collapse = " "), document[s], "EOS", collapse = " ")
}))
s <- 1
paste(paste0(rep("BOS", times = s-1), collapse = " "), document[s], "EOS", collapse = " ")
annotatedDoc <- lapply(seq_along(document), function(s) {
paste(paste0(rep("BOS", times = s-1), collapse = " "), document[s], "EOS", collapse = " ")
})
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/E1.processCorpus.R')
Q
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/E1.processCorpus.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/E1.processCorpus.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/E2.createNGrams.R')
mkn <- lm$mkn4
nGrams <- corpora$training$alpha$nGrams$words
regex <- regexPatterns
x <- 1
message(paste('...initializing', mkn$args$counts[[x]]$fileDesc))
nGram <- loadObject(nGrams[[x]])
counts <- data.table(nGram = featnames(nGram), key = 'nGram')
bosGram <- paste0(rep("BOS", x), collapse = ' ')
counts <- rbindlist(list(counts, list(bosGram)))
mkn$args$counts[[x]]$data <- counts
saveObject(mkn$args$counts[[x]])
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM01.mknInit.R')
x <- 2
message(paste('...initializing', mkn$args$counts[[x]]$fileDesc))
nGram <- loadObject(nGrams[[x]])
counts <- data.table(nGram = featnames(nGram), key = 'nGram')
# Add n BOSs where n is the order of the nGram.
bosGram <- paste0(rep("BOS", x), collapse = ' ')
counts <- rbindlist(list(counts, list(bosGram)))
if (x > 1) {
context <- gsub(regex$context[[x-1]], "\\1", counts$nGram, perl = TRUE)
suffix  <- gsub(regex$suffix[[x-1]], "\\1", counts$nGram, perl = TRUE)
counts[, c('context', 'suffix') := list(context, suffix)]
}
mkn$args$counts[[x]]$data <- counts
saveObject(mkn$args$counts[[x]])
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM00.mknPipeline.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/config/schema.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/config/schema.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/config/schema.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/E0.preprocessCorpora.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/D4.analyzeNGramCoverageTraining.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/config/schema.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/D4.analyzeNGramCoverageTraining.R')
load("~/Data Science/Data Science Projects/PredictifyR-1.0/logs/log.Rdata")
View(log)
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM00.mknPipeline.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM00.mknPipeline.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM00.mknPipeline.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM00.mknPipeline.R')
training <- corpora$training$alpha
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM00.mknPipeline.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/config/schema.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/config/schema.R')
test <- corpora$validation
message(paste('...processing OOV for', test$fileDesc, 'complete'))
test <- corpora$test$processed[[1]]
message(paste('...processing OOV for', test$fileDesc, 'complete'))
source('~/Data Science/Data Science Projects/PredictifyR-1.0/config/schema.R')
test <- corpora$test$processed[[1]]
message(paste('...processing OOV for', test$fileDesc, 'complete'))
source('~/Data Science/Data Science Projects/PredictifyR-1.0/config/schema.R')
test <- corpora$test$processed[[1]]
message(paste('...processing OOV for', test[[1]]$fileDesc, 'complete'))
message(paste('...processing OOV for', test[[1]]$fileDesc))
test$documents
validation <- corpora$validation
test <- corpora$test
message(paste('...processing OOV for', test$corpusName))
message(paste('...processing OOV for', test$corpusName, test$processed[[1]]$fileDesc))
message(paste('...processing OOV for', test$corpusName, test$processed[[1]][[1]]$fileDesc))
tSet <- 3
message(paste('...processing OOV for', test$corpusName, test$processed[[tSet]][[1]]$fileDesc))
test$processed[[tSet]]
tSet <- alpha
tSet <- 'alpha'
tSet <- 2
message(paste('...processing OOV for', test$corpusName, tSets[[tSet]], test$processed[[tSet]][[1]]$fileDesc))
tSets <- list('alpha', 'beta', 'gamma', 'delta')
message(paste('...processing OOV for', test$corpusName, tSets[[tSet]], test$processed[[tSet]][[1]]$fileDesc))
training <- corpora$training$alpha
message(paste('...processing OOV for', training$corpusName))
source('~/Data Science/Data Science Projects/PredictifyR-1.0/config/schema.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/E0.preprocessCorpora.R')
training <- corpora$training
t <- 1
validation <- corpora$validation
validation[[t]]$corpusName
validation$corpusName
source('~/Data Science/Data Science Projects/PredictifyR-1.0/config/schema.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/E1.processCorpus.R')
validation <- corpora$validation
training <- corpora$training
source('~/Data Science/Data Science Projects/PredictifyR-1.0/config/schema.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/C5.buildCorpora.R')
test <- corpora$validation[[1]]
message(paste('...processing OOV for', test$corpusName))
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/C5.buildCorpora.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/E0.preprocessCorpora.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM00.mknPipeline.R')
validation[[t]]$documents[[r]]$data
r <- 1
validation[[t]]$documents[[r]]$data
validation[[t]]$documents[[r]]
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/C5.buildCorpora.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/C6.summarizeCorpora.R')
test <- corpora$validation[[1]]
test$processed[[1]]
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/E0.preprocessCorpora.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/E0.preprocessCorpora.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/E0.preprocessCorpora.R')
korpora <- corpora$training
k <- 1
message(paste("\nPreparing nGrams for", korpora[[k]]$corpusName, 'at', startTime))
message(paste("\nPreparing nGrams for", korpora[[k]]$corpusName, 'at', Sys.time()))
x <- 1
message(paste('...creating', korpus[[k]]$processed[[x]]$fileDesc))
message(paste('...creating', korpora[[k]]$processed[[x]]$fileDesc))
message(paste('...creating', korpora[[k]]$corpusName,
korpora[[k]]$processed[[x]]$fileDesc))
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/E2.createNGrams.R')
message(paste('\nProcessing', training[[t]]$corpusName, 'at', Sys.time()))
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/E1.processCorpora.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/E2.createNGrams.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/config/schema.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/config/schema.R')
mkn$counts[[x]]$fileDesc
mkn <- lm$mkn
source('~/Data Science/Data Science Projects/PredictifyR-1.0/config/schema.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/config/schema.R')
load("~/Data Science/Data Science Projects/PredictifyR-1.0/analysis/MKN-analysis-package-10-pct_20170524_041739.Rdata")
load("~/Data Science/Data Science Projects/PredictifyR-1.0/analysis/MKN-analysis-package-35-pct_20170524_035022.Rdata")
model <- lm$mkn
startTime <- Sys.time()
message(paste('\nExecuting', model$mDesc, 'at', startTime))
training <- corpora$training
test <- corpora$validation
t <- 1
message(paste('\n...modeling', training[[t]]$corpusName))
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM00.mknPipeline.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/config/schema.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/config/schema.R')
load("~/Data Science/Data Science Projects/PredictifyR-1.0/lm/mkn4/mkn4-discounts.Rdata")
View(mkn4Discounts)
load("~/Data Science/Data Science Projects/PredictifyR-1.0/lm/mkn4/mkn4-ngram-count-summary.Rdata")
View(mkn4NGramCountSummary)
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM09.mknEstimate.R')
load("~/Data Science/Data Science Projects/PredictifyR-1.0/lm/mkn/mkn-discounts.Rdata")
View(mkn4Discounts)
View(mknDiscounts)
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM08.mknLambda.R')
load("~/Data Science/Data Science Projects/PredictifyR-1.0/analysis/quintgrams.txt-estimates_20170525_001741.Rdata")
training <- corpora$training$alpha
training$fileName
mkn <- lm$mkn
training <- corpora$training$alpha
test <- corpora$validation$alpha
training <- corpora$training$alpha[[5]]
test <- corpora$validation$alpha[[5]]
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM00.mknPipeline.R')
document <- readFile(test)
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM09.mknEstimate.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/config/initEnvironment.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM09.mknEstimate.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/config/initEnvironment.R')
document <- readFile(test)
test <- corpora$validation$alpha$processed[[5]]
document <- readFile(test)
tokens <- tokenize(document, what = 'word', remove = 'BOS')
tokens <- tokenize(document, what = 'word', tolower = FALSE)
testDfm <- quanteda::dfm(tokens, tolower = FALSE, remove = 'BOS')
N <- ntoken(testDfm)
N <- sum(ntoken(testDfm))
document <- sampleData(document, numChunks = sents, chunkSize = 1, format = 'v')
sents <- 1000
document <- sampleData(document, numChunks = sents, chunkSize = 1, format = 'v')
M <- length(document)
tokens <- tokenize(document, what = 'word')
testDfm <- quanteda::dfm(tokens, tolower = FALSE, remove = 'BOS')
N <- sum(ntoken(testDfm))
x <- 1
s <- scoreSentence(document[x])
scoreSentence <- function(sentence) {
tokens <- unlist(quanteda::tokenize(sentence, what = 'word'))
rbindlist(lapply(seq_along(tokens[1:(length(tokens)-4)]), function(x) {
nGram <- list()
nGram$quintgram <- paste0(tokens[x:(x+4)], collapse = ' ')
nGram$logProb <- score(tokens[x:(x+4)])
nGram
}))
}
s <- scoreSentence(document[x])
# Function that performs the quadgram probability estimate
score <- function(quintgram) {
# Calculate unigram probability, p1
s <- quintgram[5]
n1p <- model[[1]][ nGram == s][,c(cKN, norm)]
p1 <- as.numeric(n1p[1] / n1p[2]) + 1 / V
# Calculate bigram probability, p2
sfx <- paste0(quintgram[4:5], collapse = ' ')
cntx <- quintgram[4]
alpha <- max(0,model[[2]][ nGram == sfx][, alpha])
lambda <- model[[1]][ nGram == cntx][, lambda]
if (length(lambda) == 0) {
lambda <- 0.8 * as.numeric(discounts[2,4] / model[[1]][, norm])
}
p2 <- alpha + lambda * p1
# Calculate trigram probability, p3
sfx <- paste0(quintgram[3:5], collapse = ' ')
cntx <- paste0(quintgram[3:4], collapse = ' ')
alpha <- max(0,model[[3]][ nGram == sfx][, alpha])
lambda <- model[[2]][ nGram == cntx][, lambda]
if (length(lambda) == 0) {
lambda <- 0.8 * as.numeric(discounts[3,4] / model[[2]][, norm])
}
p3 <- alpha + lambda * p2
# Calculate quadgram probability, p4
sfx <- paste0(quintgram[2:5], collapse = ' ')
cntx <- paste0(quintgram[2:4], collapse = ' ')
alpha <- max(0,model[[4]][ nGram == sfx][, alpha])
lambda <- model[[3]][ nGram == cntx][, lambda]
if (length(lambda) == 0) {
lambda <- 0.8 * as.numeric(discounts[4,4])
}
p4 <- alpha + lambda * p3
# Calculate quintgram probability, p5
sfx <- paste0(quintgram[1:5], collapse = ' ')
cntx <- paste0(quintgram[1:4], collapse = ' ')
alpha <- max(0,model[[5]][ nGram == sfx][, alpha])
lambda <- model[[4]][ nGram == cntx][, lambda]
if (length(lambda) == 0) {
lambda <- 0.8 * as.numeric(discounts[5,4])
}
p5 <- alpha + lambda * p4
return(log(p5))
}
s <- scoreSentence(document[x])
message(paste('...loading language model'))
model <- lapply(seq_along(mkn$counts), function(x) {
loadObject(mkn$counts[[x]])
})
discounts <- loadObject(mkn$discounts)
s <- scoreSentence(document[x])
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM00.mknPipeline.R')
train <- readFile(training)
df <- quanteda::dfm(train, tolower = FALSE, remove = 'BOS')
V <- length(featnames(df))
train <- readFile(training)
training <- corpora$training$alpha$processed[[5]]
train <- readFile(training)
df <- quanteda::dfm(train, tolower = FALSE, remove = 'BOS')
df <- quanteda::dfm(train, tolower = FALSE, remove = 'BOS')
V <- length(featnames(df))
V <- length(featnames(df))
s <- scoreSentence(document[x])
View(s)
View(s)
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM09.mknEstimate.R')
load("~/Data Science/Data Science Projects/PredictifyR-1.0/analysis/quintgrams.txt-estimates_20170525_014750.Rdata")
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM09.mknEstimate.R')
load("~/Data Science/Data Science Projects/PredictifyR-1.0/analysis/quintgrams.txt-estimates_20170525_093547.Rdata")
load("~/Data Science/Data Science Projects/PredictifyR-1.0/analysis/quintgrams.txt-estimates_20170525_103631.Rdata")
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM00.mknPipeline.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM00.mknPipeline.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/config/schema.R')
validation <- corpora$validation
load("~/Data Science/Data Science Projects/PredictifyR-1.0/analysis/MKN-analysis-package-35-pct_20170524_035022.Rdata")
load("~/Data Science/Data Science Projects/PredictifyR-1.0/analysis/coverage-analysis-training-set-beta_20170523_134857.Rdata")
View(trainingSetBetaCoverageAnalysis)
View(trainingSetBetaCoverageAnalysis)
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM00.mknPipeline.R')
training <- corpora$training$alpha
training <- corpora$training$beta
test <- corpora$validation$beta
model <- lm$mkn
# Initialize MKN language model
mknInit(model, training$nGrams, regex)
# Create absolute counts of each nGram
features <- parallelizeTask(mknAbsCount, model, training$nGrams)
# Create continuation counts of each nGram
parallelizeTask(mknCKN, model, model$mOrder)
# Count nGram histories
parallelizeTask(mknHistories, model, model$mOrder)
# Calculate discounts
discounts <- mknDiscount(model)
# Update models with normalizing factors
parallelizeTask(mknNorm, model, model$mOrder)
# Calculate pseudo probability alpha
parallelizeTask(mknAlpha, model)
# Compute weighting factor lambda
parallelizeTask(mknLambda, model, model$mOrder)
source('~/Data Science/Data Science Projects/PredictifyR-1.0/config/initEnvironment.R')
message(paste('\nExecuting', model$mDesc, 'on',
training$corpusName, 'at', startTime))
startTime <- Sys.time()
message(paste('\nExecuting', model$mDesc, 'on',
training$corpusName, 'at', startTime))
startTime <- Sys.time()
message(paste('\nExecuting', model$mDesc, 'on',
training$corpusName, 'at', startTime))
# Initialize MKN language model
mknInit(model, training$nGrams, regex)
# Create absolute counts of each nGram
features <- parallelizeTask(mknAbsCount, model, training$nGrams)
# Create continuation counts of each nGram
parallelizeTask(mknCKN, model, model$mOrder)
# Count nGram histories
parallelizeTask(mknHistories, model, model$mOrder)
# Calculate discounts
discounts <- mknDiscount(model)
# Update models with normalizing factors
parallelizeTask(mknNorm, model, model$mOrder)
# Calculate pseudo probability alpha
parallelizeTask(mknAlpha, model)
# Compute weighting factor lambda
parallelizeTask(mknLambda, model, model$mOrder)
training <- corpora$training$alpha
mknInit(model, training$nGrams, regex)
training <- corpora$training$beta
mknInit(model, training$nGrams, regex)
regex <- regexPatterns
mknInit(model, training$nGrams, regex)
features <- parallelizeTask(mknAbsCount, model, training$nGrams)
parallelizeTask(mknCKN, model, model$mOrder)
parallelizeTask(mknHistories, model, model$mOrder)
discounts <- mknDiscount(model)
parallelizeTask(mknNorm, model, model$mOrder)
parallelizeTask(mknAlpha, model)
parallelizeTask(mknLambda, model, model$mOrder)
evaluation <- parallelizeTask(mknEstimate, model,
training$processed[[model$mOrder]],
test$processed[[model$mOrder]],
sents = 1000, directories)
# Format evaluation package
evalPackage <- list()
evalPackage$model = model$mDesc
evalPackage$corpus = training$corpusName
evalPackage$pct = training$pct
evalPackage$eval = evaluation
output <- list()
output$directory <- directories$analysisDir
output$fileName  <- paste0(sub('\\..*', '', paste0('MKN-analysis-package-')),
training$fileName,
format(Sys.time(),'_%Y%m%d_%H%M%S'), '.Rdata')
output$objName   <- 'evalPackage'
output$data  <- evalPackage
saveObject(output)
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM00.mknPipeline.R')
validation$beta
load("~/Data Science/Data Science Projects/PredictifyR-1.0/logs/log.Rdata")
View(log)
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM07.mknAlpha.R')
load("~/Data Science/Data Science Projects/PredictifyR-1.0/logs/log.Rdata")
View(log)
source('~/Data Science/Data Science Projects/PredictifyR-1.0/config/initEnvironment.R')
training <- corpora$training$gamma
validation <- corpora$validation$gamma
model <- lm$mkn
regex <- regexPatterns
mknInit(model, training$nGrams, regex)
# Create absolute counts of each nGram
features <- parallelizeTask(mknAbsCount, model, training$nGrams)
# Create continuation counts of each nGram
parallelizeTask(mknCKN, model, model$mOrder)
# Count nGram histories
parallelizeTask(mknHistories, model, model$mOrder)
# Calculate discounts
discounts <- mknDiscount(model)
# Update models with normalizing factors
parallelizeTask(mknNorm, model, model$mOrder)
parallelizeTask(mknAlpha, model)
parallelizeTask(mknLambda, model, model$mOrder)
mkn <- lm$mkn
N <- 5
model <- lapply(seq_along(mkn$counts), function(x) {
loadObject(mkn$counts[[x]])
})
discounts <- loadObject(mkn$discounts)
x <- 4
message(paste('...calculating scaling factor lambda for',
mkn$counts[[x]]$fileDesc))
current <- model[[x]]
D1 <- discounts[x+1,4]$D1
D2 <- discounts[x+1,5]$D2
D3 <- discounts[x+1,6]$D3
D2 <- discounts[x+1,5]$D2
D3 <- discounts[x+1,6]$D3
D <- current[,.(nGram, n1wdot, n2wdot, n3pwdot)]
D <- current[,.(nGram, n1wdot, n2wdot, n3pwdot)]
D <- D[, D1N1 := D1 * n1wdot]
D <- D[, D2N2 := D2 * n2wdot]
D <- D[, D3N3 := D3 * n3pwdot]
D <- D[, DnNn := D1N1 + D2N2 + D3N3]
D <- D[, c('n1wdot', 'n2wdot', 'n3pwdot') := NULL]
current <- merge(current, D, by = 'nGram')
memory.limit()
memory.limit(size=2000)
memory.limit(size=20000)
current <- merge(current, D, by = 'nGram')

# Count nGram histories
parallelizeTask(mknHistories, model, model$mOrder)
# Calculate discounts
discounts <- mknDiscount(model)
# Calculate pseudo probability alpha
parallelizeTask(mknAlpha, model)
# Compute weighting factor lambda
parallelizeTask(mknLambda, model)
# Compute probabilities
parallelizeTask(mknEstimate, model)
# Publish language model
parallelizeTask(mknPublish, model, directories)
mknInit(model, training$nGrams, regex)
mknInit(model, training$nGrams, regex)
model <- lm$mkn$epsilon
training <- corpora$training$epsilon
regex <- regexPatterns
mknInit(model, training$nGrams, regex)
features <- parallelizeTask(mknAbsCount, model, training$nGrams)
# Create continuation counts of each nGram
parallelizeTask(mknCKN, model, model$mOrder)
# Count nGram histories
parallelizeTask(mknHistories, model, model$mOrder)
# Calculate discounts
discounts <- mknDiscount(model)
# Calculate pseudo probability alpha
parallelizeTask(mknAlpha, model)
# Compute weighting factor lambda
parallelizeTask(mknLambda, model)
# Compute probabilities
parallelizeTask(mknEstimate, model)
# Publish language model
parallelizeTask(mknPublish, model, directories)
model <- lm$mkn$epsilon
model[[1]][ nGram == 'UNK'][, Pmkn]
mkn <- lm$mkn$epsilon
message('...loading training data')
train <- readFile(training$processed[[mkn$mOrder]])
df <- quanteda::dfm(train, tolower = FALSE, remove = 'BOS')
V <- length(featnames(df))
trainTokens <- sum(ntoken(df))
message(paste('...loading language model'))
model <- lapply(seq_along(mkn$model), function(x) {
loadObject(mkn$model[[x]])
})
message(paste('...loading test data'))
#filePath <- test$processed[[mkn$mOrder
filePath <- test
document <- readFile(filePath)
if (!(is.null(sents))) {
document <- sampleData(document, numChunks = sents, chunkSize = 1, format = 'v')
}
# Compute number of sentences and tokens w/o 'BOS'
M <- length(document) # M = number of sentences
sents = NULL
message('...loading training data')
train <- readFile(training$processed[[mkn$mOrder]])
df <- quanteda::dfm(train, tolower = FALSE, remove = 'BOS')
V <- length(featnames(df))
trainTokens <- sum(ntoken(df))
message(paste('...loading language model'))
model <- lapply(seq_along(mkn$model), function(x) {
loadObject(mkn$model[[x]])
})
message(paste('...loading test data'))
#filePath <- test$processed[[mkn$mOrder
filePath <- test
document <- readFile(filePath)
if (!(is.null(sents))) {
document <- sampleData(document, numChunks = sents, chunkSize = 1, format = 'v')
}
# Compute number of sentences and tokens w/o 'BOS'
M <- length(document) # M = number of sentences
sentence <- document
x <- 1
tokens <- unlist(quanteda::tokenize(sentence, what = 'word'))
ngram <- paste0(tokens[x:(x+3)], collapse = ' ')
unigram <- unlist(strsplit(ngram, ' ', fixed = TRUE))[1]
unigram <- unlist(strsplit(ngram, ' ', fixed = TRUE))[1]
unigramParams <- model[[1]][ nGram == unigram][,. (Pmkn, lambda)]
model <- lapply(seq_along(mkn$model), function(x) {
loadObject(mkn$model[[x]])
})
mknInit(model, training$nGrams, regex)
mkn <- lm$mkn$epsilon
mknInit(mkn, training$nGrams, regex)
features <- parallelizeTask(mknAbsCount, mkn, training$nGrams)
parallelizeTask(mknCKN, mkn, mkn$mOrder)
parallelizeTask(mknHistories, mkn, mkn$mOrder)
discounts <- mknDiscount(mkn)
parallelizeTask(mknAlpha, mkn)
parallelizeTask(mknLambda, mkn)
parallelizeTask(mknEstimate, mkn)
parallelizeTask(mknPublish, mkn, directories)
message('...loading training data')
train <- readFile(training$processed[[mkn$mOrder]])
df <- quanteda::dfm(train, tolower = FALSE, remove = 'BOS')
V <- length(featnames(df))
trainTokens <- sum(ntoken(df))
message(paste('...loading language model'))
model <- lapply(seq_along(mkn$model), function(x) {
loadObject(mkn$model[[x]])
})
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM09.mknPublish.R')
mknInit(mkn, training$nGrams, regex)
# Create absolute counts of each nGram
features <- parallelizeTask(mknAbsCount, mkn, training$nGrams)
# Create continuation counts of each nGram
parallelizeTask(mknCKN, mkn, mkn$mOrder)
# Count nGram histories
parallelizeTask(mknHistories, mkn, mkn$mOrder)
# Calculate discounts
discounts <- mknDiscount(mkn)
# Calculate pseudo probability alpha
parallelizeTask(mknAlpha, mkn)
# Compute weighting factor lambda
parallelizeTask(mknLambda, mkn)
# Compute probabilities
parallelizeTask(mknEstimate, mkn)
# Publish language mkn
parallelizeTask(mknPublish, mkn, directories)
load("~/Data Science/Data Science Projects/PredictifyR-1.0/lm/mkn/epsilon/counts/mkn-unigrams.Rdata")
load("~/Data Science/Data Science Projects/PredictifyR-1.0/lm/mkn/epsilon/counts/mkn-quadgrams.Rdata")
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM09.mknPublish.R')
parallelizeTask(mknPublish, mkn, directories)
load("~/Data Science/Data Science Projects/PredictifyR-1.0/lm/mkn/epsilon/model/mkn-unigrams.Rdata")
load("~/Data Science/Data Science Projects/PredictifyR-1.0/lm/mkn/epsilon/model/mkn-bigrams.Rdata")
load("~/Data Science/Data Science Projects/PredictifyR-1.0/lm/mkn/epsilon/model/mkn-quadgrams.Rdata")
load("~/Data Science/Data Science Projects/PredictifyR-1.0/lm/mkn/epsilon/model/mkn-trigrams.Rdata")
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM00.mknPipeline.R')
unigramParams <- model[[1]][ nGram == unigram][,. (Pmkn, lambda)]
message(paste('...loading language model'))
model <- lapply(seq_along(mkn$model), function(x) {
loadObject(mkn$model[[x]])
})
unigramParams <- model[[1]][ nGram == unigram][,. (Pmkn, lambda)]
unigramParams <- model[[1]][ nGram == 'unigrx'][,. (Pmkn, lambda)]
length(unigramProb) == 0
unigram <- unlist(strsplit(ngram, ' ', fixed = TRUE))[1:2]
unigram <- unlist(strsplit(ngram, ' ', fixed = TRUE))[1]
unigramParams <- model[[1]][ nGram == unigram][,. (Pmkn, lambda)]
if (length(unigramProb) == 0) {
unigramParams <- model[[1]][ nGram == 'UNK'][,. (Pmkn, lambda)]
}
# Compute Bigram Probability and Interpolation Weight
bigram <- unlist(strsplit(ngram, ' ', fixed = TRUE))[1:2]
unigram <- 'UNK'
paste(unigram, unlist(strsplit(ngram, ' ', fixed = TRUE))[2], collapse = ' ')
bigram[1]
paste(bigram[1], 'UNK', collapse = ' ')
bigram <- paste(bigram[1], 'UNK', collapse = ' ')
bigram <- paste(unigram, unlist(strsplit(ngram, ' ', fixed = TRUE))[2], collapse = ' ')
bigramParams <- model[[2]][ nGram == bigram][,. (Pmkn, lambda)]
if (length(bigramProb) == 0) {
bigram <- paste(bigram[1], 'UNK', collapse = ' ')
bigramParams <- model[[2]][ nGram == bigram][,. (Pmkn, lambda)]
}
unigram <- unlist(strsplit(ngram, ' ', fixed = TRUE))[1]
unigramParams <- model[[1]][ nGram == unigram][,. (Pmkn, lambda)]
if (length(unigramParams) == 0) {
unigram <- 'UNK'
unigramParams <- model[[1]][ nGram == unigram][,. (Pmkn, lambda)]
}
unigramProb <- unigramParams$Pmkn
# Compute Bigram Probability and Interpolation Weight
bigram <- paste(unigram, unlist(strsplit(ngram, ' ', fixed = TRUE))[2], collapse = ' ')
bigramParams <- model[[2]][ nGram == bigram][,. (Pmkn, lambda)]
if (length(bigramParams) == 0) {
bigram <- paste(bigram[1], 'UNK', collapse = ' ')
bigramParams <- model[[2]][ nGram == bigram][,. (Pmkn, lambda)]
}
bigramProb <- bigramParams$Pmkn + unigramParams$lambda * unigramProb
paste(bigram, unlist(strsplit(ngram, ' ', fixed = TRUE))[3], collapse = ' ')
trigram <- paste(bigram, unlist(strsplit(ngram, ' ', fixed = TRUE))[3], collapse = ' ')
trigramParams <- model[[3]][ nGram == trigram][,. (Pmkn, lambda)]
paste(trigram[1:2], 'UNK', collapse = ' ')
trigram[1:2]
paste(bigram, unlist(strsplit(ngram, ' ', fixed = TRUE))[3], collapse = ' ')
trigram[1:2]
trigram <- paste(bigram, unlist(strsplit(ngram, ' ', fixed = TRUE))[3], collapse = ' ')
trigram
str(trigram)
length(trigram)
paste(strsplit(trigram, ' ', fixed = TRUE)[1:2], 'UNK', collapse = ' ')
trigram
strsplit(trigram, ' ', fixed = TRUE)
paste(unlist(strsplit(trigram, ' ', fixed = TRUE))[1:2]
)
paste(unlist(strsplit(trigram, ' ', fixed = TRUE))[1:2], 'UNK', collapse = ' ')
strsplit(trigram, ' ', fixed = TRUE))[1:2]
unlist(strsplit(trigram, ' ', fixed = TRUE))[1:2]
c(unlist(strsplit(trigram, ' ', fixed = TRUE))[1:2], 'UNK')
paste(c(unlist(strsplit(trigram, ' ', fixed = TRUE))[1:2], 'UNK'), collapse = ' ')
paste(trigram, unlist(strsplit(ngram, ' ', fixed = TRUE))[3], collapse = ' ')
nGram
ngram
quadgram <- ngram
model[[4]][ nGram == quadgram][, Pmkn]
paste(c(unlist(strsplit(quadgram, ' ', fixed = TRUE))[1:3], 'UNK'), collapse = ' ')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM0A.mknEvaluate.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM0A.mknEvaluate.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM00.mknPipeline.R')
n <- 4
model[[n]][ nGram == ngram][, Pmkn]
unlist(strsplit(ngram, ' '))
unlist(strsplit(ngram, ' '))[2:length(ngram)]
length(ngram)
unlist(strsplit(ngram, ' '))
length(unlist(strsplit(ngram, ' ')))
unlist(strsplit(ngram, ' '))[-1]
paste0(tokens[x:(x+3)], collapse = ' ')
tokens[x:(x+3)]
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM0A.mknEvaluate.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM0A.mknEvaluate.R')
pp <- mknEvaluate(lm$mkn$epsilon, corpora$training$epsilon,  corpora$validation$epsilon, sents = NULL, directories)
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM00.mknPipeline.R')
model[[n]][ nGram == ngram][, Pmkn]
length(prob) != 0
prob <- model[[n]][ nGram == ngram][, Pmkn]
(length(prob) != 0)
qgram <- model[[4]]
View(qgram)
x <- 2
paste0(tokens[x:(x+3)], collapse = ' ')
ngram <- paste0(tokens[x:(x+3)], collapse = ' ')
sentScore$quadgram <- ngram
sentScore <- list()
sentScore$quadgram <- ngram
sentScore$logProb <- score(ngram, mkn$mOrder)
mkn$mOrder
str(mkn$mOrder)
sentScore$logProb <- score(ngram, mkn$mOrder)
sentScore$logProb <- score(ngram, mkn$mOrder)
message(paste('scoring', ngram, 'at level', n))
model[[n]][ nGram == ngram][, Pmkn]
n
mkn$mOrder
ngram
score <- function(ngram, n) {
message(paste('scoring', ngram, 'at level', n))
# Check if n-gram exists in training, and obtain probability
prob <- model[[n]][ nGram == ngram][, Pmkn]
if (length(prob) != 0) {
return(log2(prob))
} else if (n > 1) {
ngram <- unlist(strsplit(ngram, ' '))[-1]
return(score(ngram, n-1))
} else {
prob <- model[[n]][ nGram == 'UNK'][, Pmkn]
return(log2(prob))
}
}
sentScore$logProb <- score(ngram, mkn$mOrder)
score(ngram, mkn$mOrder)
score <- function(ngram, n) {
message(paste('\nscoring', ngram, 'at level', n))
# Check if n-gram exists in training, and obtain probability
prob <- model[[n]][ nGram == ngram][, Pmkn]
if (length(prob) != 0) {
return(log2(prob))
} else if (n > 1) {
ngram <- unlist(strsplit(ngram, ' '))[-1]
return(score(ngram, n-1))
} else {
prob <- model[[n]][ nGram == 'UNK'][, Pmkn]
return(log2(prob))
}
}
scoreSentence <- function(sentence) {
tokens <- unlist(quanteda::tokenize(sentence, what = 'word'))
rbindlist(lapply(seq_along(tokens[1:(length(tokens)-3)]), function(x) {
ngram <- paste0(tokens[x:(x+3)], collapse = ' ')
sentScore <- list()
sentScore$quadgram <- ngram
sentScore$logProb <- score(ngram, mkn$mOrder)
sentScore
}))
}
score <- function(ngram, n) {
message(paste('\nscoring', ngram, 'at level', n))
# Check if n-gram exists in training, and obtain probability
prob <- model[[n]][ nGram == ngram][, Pmkn]
if (length(prob) != 0) {
return(log2(prob))
} else if (n > 1) {
ngram <- unlist(strsplit(ngram, ' '))[-1]
return(score(ngram, n-1))
} else {
prob <- model[[n]][ nGram == 'UNK'][, Pmkn]
return(log2(prob))
}
}
scoreSentence <- function(sentence) {
tokens <- unlist(quanteda::tokenize(sentence, what = 'word'))
rbindlist(lapply(seq_along(tokens[1:(length(tokens)-3)]), function(x) {
ngram <- paste0(tokens[x:(x+3)], collapse = ' ')
sentScore <- list()
sentScore$quadgram <- ngram
sentScore$logProb <- score(ngram, mkn$mOrder)
sentScore
}))
}
sentScore$logProb <- score(ngram, mkn$mOrder)
unlist(strsplit(ngram, ' '))[-1]
unlist(strsplit(ngram, ' '))[-1]
paste0(unlist(strsplit(ngram, ' '))[-1], collapse = ' ')
score <- function(ngram, n) {
message(paste('\nscoring', ngram, 'at level', n))
# Check if n-gram exists in training, and obtain probability
prob <- model[[n]][ nGram == ngram][, Pmkn]
if (length(prob) != 0) {
return(log2(prob))
} else if (n > 1) {
ngram <- paste0(unlist(strsplit(ngram, ' '))[-1], collapse = ' ')
return(score(ngram, n-1))
} else {
prob <- model[[n]][ nGram == 'UNK'][, Pmkn]
return(log2(prob))
}
}
sentScore$logProb <- score(ngram, mkn$mOrder)
load("~/Data Science/Data Science Projects/PredictifyR-1.0/lm/mkn/epsilon/model/mkn-bigrams.Rdata")
load("~/Data Science/Data Science Projects/PredictifyR-1.0/lm/mkn/epsilon/model/mkn-quadgrams.Rdata")
load("~/Data Science/Data Science Projects/PredictifyR-1.0/lm/mkn/epsilon/model/mkn-trigrams.Rdata")
load("~/Data Science/Data Science Projects/PredictifyR-1.0/lm/mkn/epsilon/model/mkn-unigrams.Rdata")
View(mknBigrams)
load("~/Data Science/Data Science Projects/PredictifyR-1.0/lm/mkn/epsilon/model/mkn-unigrams.Rdata")
View(mknUnigrams)
mknInit(mkn, training$nGrams, regex)
# Create absolute counts of each nGram
features <- parallelizeTask(mknAbsCount, mkn, training$nGrams)
# Create continuation counts of each nGram
parallelizeTask(mknCKN, mkn, mkn$mOrder)
# Count nGram histories
parallelizeTask(mknHistories, mkn, mkn$mOrder)
# Calculate discounts
discounts <- mknDiscount(mkn)
# Calculate pseudo probability alpha
parallelizeTask(mknAlpha, mkn)
# Compute weighting factor lambda
parallelizeTask(mknLambda, mkn)
# Compute probabilities
parallelizeTask(mknEstimate, mkn)
# Publish language mkn
parallelizeTask(mknPublish, mkn, directories)
load("~/Data Science/Data Science Projects/PredictifyR-1.0/lm/mkn/epsilon/model/mkn-unigrams.Rdata")
View(mknUnigrams)
message('...loading training data')
train <- readFile(training$processed[[mkn$mOrder]])
df <- quanteda::dfm(train, tolower = FALSE, remove = 'BOS')
V <- length(featnames(df))
trainTokens <- sum(ntoken(df))
message(paste('...loading language model'))
model <- lapply(seq_along(mkn$model), function(x) {
loadObject(mkn$model[[x]])
})
message(paste('...loading test data'))
#filePath <- test$processed[[mkn$mOrder
filePath <- test
document <- readFile(filePath)
if (!(is.null(sents))) {
document <- sampleData(document, numChunks = sents, chunkSize = 1, format = 'v')
}
# Compute number of sentences and tokens w/o 'BOS'
M <- length(document) # M = number of sentences
load("~/Data Science/Data Science Projects/PredictifyR-1.0/lm/mkn/epsilon/model/mkn-unigrams.Rdata")
View(mknUnigrams)
mkn
training
mknInit(mkn, training$nGrams, regex)
features <- parallelizeTask(mknAbsCount, mkn, training$nGrams)
parallelizeTask(mknCKN, mkn, mkn$mOrder)
load("~/Data Science/Data Science Projects/PredictifyR-1.0/lm/mkn/epsilon/counts/mkn-unigrams.Rdata")
View(mknUnigrams)
createTestNGrams(directories)
mknInit(mkn, training$nGrams, regex)
# Create absolute counts of each nGram
features <- parallelizeTask(mknAbsCount, mkn, training$nGrams)
# Create continuation counts of each nGram
parallelizeTask(mknCKN, mkn, mkn$mOrder)
# Count nGram histories
parallelizeTask(mknHistories, mkn, mkn$mOrder)
# Calculate discounts
discounts <- mknDiscount(mkn)
# Calculate pseudo probability alpha
parallelizeTask(mknAlpha, mkn)
# Compute weighting factor lambda
parallelizeTask(mknLambda, mkn)
# Compute probabilities
parallelizeTask(mknEstimate, mkn)
# Publish language mkn
parallelizeTask(mknPublish, mkn, directories)
load("~/Data Science/Data Science Projects/PredictifyR-1.0/lm/mkn/epsilon/model/mkn-unigrams.Rdata")
View(mknUnigrams)
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM0A.mknEvaluate.R')
ngram
sentScore$logProb <- score(ngram, mkn$mOrder)
sentScore$logProb <- score(ngram, mkn$mOrder)
# Function that performs the quadgram probability estimate
score <- function(ngram, n) {
message(paste('\nscoring', ngram, 'at level', n))
# Check if n-gram exists in training, and obtain probability
prob <- model[[n]][ nGram == ngram][, Pmkn]
if (length(prob) != 0) {
message(paste('...found', ngram, 'at level', n))
return(log2(prob))
} else if (n > 1) {
ngram <- paste0(unlist(strsplit(ngram, ' '))[-1], collapse = ' ')
return(score(ngram, n-1))
} else {
message(paste('....getting probability of UNK token'))
prob <- model[[n]][ nGram == 'UNK'][, Pmkn]
return(log2(prob))
}
}
sentScore$logProb <- score(ngram, mkn$mOrder)
n
n <- 1
model[[n]][ nGram == 'UNK'][, Pmkn]
load("~/Data Science/Data Science Projects/PredictifyR-1.0/lm/mkn/epsilon/model/mkn-unigrams.Rdata")
View(mknUnigrams)
model[[n]][ nGram == 'UNK'][, Pmkn]
message('...loading training data')
train <- readFile(training$processed[[mkn$mOrder]])
df <- quanteda::dfm(train, tolower = FALSE, remove = 'BOS')
V <- length(featnames(df))
trainTokens <- sum(ntoken(df))
message(paste('...loading language model'))
model <- lapply(seq_along(mkn$model), function(x) {
loadObject(mkn$model[[x]])
})
message(paste('...loading test data'))
#filePath <- test$processed[[mkn$mOrder
filePath <- test
document <- readFile(filePath)
if (!(is.null(sents))) {
document <- sampleData(document, numChunks = sents, chunkSize = 1, format = 'v')
}
# Compute number of sentences and tokens w/o 'BOS'
M <- length(document) # M = number of sentences
sentScore$logProb <- score(ngram, mkn$mOrder)
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM0A.mknEvaluate.R')
pp <- mknEvaluate(lm$mkn$epsilon, corpora$training$epsilon,  corpora$validation$epsilon, sents = NULL, directories)
pp <- mknEvaluate(lm$mkn$alpha, corpora$training$alpha,  corpora$validation$alpha, sents = 100, directories)
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM0A.mknEvaluate.R')
pp <- mknEvaluate(lm$mkn$alpha, corpora$training$alpha,  corpora$validation$alpha, sents = 100, directories)
getScore <- function(ngram, n) {
ngramType = list('Quadgram', 'Trigram', 'Bigram', 'Unigram')
res <- list(prob = prob, type = type)
# Check if n-gram exists in training, and obtain probability
res$prob <- model[[n]][ nGram == ngram][, Pmkn]
res$type <- ngramType[[n]]
if (length(res$prob) != 0) {
return(res)
} else if (n > 1) {
ngram <- paste0(unlist(strsplit(ngram, ' '))[-1], collapse = ' ')
return(score(ngram, n-1))
} else {
res$prob <- model[[n]][ nGram == 'UNK'][, Pmkn]
return(res)
}
}
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM0A.mknEvaluate.R')
pp <- mknEvaluate(lm$mkn$epsilon, corpora$training$epsilon,  corpora$validation$epsilon, sents = NULL, directories)
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM0A.mknEvaluate.R')
pp <- mknEvaluate(lm$mkn$epsilon, corpora$training$epsilon,  corpora$validation$epsilon, sents = NULL, directories)
ngramType <- list('Quadgram', 'Trigram', 'Bigram', 'Unigram')
res <- list(prob = prob, type = type)
# Check if n-gram exists in training, and obtain probability
res$prob <- model[[n]][ nGram == ngram][, Pmkn]
res$type <- ngramType[[n]]
res <- list(prob = prob, type = type)
res <- list()
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM0A.mknEvaluate.R')
pp <- mknEvaluate(lm$mkn$epsilon, corpora$training$epsilon,  corpora$validation$epsilon, sents = NULL, directories)
getScore <- function(ngram, n) {
ngramType <- list('Quadgram', 'Trigram', 'Bigram', 'Unigram')
res <- list()
# Check if n-gram exists in training, and obtain probability
res$prob <- model[[n]][ nGram == ngram][, Pmkn]
res$type <- ngramType[[n]]
if (length(res$prob) != 0) {
return(res)
} else if (n > 1) {
ngram <- paste0(unlist(strsplit(ngram, ' '))[-1], collapse = ' ')
return(score(ngram, n-1))
} else {
res$prob <- model[[n]][ nGram == 'UNK'][, Pmkn]
return(res)
}
}
getScore <- function(ngram, n) {
ngramType <- list('Quadgram', 'Trigram', 'Bigram', 'Unigram')
res <- list()
# Check if n-gram exists in training, and obtain probability
res$prob <- model[[n]][ nGram == ngram][, Pmkn]
res$type <- ngramType[[n]]
if (length(res$prob) != 0) {
return(res)
} else if (n > 1) {
ngram <- paste0(unlist(strsplit(ngram, ' '))[-1], collapse = ' ')
return(score(ngram, n-1))
} else {
res$prob <- model[[n]][ nGram == 'UNK'][, Pmkn]
return(res)
}
}
pp <- mknEvaluate(lm$mkn$epsilon, corpora$training$epsilon,  corpora$validation$epsilon, sents = NULL, directories)
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM0A.mknEvaluate.R')
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM0A.mknEvaluate.R')
pp <- mknEvaluate(lm$mkn$epsilon, corpora$training$epsilon,  corpora$validation$epsilon, sents = NULL, directories)
source('~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM00.mknPipeline.R')

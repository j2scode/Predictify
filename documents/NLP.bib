Automatically generated by Mendeley Desktop 1.17.9
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Fazly2002,
author = {Fazly, a},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fazly - 2002 - The Use of Syntax in Word Completion Utilities.pdf:pdf},
title = {{The use of syntax in word completion utilities}},
url = {ftp://www.learning.cs.toronto.edu/public{\_}html/public{\_}html/cs/ftp/pub/gh/Fazly-thesis.pdf},
year = {2002}
}
@article{Klakow1998a,
abstract = {Building probabilistic models of language is a central task in natural language and speech processing allowing to integrate the syntactic and/or semantic (and recently pragmatic) constraints of the language into the systems. Probabilistic language models are an attractive alternative to the more traditional rule-based systems, such as context free grammars, because of the recent availability of massive amount of text corpora which can be used to efficiently train the models and because instead of binary grammaticality judgement offered by the rule-based systems, likelihood of any sequence of lexical units can be obtained, which is a crucial factor in such tasks as speech recognition. Probabilistic language models also find their application in part-of-speech tagging, machine translation, semantic disambiguation and numerous other fields. The most widely used language models are based on the estimation of the proba-bility of observing a given lexical unit conditioned on the observations of n−1 preced-ing lexical units, and are known as n-gram models. When the n-gram estimates are poor, whatever the reason for that may be, a technique called smoothing is applied to adjust the estimates and hopefully produce more accurate model. Smoothing techniques may be roughly divided into the backing-off and interpolation. In the first case, the best n-gram model in the current context is selected, whereas in the second case all the n-gram models of different specificities are combined together to form a better predictor. In this thesis, a recently proposed novel interpolation scheme is investigated, namely, the log-linear interpolation. Unlike the original publication, however, which dealt with combining the models of unrelated nature, the aim of this thesis is to formulate the theoretical framework for smoothing the n-gram probability estimates obtained from similar language models with different levels of specificity on the same corpus, which will be called log-linear n-gram smoothing, and compare it to the well-established linear interpolation and back-off methods. The framework being proposed includes probability combination, parameter optimisation, dealing with data sparsity and parameter clustering. The resulting technique is shown to outperform the conventional linear interpo-lation and back-off techniques when applied to the n-gram smoothing tasks. ii},
author = {Klakow, Dietrich},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Klakow - 1998 - Log-linear interpolation of language models.pdf:pdf},
journal = {Proc. ICSLP},
number = {January},
pages = {1--4},
title = {{Log-linear interpolation of language models}},
url = {http://www.mirlab.org/conference{\_}papers/International{\_}Conference/ICSLP 1998/PDF/SCAN/SL980522.PDF http://www.mirlab.org/conference{\_}papers/International{\_}Conference/ICSLP 1998/PDF/AUTHOR/SL980522.PDF},
year = {1998}
}
@misc{Christensen2016,
author = {Christensen, Hans},
title = {{HC Corpora}},
url = {http://www.corpora.heliohost.org/},
year = {2016}
}
@article{Griffiths2003,
abstract = {We explore the consequences of viewing semantic association as the result of attempting to predict the concepts likely to arise in a particular context. We argue that the success of existing accounts of semantic representation comes as a result of indirectly addressing this problem, and show that a closer correspondence to human data can be obtained by taking a probabilistic approach that explicitly models the generative structure of language.},
author = {Griffiths, Thomas L and Steyvers, Mark},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Griffiths, Steyvers - Unknown - Prediction and Semantic Association(2).pdf:pdf},
isbn = {0262025507},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
keywords = {Distributional semantics},
pages = {11--18},
title = {{Prediction and Semantic Association}},
url = {https://papers.nips.cc/paper/2153-prediction-and-semantic-association.pdf papers3://publication/uuid/EA850935-AD77-4502-8B69-E5DF29AF6A0B},
volume = {15},
year = {2003}
}
@article{Arnold2016,
archivePrefix = {arXiv},
arxivId = {0712.0689},
author = {Arnold, Jeffrey B.},
doi = {10.1007/978-1-61779-968-6},
eprint = {0712.0689},
isbn = {9780470057247},
issn = {00335770},
pmid = {18963060},
title = {{Introduction To ggthemes}},
url = {https://cran.r-project.org/web/packages/ggthemes/vignettes/ggthemes.html},
year = {2016}
}
@book{Yule1944,
author = {Yule, George Udny},
pages = {306},
publisher = {Cambridge University Press},
title = {{The Statistical Study of Literary Vocabulary}},
year = {1944}
}
@misc{Google,
author = {Google},
title = {{Google Books Ngram Datasets}},
url = {http://commondatastorage.googleapis.com/books/syntactic-ngrams/index.html},
urldate = {2017-03-17},
year = {2013}
}
@article{Liu2006,
abstract = {Web text has been successfully used as training data for many NLP applications. While most previous work accesses web text through search engine hit counts, we created a Web Corpus by downloading web pages to create a topic-diverse collec- tion of 10 billion words of English. We show that for context-sensitive spelling correction theWeb Corpus results are bet- ter than using a search engine. For the- saurus extraction, it achieved similar over- all results to a corpus of newspaper text. With many more words available on the web, better results can be obtained by col- lecting much larger web corpora.},
author = {Liu, Vinci and Curran, James R},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu, Curran - Unknown - Web Text Corpus for Natural Language Processing.pdf:pdf},
isbn = {1932432590},
journal = {Eacl},
pages = {233--240},
title = {{Web Text Corpus for Natural Language Processing}},
url = {http://anthology.aclweb.org/E/E06/E06-1030.pdf},
year = {2006}
}
@inbook{Baayen2001,
address = {Dordrecht},
author = {Baayen, R Harald},
booktitle = {Word Frequency Distributions},
doi = {10.1007/978-94-010-0844-0_6},
isbn = {978-94-010-0844-0},
pages = {195--236},
publisher = {Springer Netherlands},
title = {{Examples of Applications}},
url = {http://dx.doi.org/10.1007/978-94-010-0844-0{\_}6},
year = {2001}
}
@article{Covington2010b,
author = {Covington, M.A. and McFall, J.D.},
journal = {Journal of Quantitative Linguistics},
number = {2},
pages = {94--100},
title = {{Cutting the Gordian Knot: The Moving-Average Type-Token Ratio (MATTR).}},
volume = {17},
year = {2010}
}
@article{Eisenstein2015,
abstract = {Social media features a wide range of nonstandard spellings, many of which appear inspired by phonological variation. However, the nature of the connection between variation across the spoken and written modalities remains poorly understood. Are phono-logical variables transferred to writing on the level of graphemes, or is the larger system of contextual patterning also transferred? This paper considers orthographic coda deletions corresponding to the phonological variables of (ing) and (t,d). In both cases, orthography mirrors speech: reduction of the -ing suffix depends on the word's syntactic category, and reduction of the -t,-d suffix depends on the succeeding phonological context. These spellings are more frequently used in informal conversational contexts, and in areas with high propor-tions of African Americans, again mirroring the patterning of the associated phonological variables. This suggests a deep connection between variation in the two modalities, necessi-tating a new account of the production of cross-modal variation. (150 words)},
author = {Eisenstein, Jacob},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Eisenstein - 2015 - Systematic patterning in phonologically-motivated orthographic variation.pdf:pdf},
keywords = {computer-mediated communication Word count 7687,orthography,social media,variation},
title = {{Systematic patterning in phonologically-motivated orthographic variation *}},
year = {2015}
}
@article{Garay-Vitoria2010a,
abstract = {Text prediction was initially proposed to help people with a low text composition speed to enhance their message composition. After the important advancements obtained in the last years, text prediction methods may nowadays benefit anyone trying to input text messages or commands, if they are adequately integrated within the user interface of the application. Diverse text prediction methods are based in different statistic and linguistic properties of natural languages. Hence, they are very dependent on the language concerned. In order to discuss general issues of text prediction it is necessary to propose abstract descriptions of the methods used. In this paper a number of models applied to text prediction are presented. Some of them are oriented to low-inflected languages while others are for high-inflected languages. All these models have been implemented and their results are compared. Presented models may be useful for future discussion. Finally, some comments related to the comparison of previously published results are also done. ?? 2009 Elsevier Ltd. All rights reserved.},
author = {Garay-Vitoria, Nestor and Abascal, Julio},
doi = {10.1016/j.csl.2009.03.008},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Garay-Vitoria, Abascal - 2010 - Modelling text prediction systems in low- and high-inflected languages(2).pdf:pdf},
issn = {08852308},
journal = {Computer Speech and Language},
keywords = {Anticipative interfaces,Communication speed enhancement,Prediction measures,Prediction models,Text prediction},
number = {2},
pages = {117--135},
title = {{Modelling text prediction systems in low- and high-inflected languages}},
volume = {24},
year = {2010}
}
@article{Montemurro2001,
abstract = {In this paper the Zipf-Mandelbrot law is revisited in the context of linguistics. Despite its widespread popularity the Zipf-Mandelbrot law can only describe the statistical behaviour of a rather restricted fraction of the total number of words contained in some given corpus. In particular, we focus our attention on the important deviations that become statistically relevant as larger corpora are considered and that ultimately could be understood as salient features of the underlying complex process of language generation. Finally, it is shown that all the different observed regimes can be accurately encompassed within a single mathematical framework recently introduced by C. Tsallis. ?? 2001 Elsevier Science B.V. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {cond-mat/0104066},
author = {Montemurro, Marcelo A},
doi = {10.1016/S0378-4371(01)00355-7},
eprint = {0104066},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Montemurro - 2001 - Beyond the Zipf–Mandelbrot law in quantitative linguistics.pdf:pdf},
issn = {03784371},
journal = {Physica A: Statistical Mechanics and its Applications},
keywords = {Human language,Zipf-Mandelbrot law},
number = {3-4},
pages = {567--578},
pmid = {172092800018},
primaryClass = {cond-mat},
title = {{Beyond the Zipf-Mandelbrot law in quantitative linguistics}},
url = {www.elsevier.com/locate/physa},
volume = {300},
year = {2001}
}
@article{Center2015,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Ilboudo, Sidb??wendin David Olivier and Sombi??, Issa and Soubeiga, Andr?? Kamba and Dr??bel, Tania},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - PI{\_}Smartphones{\_}0401151.pdf:pdf},
isbn = {9788578110796},
issn = {09953914},
journal = {Sante Publique},
keywords = {Burkina Faso,Delivery of health care,Motivation,Patient preference,Rural health services,Treatment refusal},
number = {3},
pages = {391--397},
pmid = {25246403},
title = {{Facteurs influen??ant le refus de consulter au centre de sant?? dans la r??gion rurale Ouest du Burkina Faso}},
url = {http://www.pewinternet.org/2015/04/01/us-smartphone-use-in-2015/},
volume = {28},
year = {2016}
}
@article{Dowle2016,
author = {Dowle, Matt},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dowle - 2016 - Package ‘data.table'.pdf:pdf},
journal = {Cran},
title = {{Package ‘data.table'}},
year = {2016}
}
@article{Communication2010a,
author = {Communication, Alternative and Yarmohammadi, Mahsa A},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Communication, Yarmohammadi - 2010 - Language Modeling and Word Prediction Papers for Today.pdf:pdf},
title = {{Language Modeling and Word Prediction Papers for Today}},
year = {2010}
}
@misc{Benoit2016b,
annote = {From Duplicate 2 (quanteda: Quantitative Analysis of Textual Data - Benoit, Kenneth; Nulty, Paul)

R package version 0.9.8.5},
author = {Benoit, Kenneth and Nulty, Paul and Watanabe, Kohei and Lauderdale, Benjamin and Obeng, Adam and Barber{\'{a}}, Pablo and Lowe, Will},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Benoit et al. - 2016 - Package 'quanteda' Title Quantitative Analysis of Textual Data.pdf:pdf},
title = {{quanteda: Quantitative Analysis of Textual Data}},
url = {https://github.com/kbenoit/quanteda},
year = {2016}
}
@article{Goodman2008,
archivePrefix = {arXiv},
arxivId = {arXiv:cs/0108005v1},
author = {Goodman, Joshua T},
eprint = {0108005v1},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Goodman - 2001 - A Bit of Progress in Language Modeling Extended Version.pdf:pdf},
primaryClass = {arXiv:cs},
title = {{A Bit of Progress in Language Modeling Extended Version}},
url = {http://www.research.microsoft.com},
year = {2008}
}
@article{Evert2005a,
author = {Evert, Stefan and Baroni, Marco},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Evert, Baroni - Unknown - Testing the extrapolation quality of word frequency models.pdf:pdf},
issn = {1747-939},
journal = {Proceedings of Corpus Linguistics 2005},
pages = {1747--939},
title = {{Testing the extrapolation quality of word frequency models}},
url = {http://www.stefan-evert.de/PUB/EvertBaroni2005.pdf},
year = {2005}
}
@article{Wagacha,
author = {Wagacha, Peter Waiganjo and Chege, Dennis},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wagacha, Chege - Unknown - Adaptive and Optimization Predictive Text Entry for Short Message Service (Sms).pdf:pdf},
journal = {Review Literature And Arts Of The Americas},
title = {{Adaptive and Optimization Predictive Text Entry for Short Message Service (Sms)}}
}
@article{doi:10.1093/mind/XLI.164.409,
author = {JOHNSON, W E},
doi = {10.1093/mind/XLI.164.409},
journal = {Mind},
number = {164},
pages = {409},
title = {{I.—PROBABILITY: THE DEDUCTIVE AND INDUCTIVE PROBLEMS}},
url = {+ http://dx.doi.org/10.1093/mind/XLI.164.409},
volume = {XLI},
year = {1932}
}
@article{DerPhilosophisch-Historischen2004,
author = {der Philosophisch-Historischen, Von and {Evert aus Ludwigsburg Hauptberichter}, Stefan and {Rohrer Mitberichter}, C and {Kahnert Mitberichter}, Apl D and Heid, HD U},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Evert - 2005 - The Statistics of Word Cooccurrences Word Pairs and Collocations.pdf:pdf},
title = {{The Statistics of Word Cooccurrences Word Pairs and Collocations}},
year = {2004}
}
@article{Cruz,
abstract = {Resumen: Este trabajo presenta un m{\'{e}}todo autom{\'{a}}tico para reducir el conjunto de categor{\'{i}}as de palabras que ser{\'{a}} utilizado por un sistema de predicci{\'{o}}n de palabras en Portugu{\'{e}}s. El m{\'{e}}todo se basa en una medida de similitud que se aplica a una matriz de asociaci{\'{o}}n, generada mediante el empleo de una medida de disparidad (odds ratio) aplicada sobre la matriz de distribuci{\'{o}}n de probabilidades de bigramas de categor{\'{i}}as (bipos) presentes en un corpus. Los resultados presentados en este trabajo muestran que la utilizaci{\'{o}}n del m{\'{e}}todo de agrupamiento propuesto, con un umbral adecuado de similitud, tiene potencial para mejorar el sistema de predicci{\'{o}}n de palabras. Adem{\'{a}}s posibilita la utilizaci{\'{o}}n de nuevas t{\'{e}}cnicas de agrupamiento de categor{\'{i}}as como agrupamiento borroso. Los resultados tambi{\'{e}}n muestran que cuando se utiliza un sistema de predicci{\'{o}}n de palabras basado en un modelo sint{\'{a}}ctico, la agrupaci{\'{o}}n no se puede realizar entre las categor{\'{i}}as sint{\'{a}}cticas m{\'{a}}s importantes, aunque los grupos generados parezcan correctos desde el punto de vista ling{\"{u}}{\'{i}}stisco. Palabras clave: Agrupamiento de categor{\'{i}}as de palabras, sistema de predicci{\'{o}}n de palabras, modelo del espacio vectorial, optimizaci{\'{o}}n, portugu{\'{e}}s. Abstract: This paper presents an automatic method for reducing the part-of-speech tagset to be considered by a word prediction system in Portuguese. The method is based on a similarity measure applied to a association matrix, generated by em-ploying a odds ratio association measure in the bigrams of parts-of-speech (bipos) probability distribution in a corpus. The results reported in this paper show that using the proposed clustering method with an appropriate threshold value over the similarity has the potential to improve the word prediction system. Moreover, it makes possible to use new clustering techniques such as fuzzy clustering. The re-sults also show that when using a word prediction system based on a syntactic model, the clustering cannot be performed between the major syntactic categories, even if the clusters generated seem correct from a linguistic point of view.},
author = {Cruz, Daniel and Teodiano, Cavalieri and Bastos, Freire and M{\'{a}}rio, Filho and Filho, Sarcinelli and Elena, Sira and Cagigas, Palazuelos and Guarasa, Javier Mac{\'{i}}as and {Mart{\'{i}}n S{\'{a}}nchez}, Jos{\'{e}} L},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cruz et al. - Unknown - A Part-of-Speech Tag Clustering for a Word Prediction System in Portuguese Language Agrupamiento de Categor{\'{i}}a(2).pdf:pdf},
keywords = {Part-of-speech clustering,Portuguese language,optimization,vector space model,word prediction system},
title = {{A Part-of-Speech Tag Clustering for a Word Prediction System in Portuguese Language * Agrupamiento de Categor{\'{i}}as para un Sistema de Predicci{\'{o}}n de Palabras en Portugu{\'{e}}s}}
}
@article{Palmer2000,
author = {Palmer, David D},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Palmer - Unknown - Chapter 2 Tokenisation and Sentence Segmentation.pdf:pdf},
journal = {Handbook of Natural Language Processing},
pages = {11--35},
title = {{Tokenisation and Sentence Segmentation}},
year = {2000}
}
@misc{Google2012,
author = {Google},
title = {{Offensive Words from Google's 'What Do You Love' Project}},
url = {https://gist.github.com/jamiew/1112488},
year = {2012}
}
@misc{Baayen2013,
author = {Baayen, R H},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Baayen, Baayen - 2015 - Package 'languageR' Data sets and functions for Analyzing Linguistic Data A practical introduction to statistics.pdf:pdf},
title = {{Package 'languageR' : Data sets and functions for Analyzing Linguistic Data: A practical introduction to statistics''}},
year = {2013}
}
@unpublished{Feinerer2015,
abstract = {A framework for text mining applications within R.},
author = {Feinerer, Ingo},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Feinerer, Hornik - Unknown - Package {\&}quottm{\&}quot.pdf:pdf},
pages = {58},
title = {{Package 'tm' Text Mining Package}},
url = {https://cran.r-project.org/web/packages/tm/index.html},
year = {2015}
}
@article{Chen1998,
abstract = {We present an extensive empirical comparison of several smoothing techniques in the domain of language modeling, including those described by Jelinek and Mercer (1980), Katz (1987), and Church and Gale (1991). We investigate for the first time how factors such as training data size, corpus (e.g., Brown versus Wall Street Journal), and n-gram order (bigram versus trigram) affect the relative performance of these methods, which we measure through the cross-entropy of test data. In addition, we introduce two novel smoothing techniques, one a variation of Jelinek-Mercer smoothing and one a very simple linear interpolation technique, both of which outperform existing methods.},
archivePrefix = {arXiv},
arxivId = {cmp-lg/9606011},
author = {Chen, Stanley F. and Goodman, Joshua},
doi = {10.3115/981863.981904},
eprint = {9606011},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen, Goodman - Unknown - An Empirical Study of Smoothing Techniques for Language Modeling.pdf:pdf;:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen, Goodman - 1999 - An empirical study of smoothing techniques for language modeling.pdf:pdf;:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen, Goodman - 1996 - An Empirical Study of Smoothing Techniques for Language Modeling.pdf:pdf},
isbn = {0885-2308},
issn = {08852308},
journal = {Proceedings of the 34th Annual Meeting on Association for Computational Linguistics},
pages = {310--318},
pmid = {25246403},
primaryClass = {cmp-lg},
title = {{An Empirical Study of Smoothing Techniques for Language Modeling}},
url = {https://dash.harvard.edu/bitstream/handle/1/25104739/tr-10-98.pdf?sequence=1 http://arxiv.org/abs/cmp-lg/9606011{\%}5Cnhttp://dx.doi.org/10.3115/981863.981904 http://www.idealibrary.com https://people.eecs.berkeley.edu/{~}klein/cs294-5/chen{\_}goodman.pdf},
volume = {13},
year = {1996}
}
@article{Stolcke2011,
abstract = {The SRI Language Modeling Toolkit (SRILM for short) is an open source software toolkit for statistical language modeling and related tasks. It was first conceived and implemented— with minimal functionality—in 1995, followed by a first public (beta) release in 1999. Since then SRILM has found wide use in the speech and natural language research community. A 2002 paper [1] presented an overview of the toolkit's design and functionality, of which we provide only a brief summary here. This paper takes stock of SRILM developments since then, as well as extensions and applications of SRILM. We also summarize activities in the SRILM user community, give an overall assessment of developments so far, and point out possible future directions.},
author = {Stolcke, Andreas and Zheng, Jing and Wang, Wen and Abrash, Victor},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Stolcke et al. - 2011 - SRILM at Sixteen Update and Outlook.pdf:pdf},
journal = {Proceedings of the IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)},
pages = {5--9},
title = {{SRILM at Sixteen : Update and Outlook}},
url = {https://www.researchgate.net/profile/Victor{\_}Abrash/publication/255563494{\_}SRILM{\_}at{\_}sixteen{\_}update{\_}and{\_}outlook/links/54b6a3bc0cf2e68eb27ebf14.pdf},
year = {2011}
}
@article{Ghayoomi2008,
author = {Ghayoomi, Masood},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ghayoomi - 2008 - A POS-Based Word Prediction System for.pdf:pdf},
keywords = {pos tagging,statistical language modeling,word prediction},
pages = {5221--5221},
title = {{A POS-Based Word Prediction System for}},
year = {2008}
}
@incollection{Jurafsky2016,
abstract = {CHAPTER 4 Language Modeling with N-grams " You are uniformly charming! " cried he, with a smile of associating and now and then I bowed and they perceived a chaise and four to wish for. Random sentence generated from a Jane Austen trigram model Being able to predict the future is not always a good thing. Cassandra of Troy had the gift of foreseeing but was cursed by Apollo that her predictions would never be believed. Her warnings of the destruction of Troy were ignored and to simplify, let's just say that things just didn't go well for her later. In this chapter we take up the somewhat less fraught topic of predicting words. What word, for example, is likely to follow Please turn your homework ... Hopefully, most of you concluded that a very likely word is in, or possibly over, but probably not refrigerator or the. In the following sections we will formalize this intuition by introducing models that assign a probability to each possible next word. The same models will also serve to assign a probability to an entire sentence. Such a model, for example, could predict that the following sequence has a much higher probability of appearing in a text: all of a sudden I notice three guys standing on the sidewalk},
author = {Jurafsky, Daniel and Martin, James R},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - full-text(3).pdf:pdf},
isbn = {0131873210},
title = {{Language Modeling with N- grams}},
year = {2016}
}
@misc{Von2016,
author = {von Ahn, Luis},
title = {{Carnegie Mellon University - List of Profane Words}},
url = {https://www.cs.cmu.edu/{~}biglou/resources/bad-words.txt},
year = {2016}
}
@article{Thurlow2003,
abstract = {Abstract: The so called 'net generation' is popularly assumed to be naturally media literate and to be necessarily reinventing conventional linguistic and communicative practices. With this in mind, this essay centres around discursive analyses of qualitative data arising from an investigation of 159 older teenagers' use of mobile telephone text-messaging - or SMS (i.e. short-messaging services). In particular, against a backdrop of media commentaries, we examine the linguistic forms and communicative functions in a corpus of 544 participants' actual text-messages. While young people are surely using their mobile phones as a novel, creative means of enhancing and supporting intimate relationships and existing social networks, popular discourses about the linguistic exclusivity and impenetrability of this particular technologically-mediated discourse appear greatly exaggerated. Serving the sociolinguistic 'maxims' of (a) brevity and speed, (b) paralinguistic restitution and (c) phonological approximation, young people's messages are both linguistically unremarkable and communicatively adept.},
author = {Thurlow, Crispin},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Thurlow - Unknown - Generation Txt The socialinguistics of youn people's text-messaging.pdf:pdf},
isbn = {1477-7843},
issn = {14777843},
journal = {Discourse Analysis Online},
keywords = {1,adolescents,all tables,communication technologies,computer-mediated discourse,figures and images are,introduction and background,multimedia,new,presented in pdf format,sms,sociolinguistics,text-messaging},
pages = {1--31},
title = {{Generation Txt ? The sociolinguistics of young people ' s text-messaging}},
url = {http://extra.shu.ac.uk/daol/articles/v1/n1/a3/thurlow2002003-paper.html},
year = {2003}
}
@misc{RStudioTeam2016,
author = {{Rstudio Team}},
booktitle = {RStudio},
title = {{RStudio – Open source and enterprise-ready professional software for R}},
url = {https://www.rstudio.com/},
year = {2016}
}
@inproceedings{Powers1998,
abstract = {Recently I have been intrigued by the reappearance of an old friend, George Kingsley Zipf, in a number of not entirely expected places. The law named for him is ubiquitous, but Zipf did not actually discover the law so much as provide a plausible explanation. Others have proposed modifications to Zipf's Law, and closer examination uncovers systematic deviations from its normative form. We demonstrate how Zipf's analysis can be extended to include some of these phenomena.},
author = {Powers, David M W},
booktitle = {NeMLaP3/CoNLL '98 Proceedings of the Joint Conferences on New Methods in Language Processing and Computational Natural Language Learning},
doi = {10.3115/1603899.1603924},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Powers - Unknown - Applications and Explanations of Zipf's Law.pdf:pdf},
isbn = {0-7258-0634-6},
pages = {151--160},
title = {{Applications and Explanations of Zipf's Law}},
year = {1998}
}
@article{Lyddy,
abstract = {Concerns over effects of 'textisms' on literacy have been reinforced by research identifying processing costs associated with reading textisms. But to what extent do such studies reflect actual textism use? This study examined the textual characteristics of 936 text messages in English (13391 words). Message length, nonstandard spelling, sender and message characteristics and word frequency were analyzed. The data showed that 25{\%} of word content used nonstandard spelling, the most frequently occurring category involving omission of capital letters. Types of nonstandard spelling varied only slightly depending on the purpose of the text message, while the overall proportion of nonstandard spelling did not differ significantly. Less than 0.2{\%} of content was 'semantically unrecoverable.' Implications for experimental studies of textisms are discussed. Text messaging, short message service (SMS) or 'texting' continues to be a popular means of communi-cation, among young people in particular. A report by Lenhart, Ling, Campbell, and Purcell (2010) high-lighted the rapid increase in text messaging in the United States, where 72{\%} of teenagers use text messag-ing, compared to 51{\%} in 2006. In a British survey, 52{\%} of young people aged 11-18 (n = 1000), along with 28{\%} of adults aged 18-65 (n = 2000), named texting as the most important form of communication that they use to stay in touch with friends (Mobile Life Report, 2008); for the young people surveyed, texting ranked above instant messaging (17{\%}), e-mail (12{\%}), calls via mobile (9{\%}) or landline (10{\%}), and letters (0{\%}). A British survey of 2117 adults shows the increasing popularity of texting from 2005 to 2010, with 62{\%} of those aged 16-24 preferring texting over other means of communicating with friends (Ofcom, 2011). Ling's (2010) cross-sectional analysis suggests that texting follows a life-phase pattern, with older teens and those in their early 20s making the most use of the medium, with usage dropping off with age. The authors would like to thank Maria Bakardjieva and two anonymous reviewers for helpful criticisms and suggestions regarding data presentation and analysis.},
author = {Lyddy, Fiona and Farina, Francesca and Hanney, James and Farrell, Lynn and Kelly, Niamh and Neill, O '},
doi = {10.1111/jcc4.12045},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lyddy et al. - Unknown - An Analysis of Language in University Students' Text Messages.pdf:pdf},
keywords = {Language use,Linguistic,Literacy,Mobile phones,Text messaging},
title = {{An Analysis of Language in University Students' Text Messages *}}
}
@article{Trnka2008a,
abstract = {We present the development and tuning of a topic-adapted language model for word prediction, which improves keystroke savings over a comparable baseline. We outline our plans to develop and integrate style adaptations, building on our experience in topic modeling to dynamically tune the model to both topically and stylistically relevant texts.},
author = {Trnka, Keith},
doi = {10.3115/1564154.1564167},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Trnka - Unknown - Adaptive Language Modeling for Word Prediction(2).pdf:pdf},
journal = {HLT-SRWS '08 Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies},
number = {June},
pages = {61--66},
title = {{Adaptive language modeling for word prediction}},
url = {http://dl.acm.org/citation.cfm?id=1564167},
year = {2008}
}
@article{Whittaker2001,
abstract = {This paper describes two techniques for reducing the size of statistical back-off -gram language models in computer memory. Language model compression is achieved through a combination of quantizing language model probabilities and back-off weights and the pruning of parameters that are determined to be unnecessary after quantization. The recognition performance of the original and compressed language models is eval-uated across three different language models and two different recognition tasks. The results show that the language models can be compressed by up to 60{\%} of their original size with no significant loss in recognition performance. Moreover, the techniques that are described provide a principled method with which to compress language models further while minimising degradation in recognition performance. Abstract This paper describes two techniques for reducing the size of statistical back-off ¤ -gram language models in computer mem-ory. Language model compression is achieved through a combi-nation of quantizing language model probabilities and back-off weights and the pruning of parameters that are determined to be unnecessary after quantization. The recognition performance of the original and compressed language models is evaluated across three different language models and two different recog-nition tasks. The results show that the language models can be compressed by up to 60{\%} of their original size with no signifi-cant loss in recognition performance. Moreover, the techniques that are described provide a principled method with which to compress language models further while minimising degrada-tion in recognition performance.},
author = {Whittaker, EWD and Raj, Bhiksha},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Whittaker et al. - 2001 - Quantization-based language model compression.pdf:pdf},
isbn = {8790834100},
journal = {Interspeech},
pages = {2--5},
title = {{Quantization-based language model compression.}},
url = {http://www.merl.com https://merl.com/reports/docs/TR2001-41.pdf},
year = {2001}
}
@article{VandenBosch2008a,
abstract = {Word completion is a basic technology for reducing the effort involved in text entry on mobile devices and in augmentative communication devices, where efficiency and ease of use are needed, but where a low memory footprint is also required. Standard solutions compress a lexicon into a suffix tree with a small memory footprint and high retrieval speed. Keystroke savings, a measurable correlate of text entry effort gain, typically improve when the algorithm would also take into account the previous word; however, this comes at the cost of a large footprint. We develop two word completion algorithms that encode the previous word in the input. The first algorithm utilizes a character buffer that includes a fixed number of recent keystrokes, including those belonging to previous words. The second algorithm includes the complete previous word as an extra input feature. In simulation studies, the first algorithm yields marked improvements in keystroke savings, but has a large memory footprint. The second algorithm can be tuned by frequency thresholding to have a small footprint, and be less than one order of magnitude slower than the baseline system, while its keystroke savings improve over the baseline.},
author = {van den Bosch, Antal and Bogers, Toine},
doi = {10.1145/1409240.1409315},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/van den Bosch, Bogers - 2008 - Efficient context-sensitive word completion for mobile devices(2).pdf:pdf},
isbn = {9781595939524},
journal = {Proceedings of the 10th international conference on Human computer interaction with mobile devices and services - MobileHCI '08},
keywords = {con-,ergonomics,mobile devices,predictive text processing,scaling,text sensitivity,word completion},
pages = {465},
title = {{Efficient context-sensitive word completion for mobile devices}},
url = {http://portal.acm.org/citation.cfm?doid=1409240.1409315},
year = {2008}
}
@article{Bahl1983,
abstract = {Speech recognition is formulated as a problem of maximum likelihood decoding. This formulation requires statistical models of the speech production process. In this paper, we describe a number of statistical models for use in speech recognition. We give special attention to determining the parameters for such models from sparse data. We also describe two decoding methods, one appropriate for constrained artificial languages and one appropriate for more realistic decoding tasks. To illustrate the usefulness of the methods described, we review a number of decoding results that have been obtained with them.},
author = {Bahl, Lalit R and Jelinek, Frederick and Mercer, Robert L},
doi = {10.1109/TPAMI.1983.4767370},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bahl, Jelinek, Mercer - 1983 - A Maximum Likelihood Approach to Continuous Speech Recognition.pdf:pdf},
isbn = {1558601244},
issn = {0162-8828},
journal = {IEEE transactions on pattern analysis and machine intelligence},
keywords = {Index Terms-Markov models,maximum likelihood,parameter esti-mation,speech recognition,statistical models},
number = {2},
pages = {179--190},
pmid = {21869099},
title = {{A maximum likelihood approach to continuous speech recognition.}},
url = {https://www.cse.iitb.ac.in/{~}pb/cs626-2013/word-alignment/jelineck-speech-1983.pdf},
volume = {5},
year = {1983}
}
@book{Plag2003,
abstract = {This textbook provides an accessible introduction to the study of word-formation, focusing specifically on English. Assuming no prior linguistic knowledge, it explains the fundamentals of word-formation, encouraging students to undertake their own morphological analyses of English words, and familiarising them with the methodological tools to obtain and analyse relevant data. Cover; Half-title; Series-title; Title; Copyright; Contents; Preface; Abbreviations and notational conventions; Introduction: what this book is about and how it can be used; 1 Basic concepts; 2 Studying complex words; 3 Productivity and the mental lexicon; 4 Affixation; 5 Derivation without affixation; 6 Compounding; 7 Theoretical issues: modeling word-formation; Answer key to exercises; References; Subject index; Affix index; Author index.},
address = {Cambridge},
author = {Plag, Ingo.},
isbn = {0521525632},
pages = {240},
publisher = {Cambridge University Press},
title = {{Word-formation in English}},
year = {2003}
}
@article{mandelbrot1965information,
author = {Mandelbrot, Beno$\backslash${\^{}}$\backslash$it},
journal = {BB Wolman and E},
title = {{Information theory and psycholinguistics}},
year = {1965}
}
@article{Goodman2008,
archivePrefix = {arXiv},
arxivId = {arXiv:cs/0108005v1},
author = {Goodman, Joshua T},
eprint = {0108005v1},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Goodman - 2001 - A Bit of Progress in Language Modeling Extended Version.pdf:pdf},
primaryClass = {arXiv:cs},
title = {{A Bit of Progress in Language Modeling Extended Version}},
url = {http://www.research.microsoft.com},
year = {2008}
}
@techreport{BLS2016,
author = {{Bureau of Labor Statistics}},
title = {{Employed persons by detailed occupation and sex , 2015 annual averages}},
url = {https://www.bls.gov/cps/cpsaat11b.htm},
year = {2016}
}
@article{Vergyri2004,
abstract = {Language modeling is a difficult problem for languages with rich morphology. In this paper we investigate the use of morphology-based language models at different stages in a speech recognition system for conversational Arabic. Class-based and single-stream factored language models using morphological word representations are applied within an N-best list rescoring framework. In addition, we explore the use of factored language models in first-pass recognition, which is facilitated by two novel procedures: the data-driven optimization of a multi-stream language model structure, and the conversion of a factored language model to a standard word-based model. We evaluate these techniques on a large-vocabulary recognition task and demonstrate that they lead to perplexity and word error rate reductions.},
author = {Vergyri, Dimitra and Kirchhoff, Katrin and Duh, Kevin and Stolcke, Andreas and Park, Menlo},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Vergyri et al. - 2004 - Morphology-Based Language Modeling for Arabic Speech Recognition.pdf:pdf},
journal = {Interspeech},
keywords = {Arabic,language modeling,morphology,speech recognition},
pages = {4--7},
title = {{Morphology-Based Language Modeling for Arabic Speech Recognition}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.68.4042{\&}rep=rep1{\&}type=pdf http://academic.research.microsoft.com/Publication/2282409/morphology-based-language-modeling-for-arabic-speech-recognition},
year = {2004}
}
@incollection{DeMori1996,
abstract = {This chapter discusses cache-based language model for speech recognition. Automatic Speech Recognition (ASR) consists of two components: an acoustic component, which matches the acoustic input to words in its vocabulary, producing a set of the most plausible word candidates together with a probability for each, and the second component, which incorporates a language model, estimating the probability for each word in the vocabulary that it will occur, given a list of previously hypothesized words. The chapter focuses on the language model incorporated in the second component. A hypothesis is adopted in the chapter that a word used in the recent past is much more likely to be used sooner, from either its overall frequency in the language or that a 3g-gram model would suggest. The cache component of the combined model estimates the probability of a word from its recent frequency of use. The model uses a weighted average of the 3g-gram and cache components in calculating word probabilities, where the relative weights assigned to each component depend on the Part of Speech (POS). For purpose of comparison, a pure 3g-gram model has been created in the chapter, consisting of only the 3g-gram component of the combined model.},
author = {{De Mori}, Renato and Kuhn, Roland},
booktitle = {Recent Research Towards Advanced Man-Machine Interface Through Spoken Language},
doi = {10.1016/B978-044481607-8/50065-7},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/De Mori, Kuhn - 1996 - – A Cache-Based Natural Language Model for Speech Recognition.pdf:pdf},
isbn = {9780444816078},
number = {6},
pages = {219--228},
title = {{– A Cache-Based Natural Language Model for Speech Recognition}},
url = {http://visgraph.cs.ust.hk/biometrics/Papers/Voice/pami1990-06-01.pdf},
volume = {12},
year = {1996}
}
@techreport{Baroni2006,
abstract = {This document provides a tutorial introduction to the zipfR package (Evert and Baroni, 2007) for Large-Number-of-Rare-Events (LNRE) modeling of lexical distri- butions (Baayen, 2001). We assume that R is installed on your computer and that you have basic familiarity with it. If this is not the case, please start by visiting the R page at http://www.r-project.org/. The page provides links to download sites, documentation and introductory material, as well as a wide selection of textbooks on R programming.},
author = {Baroni, Marco and Evert, Stefan},
booktitle = {R tutorial},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Baroni, Evert - 2014 - The zipfR package for lexical statistics A tutorial introduction.pdf:pdf},
keywords = {R,R GNU},
number = {August},
pages = {1--25},
title = {{The zipfR package for lexical statistics: A tutorial introduction}},
year = {2006}
}
@techreport{Low2016,
abstract = {Text prediction is an application of language models to mobile devices. Currently, the state of the art models use neural networks. Unfortunately, mobile devices are constrainted in both computing power and space and are thus unable to run most (if not all) neural networks. Recently, however, character-level architectures have appeared that have outperformed previous architectures for machine transla-tion. They are advantageous in that they do not require a word embedding matrix and thus require a lot less space. This project evaluates one recent character-level architecture on the text prediction task. We find that the network performs qual-itatively well, as well as achieving perplexity levels close to existing methods on the Brown corpus.},
author = {Low, Melvin},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Low - Unknown - Character-level Recurrent Text Prediction(2).pdf:pdf},
pages = {1--6},
title = {{Character-level Recurrent Text Prediction}},
year = {2016}
}
@article{Khmaladze1988,
author = {Khmaladze, E. V.},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Khmaladze - 1988 - The statistical analysis of a large number of rare events.pdf:pdf},
journal = {Department of Mathematical Statistics},
language = {en},
month = {jan},
number = {R 8804},
pages = {1--21},
publisher = {CWI},
title = {{The statistical analysis of a large number of rare events}},
url = {http://oai.cwi.nl/oai/asset/5988/5988A.pdf},
year = {1988}
}
@article{Fazly2003,
abstract = {We investigate the effect of incorporat-ing syntactic information into a word-completion algorithm. We introduce lwo new algorithms lhal combine parl-of-speech tag trigrams with word bi-grams, and evaluate them with a test-bench constructed for the purpose. The results show a small but statistically sig-nificant improvement in keystroke sav-ings for one of our algorithms over base-lines that use only word n-grams.},
author = {Fazly, Afsaneh and Hirst, Graeme},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fazly, Hirst - 2003 - Testing the efficacy of part-of-speech information in word completion.pdf:pdf},
journal = {Eacl},
number = {1991},
pages = {9--16},
title = {{Testing the efficacy of part-of-speech information in word completion}},
url = {http://www.aclweb.org/anthology/W03-2502},
year = {2003}
}
@misc{Bailey2016,
author = {Bailey, Cathy},
booktitle = {Georgetown Linguistics},
title = {{Brown Corpus Word Frequency List}},
url = {http://www.lextutor.ca/freq/lists{\_}download/},
year = {2016}
}
@book{Zipf1935,
abstract = {Frequency counts of phonemes, morphemes, and words in samples of written discourse in diverse languages are presented in support of the generalization that the more complex any speech element, the less frequently does it occur. Thus, the greater the frequency of occurrence of words, the less tends to be their average length, and the smaller also is the number of different words. The relation between frequency and number of different words is said to be expressed by the formula ab2 = k, in which a represents the number of different words of a given frequency and b the frequency. The relationship between the magnitude of speech elements and their frequency is attributed to the operation of a "law" of linguistic change: that as the frequency of phonemes or of linguistic forms increases, their magnitude decreases. There is thus a tendency to "maintain an equilibrium" between length and frequency, and this tendency rests upon an "underlying law of economy." Human beings strive to maintain an "emotional equilibrium" between variety and repetitiveness of environmental factors and behavior. A speaker's discourse must represent a compromise between variety and repetitiveness adapted to the hearer's "tolerable limits of change in maintaining emotional equilibrium." This accounts for the maintenance of the relationship ab2 = k; the exponent of b expresses this "rate of variegation."},
author = {Zipf, George Kingsley},
doi = {10.1097/00005053-193701000-00041},
isbn = {9780415209762},
issn = {0022-3018},
pages = {336},
pmid = {6891221},
publisher = {Houghton Mifflin Company},
title = {{The Psycho-Biology Of Language: AN INTRODUCTION TO DYNAMIC PHILOLOGY}},
url = {http://psycnet.apa.org/psycinfo/1935-04756-000},
volume = {ix},
year = {1935}
}
@article{Wandmacher2008a,
abstract = {Most current word prediction systems make use of n-gram language models (LM) to estimate the probability of the following word in a phrase. In the past years there have been many attempts to enrich such language models with further syntactic or semantic information. We want to explore the predictive powers of Latent Semantic Analysis (LSA), a method that has been shown to provide reliable information on long-distance semantic dependencies between words in a context. We present and evaluate here several methods that integrate LSA-based information with a standard language model: a semantic cache, partial reranking, and different forms of interpolation. We found that all methods show significant improvements, compared to the 4-gram baseline, and most of them to a simple cache model as well.},
archivePrefix = {arXiv},
arxivId = {0801.4716},
author = {Wandmacher, Tonio and Antoine, Jean-Yves},
eprint = {0801.4716},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wandmacher, Antoine - Unknown - Methods to integrate a language model with semantic information for a word prediction component(2).pdf:pdf;:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wandmacher, Antoine - 2008 - Methods to integrate a language model with semantic information for a word prediction component.pdf:pdf},
journal = {Computational Linguistics},
number = {June},
pages = {10},
title = {{Methods to integrate a language model with semantic information for a word prediction component}},
url = {http://arxiv.org/abs/0801.4716 https://arxiv.org/ftp/arxiv/papers/0801/0801.4716.pdf},
year = {2008}
}
@article{Geza2002,
abstract = {The practical challenge of creating a Hungarian e-mail reader has initiated our work on statistical text analysis. The starting point was statistical analysis for automatic discrimination of the language of texts. Later it was extended to automatic re-generation of diacritic signs and more detailed language structure analysis. Parallel study of three different languages -Hungarian. German and English -using text corpora of similar size explores both similarities and differences. Corpora of publicly available Internet sources were used. The corpus size was the same (approximately 20Mbytes, 2.5-3.5 million word forms) for all languages. Besides traditional corpus coverage, word length and occurence statistics, some new features about prosodic boundaries (sentence beginning and final positions, preceding and following a comma) were also computed. Among others, it was found, that the coverage of corpora by the most frequent words follows a parallel logarithmic rule for all languages in the 40-85{\%} coverage range, known as Zipf's law in linguistics. The functions are much nearer for English and German than for Hungarian. Further conclusions are also drawn. The language detection and diacritic re-generation applications are discussed in detail with implications on Hungarian speech generation. Diverse further application domains, such as predictive text input, word hyphenation, language modeling in speech recognition, corpus-based speech synthesis, etc. are also foreseen.},
author = {Geza, Nemeth and {Csaba Zainko}},
doi = {10.1556/ALing.49.2002.3-4.8},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Geza, Csaba Zainko - 2002 - Multilingual Statistical Text Analysis, Zipf's Law and Hungarian Speech Generation.pdf:pdf},
issn = {1216-8076},
journal = {Corpus},
keywords = {Zipf's law,corpus analysis,corpus-based speech synthesis,language modeling,multilinguality,text corpora,word length},
number = {2001},
pages = {3--4},
title = {{Multilingual Statistical Text Analysis, Zipf's Law and Hungarian Speech Generation}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.120.679{\&}rep=rep1{\&}type=pdf},
volume = {49},
year = {2002}
}
@book{Jeffreys61,
address = {Oxford, England},
author = {Jeffreys, H},
booktitle = {Theory of Probability},
edition = {Third},
publisher = {Oxford},
title = {{Theory of Probability}},
year = {1961}
}
@misc{TheRFoundation2016,
author = {{The R Foundation}},
title = {{The Comprehensive R Archive Network}},
url = {https://cran.r-project.org/},
year = {2016}
}
@article{Agarwal2007a,
abstract = {The use of digital mobile phones has led to a tremendous increase in communication using SMS. On a phone keypad, multiple words are mapped to same numeric code. We propose a Context Based Word Prediction system for SMS messaging in which context is used to predict the most appropriate word for a given code. We extend this system to allow informal words (short forms for proper English words). The mapping from informal word to its proper English words is done using Double Metaphone Encoding based on their phonetic similarity. The results show 31{\%} improvement over the traditional frequency based word estimation. Introduction The growth of wireless technology has provided us with many new ways of communication such as SMS (Short Message Service). SMS messaging can also be used to interact with automated systems or participating in contests. With tremendous increase in Mobile Text Messaging, there is a need for an efficient text input system. With limited keys on the mobile phone, multiple letters are mapped to same number (8 keys, 2 to 9, for 26 alphabets). The many to one mapping of alphabets to numbers gives us same numeric code for multiple words.},
author = {Agarwal, Sachin},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Agarwal, Arora - Unknown - Context Based Word Prediction for Texting 1 Language(2).pdf:pdf},
journal = {Large Scale Semantic Access to Content (Text,},
pages = {360--368},
title = {{Context based word prediction for texting language}},
url = {http://dl.acm.org/citation.cfm?id=1931426},
year = {2007}
}
@article{Carlberger1997,
abstract = {Word prediction is the problem of guessing which words are likely to follow a given segment of text. A computer program performing word prediction can be an important writing aid for physically or linguistically handicapped people. Using a word predictor ensures correct spelling and saves keystrokes and effort for the user. The goal of this project was to design and implement a word predictor for Swedish that would suggest better words, and thus save more keystrokes, than any other word predictor available on the market. The constructed program uses a probabilistic language model based on the well-established ideas of the trigram predictor developed at IBM. By using such a model, the program can be easily modified to work with languages other than Swedish. In tests, the program has been shown to save more than 45 percent of the keystrokes for the user. The report focuses on the technical aspects of designing an efficient algorithm and optimizing it to save as many keystrokes as possible. Design och implementering av ett probabilistiskt ordprediktionsprogram Sammanfattning Ordprediktion {\"{a}}r att gissa vilket ord som sannolikt f{\"{o}}ljer en given sekvens av ord. Ett program som utf{\"{o}}r ordprediktion kan vara ett bra hj{\"{a}}lpmedel f{\"{o}}r motoriskt och spr{\aa}kligt handikappade vid datorarbete och kommunikation. Anv{\"{a}}ndningen av ett ordprediktionsprogram underl{\"{a}}ttar stavning och besparar knapptryckningar och d{\"{a}}rmed anstr{\"{a}}ngning f{\"{o}}r anv{\"{a}}ndaren. M{\aa}let med projektet var att utforma och implementera ett ordprediktionsprogram f{\"{o}}r svenska som f{\"{o}}resl{\aa}r b{\"{a}}ttre ord, och d{\"{a}}rmed besparar anv{\"{a}}ndaren fler knapp-tryckningar, {\"{a}}n n{\aa}got annat ordprediktionsprogram p{\aa} marknaden. Det konstruerade programmet anv{\"{a}}nder en probabilistisk spr{\aa}kmodell baserad p{\aa} den v{\"{a}}lk{\"{a}}nda trigram-prediktorn som utvecklats vid IBM. Detta val av spr{\aa}kmodell medf{\"{o}}r att programmet enkelt kan modifieras f{\"{o}}r att fungera f{\"{o}}r andra spr{\aa}k {\"{a}}n svenska. Utf{\"{o}}rda tester visar att programmet kan spara mer {\"{a}}n 45 procent av knapptryckningarna f{\"{o}}r anv{\"{a}}ndaren. I denna rapport har tyngdpunkten lagts p{\aa} de tekniska aspekterna av att skapa en effektiv algoritm och optimering av denna.},
author = {Carlberger, J.},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Carlberger - 1997 - Design and Implementation of a Probabilistic Word Prediciton Program(2).pdf:pdf},
journal = {Language},
title = {{Design and Implementation of a Probabilistic Word Prediciton Program}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.55.2073},
year = {1997}
}
@article{Sundarkantham2011,
abstract = {Language is a unique phenomenon that distinguishes man from other animals. It is our primary method of communication with each other, yet very little is understood about how language is acquired when we are infants. A greater understanding in this area would have the potential to improve man machine communication The problem that is attempted to be solved in this paper is that of programming a computer to play the Shannon Game. To play the Shannon game, one must predict which words are most likely to follow a given segment of English Text. Word Prediction would be most useful for writers with physical disabilities and severe spelling problems. The aim of this paper is to improve on existing results by writing a program that is capable of automatically inferring a grammar from a Natural Language Corpus, and applying this to the Shannon Game. To play the Shannon Game, a stochastic Grammar for an approximation to the target language must be inferred from a text sample, and as the quality of this grammar improves so too does the quality of the predictor that uses the inferred grammar. The proposed algorithm in the paper uses Support Vector Machine to perform the part of speech tagging which produces 97.6{\%} correct predictions.},
author = {Sundarkantham, K and Shalinie, S Mercy and Pushparathi, S.},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sundarkantham, Shalinie - Unknown - WORD PREDICTOR USING NATURAL LANGUAGE GRAMMAR INDUCTION TECHNIQUE(2).pdf:pdf},
issn = {18234690},
journal = {Journal of Engineering Science and Technology},
keywords = {K-means clustering,Natural language grammatical inference,Support vector machines},
number = {2},
pages = {204--215},
title = {{Word predictor using natural language grammar induction technique}},
url = {www.jatit.org},
volume = {6},
year = {2011}
}
@article{Stolcke2002,
abstract = {SRILM is a collection of C++ libraries, executable programs, and helper scripts designed to allow both production of and experimentation with statistical language models for speech recognition and other applications. SRILM is freely available for noncommercial purposes. The toolkit supports creation and evaluation of a variety of language model types based on N-gram statistics, as well as several related tasks, such as statistical tagging and manipulation of N-best lists and word lattices. This paper summarizes the functionality of the toolkit and discusses its design and implementation, highlighting ease of rapid prototyping, reusability, and combinability of tools.},
author = {Stolcke, Andreas},
doi = {10.1.1.157.2429},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Stolcke - 2002 - Srilm — an Extensible Language Modeling Toolkit.pdf:pdf},
journal = {Interspeech},
number = {Denver, Colorado},
pages = {901--904},
title = {{Srilm — an Extensible Language Modeling Toolkit}},
url = {http://www.speech.sri.com/ http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.7.461{\&}rep=rep1{\&}type=pdf},
volume = {2},
year = {2002}
}
@misc{Koohafkan2015,
author = {Koohafkan, Michael C},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Koohafkan, Maintainer - 2015 - Package 'kfigr' Integrated Code Chunk Anchoring and Referencing for R Markdown.pdf:pdf},
title = {{Package 'kfigr' | Integrated Code Chunk Anchoring and Referencing for R Markdown}},
year = {2015}
}
@misc{Brants2006,
author = {Brants, T and Franz, A},
booktitle = {Linguistic Data Consortium, Philiadelphia},
title = {{Web {\{}1T{\}} 5-gram version 1}},
url = {https://catalog.ldc.upenn.edu/ldc2006t13},
urldate = {2017-04-03},
year = {2006}
}
@book{Wierzbicka1996,
abstract = {Includes previously published material rev. and expanded for this publication. 1. Introduction -- 2. Survey of Semantic Primitives -- 3. Universal Grammar: The Syntax of Universal Semantic Primitives -- 4. Prototypes and Invariant -- 5. Semantic Primitives and Semantic Fields -- 6. Semantics and "Primitive Thought" -- 7. Semantic Complexity and the Role of Ostension in the Acquisition of Concepts -- 8. Against "Against Definitions" -- 9. Semantics and Lexicography -- 10. Meaning of Colour Terms and the Universals of Seeing -- 11. Semantics of Natural Kinds -- 12. Semantics and Ethnobiology -- 13. Semantic Rules in Grammar -- 14. Semantic Basis for Grammatical Description and Typology: Transitivity and Reflexives -- 15. Comparing Grammatical Categories across Languages: The Semantics of Evidentials.},
author = {Wierzbicka, Anna.},
isbn = {0198700024},
pages = {500},
publisher = {Oxford University Press},
title = {{Semantics : primes and universals}},
year = {1996}
}
@article{Gries2014,
author = {Gries, Stefan Th.},
doi = {10.1017/CBO9781139833882.005},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gries - Unknown - Quantitative corpus approaches to linguistic analysis seven or eight levels of resolution and lessons they teach us.pdf:pdf},
isbn = {9781107038509},
journal = {Developments in English: expanding electronic evidence},
number = {3},
pages = {29--47},
title = {{Quantitative corpus approaches to linguistic analysis: seven or eight levels of resolution and the lessons they teach us}},
volume = {i},
year = {2014}
}
@book{Goddard2002,
address = {Amsterdam},
author = {Goddard, Cliff},
pages = {5--40},
publisher = {John Benjamins},
title = {{Meaning and Universal Grammar: Theory and Empirical Findings}},
url = {http://www.une.edu.au/lcl/nsm/pdf/Goddard{\_}Ch1{\_}2002.pdf},
year = {2002}
}
@article{Gustavii2003a,
abstract = {Word prediction may be of great help in situations where text entering is cumbersome due to a physical or a cognitive disability, or due to the input device being small. Traditionally, word prediction has solely been based on statistic language modelling, but lately knowledge-based approaches, including the use of grammatical language descriptions, have entered the arena. By making use of grammar rules, the accuracy of the prediction suggestions is expected to increase, and the word predictor to give a more intelligent impression. We have defined and implemented a Swedish grammar for the FASTY word predictor. The gram- mar rules, defined in terms of traditional grammatical functions, are expressed in the procedural UCP- formalism. The grammar functions as a grammar checking filter, reranking the suggestions proposed by a statistic n-gram model on the basis of both confirming and rejecting rules. What structures to cover has been decided in accordance with an investigation on what syntactic errors are most frequently produced by the statistic model. The investigation led us to prioritize rules for handling word order in the main clause, agreement within the noun phrase, verb inflection and prepositional phrases. A preliminary evaluation of the grammar module was carried out, using Keystroke Saving Rate (KSR) estimations. The results showed only a slight improvement in KSR when adding the grammar module to the system, as compared to using only the statistic model based on word form bigrams and part-of-speech tag trigrams. A list length of one suggestion gave a larger improvement, than a list length of five, indicating that the strength of the grammar module lies in the reranking of already displayed suggestions, rather than the addition of new suggestions to a long list of suggestions.},
author = {Gustavii, Ebba and Pettersson, Eva},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gustavii, Pettersson - 2003 - A Swedish Grammar for Word Prediction(2).pdf:pdf},
number = {June},
title = {{A Swedish Grammar for Word Prediction}},
year = {2003}
}
@article{Ostrogonac2012,
author = {Ostrogonac, Stevan and Secujski, Milan and Miskovic, Dragisa},
doi = {10.1109/TELFOR.2012.6419309},
file = {:C$\backslash$:/Users/John/Documents/Data Science/Data Science Resources/Impact of training corpus size on the quality of different types of language models for Serbian.pdf:pdf},
isbn = {9781467329842},
journal = {2012 20th Telecommunications Forum, TELFOR 2012 - Proceedings},
keywords = {Language model,discrimination coefficient,evaluation,perplexity},
number = {November},
pages = {720--723},
title = {{Impact of training corpus size on the quality of different types of language models for Serbian}},
year = {2012}
}
@misc{Wiktionary2016,
author = {Wiktionary},
title = {{Appendix:English internet slang - Wiktionary}},
url = {https://en.wiktionary.org/wiki/Appendix:English{\_}internet{\_}slang},
year = {2016}
}
@article{Carlberger1997,
author = {Carlberger, J.},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Carlberger - 1997 - Design and Implementation of a Probabilistic Word Prediciton Program(2).pdf:pdf},
journal = {Language},
title = {{Design and Implementation of a Probabilistic Word Prediciton Program}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.55.2073},
year = {1997}
}
@article{Markov1906,
author = {Markov, A},
journal = {Izvestiya Fiziko-matematicheskogo obschestva pri Kazanskom universitete},
number = {tom 15},
pages = {135--156},
title = {{Rasprostranenie zakona bol'shih chisel na velichiny, zavisyaschie drug ot druga}},
volume = {2-ya seriy},
year = {1906}
}
@article{Wild2015,
abstract = {The basic idea of latent semantic analysis (LSA) is, that text do have a higher order (=latent semantic) structure which, however, is obscured by word usage (e.g. through the use of synonyms or polysemy). By using conceptual indices that are derived statistically via a truncated singular value decomposition (a two-mode factor analysis) over a given document-term matrix, this variability problem can be overcome.},
author = {Wild, Fridolin},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - 2015 - Package 'lsa' Title Latent Semantic Analysis.pdf:pdf},
pages = {1--11},
title = {{Package ‘ lsa '}},
url = {https://cran.r-project.org/web/packages/lsa/lsa.pdf},
year = {2015}
}
@book{Adger2003,
abstract = {1. Core Concepts -- 2. Morphosyntactic Features -- 3. Constituency and Theta Roles -- 4. Representing Phrase Structure -- 5. Functional Categories I -- TP -- 6. Subjects and Objects -- 7. Functional Categories II -- the DP 8. Functional Categories III -- CP -- 9. Wh-movement -- 10. Locality.},
address = {Oxford},
author = {Adger, David.},
isbn = {0199243700},
pages = {424},
publisher = {Oxford University Press},
title = {{Core syntax : a minimalist approach}},
year = {2003}
}
@article{Sundarkantham2011,
abstract = {Language is a unique phenomenon that distinguishes man from other animals. It is our primary method of communication with each other, yet very little is understood about how language is acquired when we are infants. A greater understanding in this area would have the potential to improve man machine communication The problem that is attempted to be solved in this paper is that of programming a computer to play the Shannon Game. To play the Shannon game, one must predict which words are most likely to follow a given segment of English Text. Word Prediction would be most useful for writers with physical disabilities and severe spelling problems. The aim of this paper is to improve on existing results by writing a program that is capable of automatically inferring a grammar from a Natural Language Corpus, and applying this to the Shannon Game. To play the Shannon Game, a stochastic Grammar for an approximation to the target language must be inferred from a text sample, and as the quality of this grammar improves so too does the quality of the predictor that uses the inferred grammar. The proposed algorithm in the paper uses Support Vector Machine to perform the part of speech tagging which produces 97.6{\%} correct predictions.},
author = {Sundarkantham, K and Shalinie, S Mercy and Pushparathi, S.},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sundarkantham, Shalinie - Unknown - WORD PREDICTOR USING NATURAL LANGUAGE GRAMMAR INDUCTION TECHNIQUE(2).pdf:pdf},
issn = {18234690},
journal = {Journal of Engineering Science and Technology},
keywords = {K-means clustering,Natural language grammatical inference,Support vector machines},
number = {2},
pages = {204--215},
title = {{Word predictor using natural language grammar induction technique}},
url = {www.jatit.org},
volume = {6},
year = {2011}
}
@inproceedings{How2005,
abstract = {1},
author = {How, Yijue and Kan, Min-Yen},
booktitle = {Proceedings of Human Computer Interfaces International},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/How, Kan - 2005 - Optimizing predictive text entry for short message service on mobile phones.pdf:pdf},
pages = {1--10},
title = {{Optimizing predictive text entry for short message service on mobile phones}},
year = {2005}
}
@article{Carlberger1997,
abstract = {Word prediction is the problem of guessing which words are likely to follow a given segment of text. A computer program performing word prediction can be an important writing aid for physically or linguistically handicapped people. Using a word predictor ensures correct spelling and saves keystrokes and effort for the user. The goal of this project was to design and implement a word predictor for Swedish that would suggest better words, and thus save more keystrokes, than any other word predictor available on the market. The constructed program uses a probabilistic language model based on the well-established ideas of the trigram predictor developed at IBM. By using such a model, the program can be easily modified to work with languages other than Swedish. In tests, the program has been shown to save more than 45 percent of the keystrokes for the user. The report focuses on the technical aspects of designing an efficient algorithm and optimizing it to save as many keystrokes as possible. Design och implementering av ett probabilistiskt ordprediktionsprogram Sammanfattning Ordprediktion {\"{a}}r att gissa vilket ord som sannolikt f{\"{o}}ljer en given sekvens av ord. Ett program som utf{\"{o}}r ordprediktion kan vara ett bra hj{\"{a}}lpmedel f{\"{o}}r motoriskt och spr{\aa}kligt handikappade vid datorarbete och kommunikation. Anv{\"{a}}ndningen av ett ordprediktionsprogram underl{\"{a}}ttar stavning och besparar knapptryckningar och d{\"{a}}rmed anstr{\"{a}}ngning f{\"{o}}r anv{\"{a}}ndaren. M{\aa}let med projektet var att utforma och implementera ett ordprediktionsprogram f{\"{o}}r svenska som f{\"{o}}resl{\aa}r b{\"{a}}ttre ord, och d{\"{a}}rmed besparar anv{\"{a}}ndaren fler knapp-tryckningar, {\"{a}}n n{\aa}got annat ordprediktionsprogram p{\aa} marknaden. Det konstruerade programmet anv{\"{a}}nder en probabilistisk spr{\aa}kmodell baserad p{\aa} den v{\"{a}}lk{\"{a}}nda trigram-prediktorn som utvecklats vid IBM. Detta val av spr{\aa}kmodell medf{\"{o}}r att programmet enkelt kan modifieras f{\"{o}}r att fungera f{\"{o}}r andra spr{\aa}k {\"{a}}n svenska. Utf{\"{o}}rda tester visar att programmet kan spara mer {\"{a}}n 45 procent av knapptryckningarna f{\"{o}}r anv{\"{a}}ndaren. I denna rapport har tyngdpunkten lagts p{\aa} de tekniska aspekterna av att skapa en effektiv algoritm och optimering av denna.},
author = {Carlberger, J.},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Carlberger - 1997 - Design and Implementation of a Probabilistic Word Prediciton Program(2).pdf:pdf},
journal = {Language},
title = {{Design and Implementation of a Probabilistic Word Prediciton Program}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.55.2073},
year = {1997}
}
@article{Duggan2013,
abstract = {Fully 91{\%} of American adults own a cell phone and many use the devices for much more than phone calls. In our most recent nationally representative survey, we checked in on some of the most popular activities people perform on their cell phones: Cell phone activities The {\%} of cell phone owners who use their cell phone to{\ldots} 81 send or receive text messages 60 access the internet 52 send or receive email 50 download apps 49 get directions, recommendations, or other location-based information 48 listen to music 21 participate in a video call or video chat 8 “check in” or share your location Source: Pew Research Center's Internet {\&} American Life Project Spring Tracking Survey, April 17 – May 19, 2013. N=2,076 cell phone owners. Interviews were conducted in English and Spanish and on landline and cell phones. The margin of error for results based on all cell phone owners is +/- 2.4 percentage points. Texting, accessing the internet and sending and receiving email remain popular. Some 50{\%} of cell owners download apps—up from 22{\%} in 2009. Many use certain location-based services like getting directions or recommendations. Nearly half of cell owners (48{\%}) use their phones to listen to music. The proportion of cell owners who use video calling has tripled since May 2011. Overall, almost all activities have seen steady upward growth over time},
author = {Duggan, Maeve},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Duggan - 2013 - Cell Phone Activities 2013.pdf:pdf},
pages = {18},
title = {{Cell Phone Activities 2013}},
url = {http://pewinternet.org/{~}/media//Files/Reports/2013/PIP{\_}Cell Phone Activities May 2013.pdf{\%}5Cnhttp://www.pewinternet.org/files/old-media//Files/Reports/2013/PIP{\_}Cell Phone Activities May 2013.pdf},
year = {2013}
}
@article{Derczynski2013a,
abstract = {Part-of-speech information is a pre-requisite in many NLP algorithms. However, Twitter text is difficult to part-of-speech tag: it is noisy, with linguistic errors and idiosyncratic style. We present a detailed error analysis of existing taggers, motivating a series of tagger augmentations which are demonstrated to improve performance. We identify and evaluate techniques for improving English part-of-speech tagging performance in this genre. Further, we present a novel approach to system combination for the case where available taggers use different tagsets, based on voteconstrained bootstrapping with unlabeled data. Coupled with assigning prior probabilities to some tokens and handling of unknown words and slang, we reach 88.7{\%} tagging accuracy (90.5{\%} on development data). This is a new high in PTB-compatible tweet part-of-speech tagging, reducing token error by 26.8{\%} and sentence error by 12.2{\%}. The model, training data and tools are made available.},
author = {Derczynski, Leon and Ritter, Alan and Clark, Sam and Bontcheva, Kalina},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Derczynski et al. - 2013 - Twitter part-of-speech tagging for all Overcoming sparse and noisy data.pdf:pdf},
issn = {13138502},
journal = {Proceedings of the Recent Advances in Natural Language Processing},
number = {September},
pages = {198--206},
title = {{Twitter part-of-speech tagging for all: Overcoming sparse and noisy data}},
url = {http://www.aclweb.org/website/old{\_}anthology/R/R13/R13-1026.pdf},
year = {2013}
}
@article{Wickham2016a,
author = {Wickham, Hadley and Maintainer, ]},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wickham, Maintainer - 2016 - Package ‘reshape'.pdf:pdf},
title = {{Package ‘reshape'}},
url = {http://had.co.nz/reshape},
year = {2016}
}
@article{Kornai2002,
abstract = {The commonsensical assumption that any language has only finitely many words is shown to be false by a combination of formal and empirical arguments. Zipf's Law and related formulas are investigated and a more complex model is offered.},
author = {Kornai, Andr{\'{a}}s},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kornai - 2002 - How many words are there.pdf:pdf},
issn = {10542353},
journal = {Glottometrics},
keywords = {s law,vocabulary size,zipf},
number = {2002},
pages = {61--86},
pmid = {12374001},
title = {{How Many Words Are There?}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.6.5245{\&}rank=1},
volume = {4},
year = {2002}
}
@article{Clarkson1997,
author = {Clarkson, Philip and Rosenfeld, Ronald},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Clarkson, Rosenfeld - 1997 - Statistical Language Modeling Using the CMU-Cambridge Toolkit.pdf:pdf},
journal = {Proceedings Eurospeech '97},
pages = {2707--2710},
title = {{Statistical Language Modeling Using the {\{}CMU{\}}--Cambridge Toolkit}},
url = {http://repository.cmu.edu/compsci},
year = {1997}
}
@article{Carmignani2006,
author = {Carmignani, Nicola},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Carmignani - 2006 - Predicting Words and Sentences using Statistical Models(2).pdf:pdf},
title = {{Predicting Words and Sentences using Statistical Models}},
year = {2006}
}
@book{Zipf49,
annote = {Basic Study on word frequency relative to word lengths},
author = {Zipf, George K},
keywords = {bibtex-import},
publisher = {Addison-Wesley (Reading MA)},
title = {{Human Behavior and the Principle of Least Effort}},
year = {1949}
}
@article{Trnka2008b,
author = {Trnka, Keith},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Trnka - Unknown - WORD PREDICTION TECHNIQUES FOR USER ADAPTATION AND SPARSE DATA MITIGATION.pdf:pdf},
journal = {Information Sciences},
keywords = {proposal},
title = {{Word Prediction Techniques for User Adaptation and Sparse Data Mitigation Ph . D . Thesis Proposal}},
url = {https://pdfs.semanticscholar.org/e7cd/4e4e31ed6d5b52593c2e434029f3fa1ec50a.pdf},
year = {2008}
}
@book{Zipf:36,
address = {London},
author = {Zipf, George K},
publisher = {Routledge},
title = {{The Psychobiology of Language: An Introduction to Dynamic Philology}},
year = {1936}
}
@article{Johnson1944,
author = {Johnson, W},
journal = {Psychological Monographs},
pages = {1--15},
title = {{Studies in Language Behaviour: I. {\{}{\{}{\}}A{\{}{\}}{\}} Program of Research}},
volume = {56},
year = {1944}
}
@misc{Silvola,
author = {Silvola, Vesa},
title = {{VariKN toolkit}},
url = {http://vsiivola.github.io/variKN/},
urldate = {2017-04-03}
}
@article{Ney1994,
abstract = {We study the problem of stochastic language modelling from the viewpoint of introducing suitable structures into the conditional probability distributions. The task of these distributions is to predict the probability of a new word by looking at M or even all predecessor words. The conventional approach is to limit M to 1 or 2 and to interpolate the resulting bigram and trigram models with a unigram model in a linear fashion. However, there are many other structures that can be used to model the probabilistic dependences between the predecessor word and the word to be predicted. The structures considered in this paper are: nonlinear interpolation as an alternative to linear interpolation; equivalence classes for word histories and single words; cache memory and word associations. For the optimal estimation of nonlinear and linear interpolation parameters, the leaving-one-out method is systematically used. For the determination of word equivalence classes in a bigram model, an automatic clustering procedure has been adapted. To capture long-distance dependences, we consider various models for word-by-word dependences; the cache model may be viewed as a special type of self-association. Experimental results are presented for two text databases, a German database and an English database},
author = {Ney, Hermann and Essen, Ute and Kneser, Reinhard},
doi = {10.1006/csla.1994.1001},
issn = {08852308},
journal = {Computer Speech {\&} Language},
month = {jan},
number = {1},
pages = {1--38},
title = {{On Structuring Probabilistic Dependences in Stochastic Language Modelling}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0885230884710011},
volume = {8},
year = {1994}
}
@article{Agarwal2007b,
abstract = {The use of digital mobile phones has led to a tremendous increase in communication using SMS. On a phone keypad, multiple words are mapped to same numeric code. We propose a Context Based Word Prediction system for SMS messaging in which context is used to predict the most appropriate word for a given code. We extend this system to allow informal words (short forms for proper English words). The mapping from informal word to its proper English words is done using Double Metaphone Encoding based on their phonetic similarity. The results show 31{\%} improvement over the traditional frequency based word estimation. Introduction The growth of wireless technology has provided us with many new ways of communication such as SMS (Short Message Service). SMS messaging can also be used to interact with automated systems or participating in contests. With tremendous increase in Mobile Text Messaging, there is a need for an efficient text input system. With limited keys on the mobile phone, multiple letters are mapped to same number (8 keys, 2 to 9, for 26 alphabets). The many to one mapping of alphabets to numbers gives us same numeric code for multiple words.},
author = {Agarwal, Sachin},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Agarwal, Arora - Unknown - Context Based Word Prediction for Texting 1 Language(2).pdf:pdf},
journal = {Large Scale Semantic Access to Content (Text,},
pages = {360--368},
title = {{Context based word prediction for texting language}},
url = {http://dl.acm.org/citation.cfm?id=1931426},
year = {2007}
}
@article{Trnka2006,
abstract = {Word prediction can be used for enhancing the communication ability of persons with speech and language impairments. In this work, we explore two methods of adapting a language model to the topic of conversation, and apply these methods to the prediction of fringe words.},
author = {Trnka, Keith and Yarrington, Debra and McCoy, Kathleen and Pennington, Christopher},
doi = {10.1145/1111449.1111509},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Trnka et al. - 2006 - Topic modeling in fringe word prediction for AAC.pdf:pdf},
isbn = {1595932879},
journal = {Proceedings of the 11th international conference on Intelligent user interfaces - IUI '06},
keywords = {language mod-,topic modeling,word prediction},
pages = {276},
title = {{Topic modeling in fringe word prediction for AAC}},
url = {http://portal.acm.org/citation.cfm?doid=1111449.1111509},
year = {2006}
}
@article{Trnka2007a,
abstract = {Word prediction can be used to enhance the communication rate of people with disabilities who use Augmentative and Alternative Communication (AAC) devices. We use statistical methods in a word prediction system, which are trained on a corpus, and then measure the efficacy of the resulting system by calculating the theoretical keystroke savings on some held out data. Ideally training and testing should be done on a large corpus of AAC text covering a variety of topics, but no such corpus exists. We discuss training and testing on a wide variety of corpora meant to approximate text from AAC users. We show that training on a combination of in-domain data with out-of-domain data is often more beneficial than either data set alone and that advanced language modeling such as topic modeling is portable even when applied to very different text.},
author = {Trnka, Keith and McCoy, Kathleen F.},
doi = {10.1145/1296843.1296877},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Trnka, McCoy - 2007 - Corpus studies in word prediction.pdf:pdf},
isbn = {9781595935731},
journal = {Proceedings of the 9th international ACM SIGACCESS conference on Computers and accessibility - Assets '07},
keywords = {cor-,language modeling,statistical methods,word prediction},
pages = {195},
title = {{Corpus studies in word prediction}},
url = {http://portal.acm.org/citation.cfm?doid=1296843.1296877},
year = {2007}
}
@article{Johnson1944a,
author = {Johnson, W},
number = {2},
pages = {1--15},
title = {{Studies in Language Behavior: I. A program of research}},
volume = {56},
year = {1944}
}
@article{Toutanova2003,
abstract = {We present a new part-of-speech tagger that demonstrates the following ideas: (i) explicit use of both preceding and following tag contexts via a dependency network representation, (ii) broad use of lexical features, including jointly conditioning on multiple consecutive words, (iii) effective use of priors in conditional loglinear models, and (iv) ﬁne-grained modeling of unknown word features. Using these ideas together, the resulting tagger gives a 97.24{\%} accuracy on the Penn Treebank WSJ, an error reduction of 4.4{\%} on the best previous single automatically learned tagging result},
author = {Toutanova, Kristina and Klein, Dan and Manning, Christopher D},
doi = {10.3115/1073445.1073478},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Toutanova et al. - Unknown - Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network.pdf:pdf},
journal = {In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1 (NAACL '03),},
pages = {252--259},
title = {{Feature-rich part-of-speech tagging with a cyclic dependency network}},
url = {http://dl.acm.org/citation.cfm?id=1073478},
year = {2003}
}
@article{Biber1993,
abstract = {The present paper addresses a number of issues related to achieving 'representativeness' in linguistic corpus design, including: discussion of what it means to 'represent' a lan-guage, definition of the target population, stratified versus proportional sampling of a language, sampling within texts, and issues relating to the required sample size (number of texts) of a corpus. The paper distinguishes among various ways that linguistic features can be distributed within and across texts; it analyses the distributions of several particular features, and it discusses the implications of these distribu-tions for corpus design. The paper argues that theoretical research should be prior in corpus design, to identify the situational parameters that distinguish among texts in a speech community, and to identify the types of linguistic features that will be analysed in the corpus. These theoretical considerations should be complemented by empirical investigations of linguistic variation in a pilot corpus of texts, as a basis for specific sampling decisions. The actual construction of a corpus would then proceed in cycles: the original design based on theoretical and pilot-study analyses, followed by collection of texts, followed by further empirical investigations of lin-guistic variation and revision of the design.},
author = {Biber, Douglas},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Biber - 1993 - Representativeness in Corpus Design.Pdf.pdf:pdf},
title = {representativeness in corpus design.pdf},
year = {1993}
}
@article{Owoputi2013,
abstract = {We consider the problem of part-of-speech tagging for informal, online conversational text. We systematically evaluate the use of large-scale unsupervised word clustering and new lexical features to improve tagging accuracy. With these features, our system achieves state-of-the-art tagging results on both Twitter and IRC POS tagging tasks; Twitter tagging is improved from 90{\%} to 93{\%} accuracy (more than 3{\%} absolute). Quali- tative analysis of these word clusters yields insights about NLP and linguistic phenomena in this genre. Additionally, we contribute the first POS annotation guidelines for such text and release a new dataset of English language tweets annotated using these guidelines. Tagging software, annotation guidelines, and large-scale word clusters are available at: http://www.ark.cs.cmu.edu/TweetNLP This paper describes release 0.3 of the “CMU Twitter Part-of-Speech Tagger” and annotated data},
author = {Owoputi, Olutobi and O'Connor, Brendan and Dyer, Chris and Gimpel, Kevin and Schneider, Nathan and Smith, Noah A},
doi = {10.1177/001316446002000104},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Owoputi et al. - 2013 - Improved Part-of-Speech Tagging for Online Conversational Text with Word Clusters(2).pdf:pdf},
isbn = {9781937284473},
journal = {Proceedings of NAACL-HLT 2013},
number = {June},
pages = {380--390},
title = {{Improved Part-of-Speech Tagging for Online Conversational Text with Word Clusters}},
year = {2013}
}
@misc{Wikipedia2016,
author = {Wikipedia},
booktitle = {Wikipedia},
title = {{Wikipedia:List of English contractions}},
url = {http://www.oxforddictionaries.com/us/definition/english/amn't},
urldate = {2016-11-30},
year = {2016}
}
@manual{Feinerer2015a,
annote = {R package version 0.6-2},
author = {Feinerer, Ingo and Hornik, Kurt},
title = {{tm: Text Mining Package}},
url = {http://cran.r-project.org/package=tm},
year = {2015}
}
@article{Moreno-Sanchez2016,
abstract = {Despite being a paradigm of quantitative linguistics, Zipf's law for words suffers from three main problems: its formulation is ambiguous, its validity has not been tested rigorously from a statistical point of view, and it has not been confronted to a representatively large number of texts. So, we can summarize the current support of Zipf's law in texts as anecdotic. We try to solve these issues by studying three different versions of Zipf's law and fitting them to all available English texts in the Project Gutenberg database (consisting of more than 30 000 texts). To do so we use state-of-the art tools in fitting and goodness-of-fit tests, carefully tailored to the peculiarities of text statistics. Remarkably, one of the three versions of Zipf's law, consisting of a pure power-law form in the complementary cumulative distribution function of word frequencies, is able to fit more than 40{\%} of the texts in the database (at the 0.05 significance level), for the whole domain of frequencies (from 1 to the maximum value), and with only one free parameter (the exponent).},
archivePrefix = {arXiv},
arxivId = {1509.04486},
author = {Moreno-Sanchez, Isabel and Font-Clos, Francesc and Corral, Alvaro},
doi = {10.1371/journal.pone.0147073},
eprint = {1509.04486},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Moreno-S{\'{a}}nchez, Font-Clos, Corral - Unknown - Large-scale analysis of Zipf 's law in English texts.pdf:pdf},
issn = {19326203},
journal = {PLoS ONE},
keywords = {PACS numbers:},
number = {1},
pmid = {26800025},
title = {{Large-scale analysis of Zipf's law in English texts}},
volume = {11},
year = {2016}
}
@misc{Norvig,
author = {Norvig, Peter},
title = {{English Letter Frequency Counts: Mayzner Revisited or ETAOIN SRHLDCU}},
url = {http://norvig.com/mayzner.html}
}
@article{Suster2011,
author = {Suster, Simoň},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Suster - 2011 - Measuring lexical and syntactic complexity of the language production of the World-of-Warcraft gaming community(2).pdf:pdf},
title = {{Measuring lexical and syntactic complexity of the language production of the World-of-Warcraft gaming community}},
year = {2011}
}
@article{Edward1996,
abstract = {Many disabled persons in the world are unable to communicate verbally with those around them. This may be because they have motor-control diiculties and so cannot operate the muscles which actually produce the speech. Conversely, i t m a y be a neurophysiological problem, either congenital or as a result of trauma, resulting in an inability of the brain to create the speech in the place. Either way the diiculties inherent with such problems are immense, and anything which can be used to help the situation can bring welcome relief. Many people have n o v erbal communication skills at all and as such need some form of prosthetic aid known as an Augmentative and Alternative Communication AAC device. There are many forms of device from very basic pointing boards to complex computer systems and a device will be chosen to suit the needs and abilities of an individual. However, people with such diiculties often have their problems compounded by not being able to use the rest of their bodies properly either. If one cannot use one's voice box because of motor-control diiculties then it is very likely that one will not be able to co-ordinate limbs properly. As such, using a complex device can be very diicult, and needless to say time consuming. For instance, typing on a keyboard can become an almost impossible task. The most common method of operating an AAC device is with a single switch, selecting the letters one-by-one as the system scans through them on a screen or lightboard. This produces prohibitively slow communication rates and as a consequence, even using an ordinary AAC device, users cannot join in with a normal conversation. A method is required which will improve the rate at which users can participate in a conversation using an AAC device.},
author = {Edward, Matthew and Wood, John},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Edward, Wood - 1996 - Matthew Edward John Wood.pdf:pdf},
keywords = {text input speed,word prediction},
number = {June},
title = {{Matthew Edward John Wood}},
year = {1996}
}
@article{Bernicot,
abstract = {The purpose of this research was to gain insights into SMS communication among French-speaking adolescents. We analyzed the effects of the writers' characteristics (age, gender, and SMS-messaging experience) on message length (number of characters and number of words), dialogue structure (with or without an opening and a closing), and message function (informative vs. relational). The SMS messages were produced in a real-world situation. We found differences across writers' characteristics for all the dependant variables. The commonly reported distinctions between girls and boys were mitigated. Moreover, for dialogical structure, the messages differed from those found in traditional oral and written interactions since 73{\%} of them did not have the conventional opening-message-closing format (the opening and/or the closing was missing). The results are discussed in terms of the specific characteristics that define the SMS register, and potentially relevant approaches to be taken in future research are addressed.},
author = {Bernicot, Josie and Volckaert-Legrier, Olga and Goumi, Antonine and Bert-Erboul, Alain},
doi = {10.1016/j.pragma.2012.07.009},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bernicot et al. - Unknown - Forms and Functions of SMS Messages A Study of Variations in a Corpus Written by Adolescents.pdf:pdf},
issn = {1701-1715},
keywords = {French language,adolescents,gender,register,text-messaging,writing},
title = {{Forms and Functions of SMS Messages: A Study of Variations in a Corpus Written by Adolescents}}
}
@article{Kneser1997,
abstract = {A new method is presented to quickly adapt a given language model to local text characteristics. The ba-sic approach is to choose the adaptive models as close as possible to the background estimates while con-straining them to respect the locally estimated uni-gram probabilities. Several means are investigated to speed up the calculations. We measure both perplex-ity and word error rate to gauge the quality of our model.},
author = {Kneser, Reinhard and Peters, Jochen and Klakow, Dietrich},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kneser, Peters, Klakow - 1997 - Language model adaptation using dynamic marginals.pdf:pdf},
number = {140403},
pages = {1--24},
title = {{Language model adaptation using dynamic marginals}},
url = {http://ai2-s2-pdfs.s3.amazonaws.com/f3f3/be564291d3782ac99d27810038f297bfe7ba.pdf},
volume = {2},
year = {1997}
}
@article{Matiasek2003,
abstract = {In this paper, we report about some preliminary experiments in which we tried to improve the performance of a state-of-the-art Predictive Typing sys-tem for the German language by adding a collocation-based prediction compo-nent. This component tries to ex-ploit the fact that texts have a topic and are semantically coherent. Thus, the appearance in a text of a certain word can be a cue that other, semanti-cally related words are likely to appear soon. The collocation-based module ex-ploits this kind of topical/semantic re-latedness by relying on statistics about the co-occurrence of words within a large window of text in the training corpus. Our current experimental re-sults indicate that using the collocation-based prediction module has a small but consistent positive effect on the perfor-mance of the system.},
author = {Matiasek, Johannes and Baroni, Marco},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Matiasek, Baroni - Unknown - Exploiting long distance collocational relations in predictive typing.pdf:pdf},
journal = {Proceedings of the EACL Workshop on Language Modeling for Text Entry Methods},
keywords = {text completion,word completion},
pages = {1--8},
title = {{Exploiting Long Distance Collocational Relations in Predictive Typing}},
year = {2003}
}
@article{Chen1996,
abstract = {We present an extensive empirical comparison of several smoothing techniques in the domain of language modeling, including those described by Jelinek and Mercer (1980), Katz (1987), and Church and Gale (1991). We investigate for the first time how factors such as training data size, corpus (e.g., Brown versus Wall Street Journal), and n-gram order (bigram versus trigram) affect the relative performance of these methods, which we measure through the cross-entropy of test data. In addition, we introduce two novel smoothing techniques, one a variation of Jelinek-Mercer smoothing and one a very simple linear interpolation technique, both of which outperform existing methods.},
archivePrefix = {arXiv},
arxivId = {cmp-lg/9606011},
author = {Chen, Stanley F and Goodman, Joshua},
doi = {10.3115/981863.981904},
eprint = {9606011},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen, Goodman - 1999 - An empirical study of smoothing techniques for language modeling.pdf:pdf},
isbn = {0885-2308},
issn = {08852308},
journal = {Proceedings of the 34th Annual Meeting on Association for Computational Linguistics},
pages = {310--318},
pmid = {25246403},
primaryClass = {cmp-lg},
title = {{An Empirical Study of Smoothing Techniques for Language Modeling}},
url = {http://www.idealibrary.com http://arxiv.org/abs/cmp-lg/9606011{\%}5Cnhttp://dx.doi.org/10.3115/981863.981904},
volume = {13},
year = {1996}
}
@article{Kilgarriff2001,
abstract = {Corpus linguistics lacks strategies for describing and compar-ing corpora. Currently most descriptions of corpora are textual, and questions such as 'what sort of a corpus is this?', or 'how does this corpus compare to that?' can only be answered impressionistically. This paper considers various ways in which different corpora can be compared more objectively. First we address the issue, 'which words are particularly characteristic of a corpus?', reviewing and critiquing the statistical methods which have been applied to the question and proposing the use of the Mann-Whitney ranks test. Results of two corpus com-parisons using the ranks test are presented. Then, we consider measures for corpus similarity. After discussing limitations of the idea of corpus similarity, we present a method for evaluat-ing corpus similarity measures. We consider several measures and establish that a $\chi$ 2 -based one performs best. All methods considered in this paper are based on word and ngram fre-quencies; the strategy is defended.},
author = {Kilgarriff, Adam},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kilgarriff - 2001 - 29 COMPARING CORPORA(2).pdf:pdf},
journal = {International Journal of Corpus Linguistics},
number = {1},
pages = {97--133},
title = {{29 COMPARING CORPORA}},
volume = {6},
year = {2001}
}
@article{Bickel2005,
abstract = {We explore the benefit that users in several application areas can$\backslash$nexperience from a "tab-complete" editing assistance function. We$\backslash$ndevelop an evaluation metric and adapt N-gram language models to$\backslash$nthe problem of predicting the subsequent words, given an initial$\backslash$ntext fragment. Using an instance-based method as baseline, we empirically$\backslash$nstudy the predictability of call-center emails, personal emails,$\backslash$nweather reports, and cooking recipes.},
author = {Bickel, Steffen and Haider, Peter and Scheffer, Tobias},
doi = {10.3115/1220575.1220600},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bickel, Haider, Scheffer - Unknown - Predicting Sentences using N-Gram Language Models.pdf:pdf},
journal = {Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing},
pages = {193--200},
title = {{Predicting sentences using N-gram language models}},
url = {http://dx.doi.org/10.3115/1220575.1220600},
year = {2005}
}
@article{Neunerdt2013a,
abstract = {—Using social media tools such as blogs and forums have become more and more popular in recent years. Hence, a huge collection of social media texts from different communities is available for accessing user opinions, e.g., for marketing studies or acceptance research. Typically, methods from Natural Language Processing are applied to social media texts to automatically recognize user opinions. A fundamental component of the linguistic pipeline in Natural Language Processing is Part-of-Speech tagging. Most state-of-the-art Part-of-Speech taggers are trained on newspaper corpora, which differ in many ways from non-standardized social media text. Hence, applying common taggers to such texts results in performance degradation. In this paper, we present extensions to a basic Markov model tagger for the annotation of social media texts. Considering the German standard Stuttgart/ ubinger TagSet (STTS), we distinguish 54 tag classes. Applying our approach improves the tagging accuracy for social media texts considerably, when we train our model on a combination of annotated texts from newspapers and Web comments.},
author = {Neunerdt, Melanie and Reyer, Michael and Mathar, Rudolf},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Neunerdt, Reyer, Mathar - Unknown - A POS Tagger for Social Media Texts Trained on Web Comments(2).pdf:pdf},
journal = {Polibits},
keywords = {German,Index Terms—Natural language processing,opinion mining,part-of-speech tagging},
number = {48},
pages = {61--68},
title = {{A POS Tagger for Social Media Texts trained on web comments}},
url = {http://polibits.ojs.gelbukh.com/ojs/index.php/polibits/article/viewFile/1783/1723},
year = {2013}
}
@article{Ghayoomia,
abstract = {—The prediction task in national language processing means to guess the missing letter, word, phrase, or sentence that likely follow in a given segment of a text. Since 1980s many systems with different methods were developed for different languages. In this paper an overview of the existing prediction methods that have been used for more than two decades are described and a general classification of the approaches is presented. The three main categories of the classification are statistical modeling, knowledge-based modeling, and heuristic modeling (adaptive).},
author = {Ghayoomi, Masood and Momtazi, Saeedeh},
doi = {10.1109/ICSMC.2009.5346027},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ghayoomi, Momtazi - Unknown - An Overview on the Existing Language Models for Prediction Systems as Writing Assistant Tools.pdf:pdf;:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ghayoomi, Momtazi - Unknown - An Overview on the Existing Language Models for Prediction Systems as Writing Assistant Tools(2).pdf:pdf},
isbn = {9781424427949},
issn = {1062922X},
journal = {Conference Proceedings - IEEE International Conference on Systems, Man and Cybernetics},
keywords = {Assistant technology,Index Terms—Word prediction,Lan-guage modeling,Language modeling,Word prediction},
pages = {5083--5087},
title = {{An Overview on the Existing Language Models for Prediction Systems as Writing Assistant Tools}},
url = {http://hpsg.fu-berlin.de/{~}ghayoomi/Publishedpapers/ghayoomi-2009[3].pdf},
year = {2009}
}
@article{Neunerdt2013a,
abstract = {—Using social media tools such as blogs and forums have become more and more popular in recent years. Hence, a huge collection of social media texts from different communities is available for accessing user opinions, e.g., for marketing studies or acceptance research. Typically, methods from Natural Language Processing are applied to social media texts to automatically recognize user opinions. A fundamental component of the linguistic pipeline in Natural Language Processing is Part-of-Speech tagging. Most state-of-the-art Part-of-Speech taggers are trained on newspaper corpora, which differ in many ways from non-standardized social media text. Hence, applying common taggers to such texts results in performance degradation. In this paper, we present extensions to a basic Markov model tagger for the annotation of social media texts. Considering the German standard Stuttgart/ ubinger TagSet (STTS), we distinguish 54 tag classes. Applying our approach improves the tagging accuracy for social media texts considerably, when we train our model on a combination of annotated texts from newspapers and Web comments.},
author = {Neunerdt, Melanie and Reyer, Michael and Mathar, Rudolf},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Neunerdt, Reyer, Mathar - Unknown - A POS Tagger for Social Media Texts Trained on Web Comments(2).pdf:pdf},
journal = {Polibits},
keywords = {German,Index Terms—Natural language processing,opinion mining,part-of-speech tagging},
number = {48},
pages = {61--68},
title = {{A POS Tagger for Social Media Texts trained on web comments}},
url = {http://polibits.ojs.gelbukh.com/ojs/index.php/polibits/article/viewFile/1783/1723},
year = {2013}
}
@book{Taylor2015,
author = {Taylor, John},
pages = {93},
title = {{The Oxford Handbook of the Word}},
year = {2015}
}
@article{Ghayoomi2005a,
abstract = {Word prediction is the problem of guessing which words are likely to follow in a given segment of a text to help a user with disabilities. As the user enters each letters of the required word, the system displays a list of the most probable words that could appear in that position. In our research we designed and implemented a word predictor for the Persian language. Three standard performance metrics were used to evaluate the system including keystroke saving, the most important one. The system achieved 57.57{\%} saving in keystrokes.},
author = {Ghayoomi, Masood and Assi, Seyyed Mostafa},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ghayoomi, Assi - 2005 - Word Prediction in a Running Text A Statistical Language Modeling for the Persian Language(2).pdf:pdf},
journal = {Proceedings of the Australasian Language Technology Workshop},
number = {December},
pages = {57--63},
title = {{Word Prediction in a Running Text: A Statistical Language Modeling for the Persian Language}},
year = {2005}
}
@book{Katamba2005,
abstract = {2nd ed. 'English Words' assumes no prior knowledge of linguistics in introducing the vocabulary of modern English usage. It covers meaning, history, pronunciation {\&} the structure of words. This new edition has been extensively updated with new chapters, new exercises, an improved index {\&} links to web resources. 1. Introduction -- 2. What is a word? -- 3. Close encounters of a morphemic kind -- 4. Building words -- 5. lexicon with layers -- 6. Word meaning -- 7. lexical mosaic : sources of English vocabulary -- 8. Words galore : innovation and change -- 9. Should English be spelt as she is spoke? -- 10. Speech recognition -- 11. Speech production.},
author = {Katamba, Francis},
isbn = {0415298938},
pages = {322},
publisher = {Routledge},
title = {{English words : structure, history, usage}},
year = {2005}
}
@article{Suster2011,
author = {Suster, Simoň},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Suster - 2011 - Measuring lexical and syntactic complexity of the language production of the World-of-Warcraft gaming community(2).pdf:pdf},
title = {{Measuring lexical and syntactic complexity of the language production of the World-of-Warcraft gaming community}},
year = {2011}
}
@misc{Beal2016,
author = {Beal, Vangie},
title = {{Huge List of Text Message {\&} Chat Abbreviations - Webopedia}},
url = {http://www.webopedia.com/quick{\_}ref/textmessageabbreviations.asp},
year = {2016}
}
@article{BNC2005,
abstract = {The written part of the (90{\%}) includes, for example, extracts from regional and national newspapers, specialist periodicals and journals for all ages and interests, academic books and popular fiction, published and unpublished letters and memoranda, school and university},
author = {BNC, Webmaster},
isbn = {Version 3, BNC XML Edition},
journal = {British National Corpus},
number = {2001},
pages = {6},
title = {{British National Corpus}},
url = {http://www.natcorp.ox.ac.uk/},
year = {2005}
}
@article{Hornik2009,
author = {Hornik, Kurt and Buchta, Christian and Zeileis, Achim},
doi = {10.1007/s00180-008-0119-7},
journal = {Computational Statistics},
number = {2},
pages = {225--232},
title = {{Open-Source Machine Learning: {\{}R{\}} Meets {\{}Weka{\}}}},
volume = {24},
year = {2009}
}
@misc{Michalke2015,
author = {eik Michalke, M.},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - koRpus An R Package for Text Analysis.pdf:pdf},
title = {{koRpus: An R Package for Text Analysis}},
url = {http://reaktanz.de/?c=hacking{\&}s=koRpus},
year = {2015}
}
@article{Chelba1998,
abstract = {The paper presents a language model that devel-ops syntactic structure and uses it to extract mean-ingful information from the word history, thus en-abling the use of long distance dependencies. The model assigns probability to every joint sequence of words–binary-parse-structure with headword an-notation and operates in a left-to-right manner — therefore usable for automatic speech recognition. The model, its probabilistic parameterization, and a set of experiments meant to evaluate its predictive power are presented; an improvement over standard trigram modeling is achieved.},
archivePrefix = {arXiv},
arxivId = {arXiv:cs/9811022v2},
author = {Chelba, Ciprian and Jelinek, Frederick},
eprint = {9811022v2},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chelba, Jelinek - 2000 - Exploiting Syntactic Structure for Language Modeling.pdf:pdf;:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chelba, Jelinek - Unknown - Exploiting Syntactic Structure for Language Modeling.pdf:pdf},
journal = {Proceedings of the Thirty-Sixth Annual Meeting of the {\{}A{\}}ssociation for {\{}C{\}}omputational {\{}L{\}}inguistics and Seventeenth International Conference on Computational Linguistics},
pages = {225--231},
primaryClass = {arXiv:cs},
title = {{Exploiting Syntactic Structure for Language Modeling}},
url = {https://arxiv.org/pdf/cs/9811022.pdf http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.556.577{\&}rep=rep1{\&}type=pdf},
year = {1998}
}
@book{Bell:1990:TC:77753,
address = {Upper Saddle River, NJ, USA},
author = {Bell, Timothy C and Cleary, John G and Witten, Ian H},
isbn = {0-13-911991-4},
publisher = {Prentice-Hall, Inc.},
title = {{Text Compression}},
year = {1990}
}
@article{Ling2005,
abstract = {Text messaging–or texting–via mobile telephones has become a fixture in many parts of the world. The ability to cheaply send text messages on a mobile asynchronous basis was adopted first by teens and is now spreading to other parts of the population. This said, texting is not an intuitive process. The interface is difficult to master, and the technology is being pressed into areas for which it was not necessarily intended. It is into this arena that systems of predictive texting have been introduced.},
author = {Ling, Rich},
doi = {10.13140/RG.2.1.1922.6089},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ling - Unknown - THE LENGTH OF TEXT MESSAGES AND USE OF PREDICTIVE TEXTING WHO USES IT AND HOW MUCH DO THEY HAVE TO SAY.pdf:pdf},
journal = {Association of Internet Researchers},
number = {l},
pages = {1--18},
title = {{The Length of Text Messages and Use of Predictive Texting: Who Uses it and How Much Do They Have to Say?}},
volume = {4},
year = {2005}
}
@article{Evert2004,
abstract = {This paper describes a population model for word frequency distributions based on the Zipf-Mandelbrot law, corresponding to the word frequency distribution induced by a random character sequence. The model, which has convenient analytical and numerical properties, is shown to be adequate for the description of language data extracted by automatic means from large text corpora. It can thus be used to study the problems faced by the statistical analysis of such data in the field of natural-language processing.},
author = {Evert, Stefan},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Evert - Unknown - A Simple LNRE Model for Random Character Sequences.pdf:pdf},
isbn = {2-930344-49-0},
journal = {Proceedings of JADT},
keywords = {1 introduction to lexical,1 this,area of lexical statistics,cooccurrence statistics,is based on random,lexical statistics,lnre models,model assumes a population,most work in the,of types w 1,probabilities $\pi$ 1,random text,s,sampling with replacement,statistics and lnre models,w s with occurrence,zipf-mandelbrot law,$\pi$ s},
pages = {1--12},
title = {{A simple LNRE model for random character sequences}},
url = {http://www.cogsci.uni-osnabrueck.de/{~}severt/PUB/Evert2004a.pdf},
year = {2004}
}
@article{Dunlop2000,
abstract = {Mobile phone networks are increasingly supporting the transmission of textual messages between mobile phones and between mobile phones and other services. This paper describes the current text entry method on mobile phones and describes a new text entry method using a single key-press per letter together with a large dictionary of words for disambiguation. This approach, which is similar to technology recently licensed, independently, to several phone companies, is then extended with automatic word completion. The paper reports the results of initial user tests comparing the text entry methods, analysis of word clashes with the dictionary-based methods and keystroke level modelling of the different input methods.},
author = {Dunlop, M.D. and Crossan, Andrew},
doi = {10.1007/BF01324120},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - No Title(6).pdf:pdf},
issn = {0949-2054},
journal = {Personal Technologies},
number = {2-3},
pages = {134--143},
title = {{Predictive text entry methods for mobile phones}},
url = {http://link.springer.com/article/10.1007/BF01324120},
volume = {4},
year = {2000}
}
@article{Sichel1986,
author = {Sichel, H S},
doi = {10.1007/BF01599746},
issn = {1572-9486},
journal = {Czechoslovak Journal of Physics B},
number = {1},
pages = {133--137},
title = {{The GIGP distribution model with applications to physics literature}},
url = {http://dx.doi.org/10.1007/BF01599746},
volume = {36},
year = {1986}
}
@misc{M.eikmichalke2016,
annote = {From Duplicate 1 (koRpus: An R Package for Text Analysis - m.eik michalke)

(Version 0.06-5)},
author = {eik Michalke, M. and M.eik michalke},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - koRpus An R Package for Text Analysis.pdf:pdf},
title = {{koRpus: An R Package for Text Analysis}},
url = {http://reaktanz.de/?c=hacking{\&}s=koRpus},
year = {2016}
}
@article{Manning2014,
abstract = {We describe the design and use of the Stanford CoreNLP toolkit, an extensible pipeline that provides core natural language analysis. This toolkit is quite widely used, both in the research NLP community and also among commercial and government users of open source NLP technology. We suggest that this follows from a simple, approachable design, straightforward interfaces, the inclusion of robust and good quality analysis components, and not requiring use of a large amount of associated baggage.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Manning, Christopher D and Bauer, John and Finkel, Jenny and Bethard, Steven J and Surdeanu, Mihai and McClosky, David},
doi = {10.3115/v1/P14-5010},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Manning et al. - Unknown - The Stanford CoreNLP Natural Language Processing Toolkit.pdf:pdf},
isbn = {9781941643006},
issn = {1098-6596},
journal = {Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations},
pages = {55--60},
pmid = {25246403},
title = {{The Stanford CoreNLP Natural Language Processing Toolkit}},
url = {http://aclweb.org/anthology/P14-5010},
year = {2014}
}
@article{Zhu,
author = {Zhu, Xiaojin},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhu - Unknown - CS769 Spring 2010 Advanced Natural Language Processing Basic Text Process.pdf:pdf},
title = {{CS769 Spring 2010 Advanced Natural Language Processing Basic Text Process}}
}
@article{Siivola2007a,
abstract = {— -gram models are the most widely used language models in large vocabulary continuous speech recognition. Since the size of the model grows rapidly with respect to the model order and available training data, many methods have been proposed for pruning the least relevant -grams from the model. However, correct smoothing of the -gram probability distri-butions is important and performance may degrade significantly if pruning conflicts with smoothing. In this paper, we show that some of the commonly used pruning methods do not take into account how removing an -gram should modify the backoff distributions in the state-of-the-art Kneser–Ney smoothing. To solve this problem, we present two new algorithms: one for pruning Kneser–Ney smoothed models, and one for growing them incrementally. Experiments on Finnish and English text corpora show that the proposed pruning algorithm provides considerable improvements over previous pruning algorithms on Kneser–Ney-smoothed models and is also better than the baseline entropy pruned Good–Turing smoothed models. The models created by the growing algorithm provide a good starting point for our pruning algorithm, leading to further improvements. The improvements in the Finnish speech recognition over the other Kneser–Ney smoothed models are statistically significant, as well.},
author = {Siivola, Vesa and Hirsim{\"{a}}ki, Teemu and Virpioja, Sami},
doi = {10.1109/TASL.2007.896666},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Siivola, Hirsim{\"{a}}ki, Virpioja - 2007 - On Growing and Pruning Kneser–Ney Smoothed N-Gram Models(2).pdf:pdf},
issn = {15587916},
journal = {IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING},
keywords = {Index Terms—Modeling,Modeling,Natural languages,Smoothing methods,Speech recognition,natural languages,smoothing methods,speech recognition},
number = {5},
pages = {1617--1624},
title = {{On Growing and Pruning Kneser – Ney Smoothed N -Gram Models}},
url = {https://d9b0b70b-a-62cb3a1a-s-sites.googlegroups.com/site/vesassiivola/publications/TASLP2007.pdf?attachauth=ANoY7co{\_}yL6A2AcUftxT0UElk2lwJzg6GS5qtDIzthvhtLroCrvE0o{\_}T2nJQ9inyZ-0cQ2tsXoCCqDrabPuADszWxeHzt8{\_}DtnYcC4a-kfFeuF1lJe7DWYH3YYEg0xl09utrk1hkg5SREJu6DX},
volume = {15},
year = {2007}
}
@article{Kremers,
author = {Kremers, Jerome},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kremers - Unknown - TEXT MESSAGING ON RELATIONAL INTIMACY The Effects of Text Messaging on Relational Intimacy.pdf:pdf},
title = {{TEXT MESSAGING ON RELATIONAL INTIMACY The Effects of Text Messaging on Relational Intimacy}}
}
@article{Ling2007a,
author = {Ling, Rich and Baron, Naomi S and R{\&}d, Telenor},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ling, Baron - 2007 - Text Messaging and IM Linguistic Comparison of American College Data.pdf:pdf},
title = {{Text Messaging and IM: Linguistic Comparison of American College Data}},
year = {2007}
}
@misc{TylerWRinker2016,
author = {{Tyler W Rinker}},
title = {{qdap: Quantitative Discourse Analysis Package}},
url = {https://trinker.github.io/qdap/ http://github.com/trinker/qdap},
year = {2016}
}
@article{Arnold2016a,
author = {Arnold, Taylor and Tilton, Lauren},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Arnold, Tilton - 2016 - Package 'coreNLP' Wrappers Around Stanford CoreNLP Tools.pdf:pdf},
title = {{Package 'coreNLP' | Wrappers Around Stanford CoreNLP Tools}},
year = {2016}
}
@article{VandenBosch2008a,
abstract = {Word completion is a basic technology for reducing the effort involved in text entry on mobile devices and in augmentative communication devices, where efficiency and ease of use are needed, but where a low memory footprint is also required. Standard solutions compress a lexicon into a suffix tree with a small memory footprint and high retrieval speed. Keystroke savings, a measurable correlate of text entry effort gain, typically improve when the algorithm would also take into account the previous word; however, this comes at the cost of a large footprint. We develop two word completion algorithms that encode the previous word in the input. The first algorithm utilizes a character buffer that includes a fixed number of recent keystrokes, including those belonging to previous words. The second algorithm includes the complete previous word as an extra input feature. In simulation studies, the first algorithm yields marked improvements in keystroke savings, but has a large memory footprint. The second algorithm can be tuned by frequency thresholding to have a small footprint, and be less than one order of magnitude slower than the baseline system, while its keystroke savings improve over the baseline.},
author = {van den Bosch, Antal and Bogers, Toine},
doi = {10.1145/1409240.1409315},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/van den Bosch, Bogers - 2008 - Efficient context-sensitive word completion for mobile devices(2).pdf:pdf},
isbn = {9781595939524},
journal = {Proceedings of the 10th international conference on Human computer interaction with mobile devices and services - MobileHCI '08},
keywords = {con-,ergonomics,mobile devices,predictive text processing,scaling,text sensitivity,word completion},
pages = {465},
title = {{Efficient context-sensitive word completion for mobile devices}},
url = {http://portal.acm.org/citation.cfm?doid=1409240.1409315},
year = {2008}
}
@article{Li2005,
abstract = {We propose an integrated approach to interactive word-completion for users with linguistic disabilities in which semantic knowledge combines with n-gram probabilities to predict semantically more-appropriate words than n-gram methods alone. First, semantic relatives are found for English words, specifically for nouns, and they form the semantic knowledge base. The selection process for these semantically related words is first to rank the pointwise mutual information of co-occurring words in a large corpus and then to identify the semantic relatedness of these words by a Lesk-like filter. Then, the semantic knowledge is used to measure the semantic association of completion candidates with the context. Those that are semantically appropriate to the context are promoted to the top positions in prediction lists due to their high association with context. Experimental results show a performance improvement when using the integrated model for the completion of nouns.},
author = {Li, Jianhua and Hirst, Graeme},
doi = {10.1145/1090785.1090809},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li, Hirst - Unknown - Semantic Knowledge in Word Completion(3).pdf:pdf},
isbn = {1-59593-159-7},
journal = {Proceedings of the 7th International ACM SIGACCESS Conference on Computers and Accessibility},
keywords = {For-Motor Disability,Full Paper,Tec-Natural Language Processing,Tec-word prediction,assistive technologies,linguistic semantics,pointwise mutual information,word completion},
pages = {121--128},
title = {{Semantic Knowledge in Word Completion}},
url = {http://doi.acm.org/10.1145/1090785.1090809},
year = {2005}
}
@misc{Norvig2015,
author = {Norvig, Peter},
booktitle = {Quora},
title = {{What is the average number of letters for an English word? - Quora}},
url = {https://www.quora.com/What-is-the-average-number-of-letters-for-an-English-word},
year = {2015}
}
@misc{Benoit2016a,
author = {Benoit, Kenneth},
title = {{Quanteda | Quantitative Text Analysis with R}},
url = {https://kbenoit.github.io/quanteda/intro/overview.html},
year = {2016}
}
@incollection{JelMer80,
address = {Amsterdam},
author = {Jelinek, Fred and Mercer, Robert L},
booktitle = {Proceedings, Workshop on Pattern Recognition in Practice},
editor = {Gelsema, Edzard S and Kanal, Laveen N},
keywords = {2000 book nlp},
pages = {381--397},
publisher = {North Holland},
title = {{Interpolated estimation of {\{}M{\}}arkov source parameters from sparse data}},
year = {1980}
}
@article{Zurhellena,
abstract = {This article is one of a series of short essays, collectively titled " Further Explorations, " published as part of a special issue of Oral Tradition in honor of John Miles Foley's 65 th birthday and 2011 retirement. The surprise Festschrift, guest-edited by Lori and Scott Garner entirely without his knowledge, celebrates John's tremendous impact on studies in oral tradition through a series of essays contributed by his students from the University of Missouri-Columbia (1979-present) and from NEH Summer Seminars that he has directed (1987-1996). http://journal.oraltradition.org/issues/26ii In The Pathways Project, John Miles Foley (2011-) discusses briefly the social role of SMS (Short Message Service), suggesting that " even so-called text messaging, a misnomer of sizeable proportions given that the activity really amounts to a long-distance emergent communication enacted virtually, knits people together into interactive groups and keeps them connected and 'present' to one another. " 1 In this essay, I propose a merger of current research on text messaging and the study of oral traditions in order to shed light on the relationship between this new mode of communication and the workings of consciousness being transformed by the eAgora. Focusing first on the limitations of text messaging as a medium that unexpectedly encouraged language innovation, we can explore how text messaging language merges effective communicative practices from both oral and written technologies in order to generate more efficient communication within a newly-limited, writing-based technology. Moreover, in addition to its efficiency, the kind of linguistic play found in text messaging can be viewed as a source of pleasure for those who engage in texting (" texters "). Thus, by employing the discourse of orality and literacy, we can explain how text messaging, while impossible to imagine without the myriad writing technologies mastered before it, actually encourages its literacy-obsessed users to practice communicative techniques more often found within oral cultures, or more precisely, communicative techniques found in cultures in the incipient stages of literacy. Such cultures are ripe for language innovation precisely because they have begun to record knowledge but have not yet standardized the recording procedure. Coincident with a perspective that sees text messaging as bridging a consciousness gap between oral and literate cultures, then, is the recognition that close study of the ways in which text messaging reworks language could lead to fruitful discoveries about the most current ways in which Computer-Mediated Communication (CMC) directs human life toward ever-emerging horizons of consciousness. When David Crystal (2008) hyperbolized the emergence of text messaging in the following passage, this form of communication was already a well-developed medium. Nevertheless, his humorous figuring of text messaging's inception, while not quite accurate, highlights precisely the form's limits that made it such an unlikely competitor in the tightly-wound market of twenty-first-century technologies (173-74): Oral Tradition, 26/2 (2011): 637-624},
author = {Zurhellen, Sarah},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zurhellen - 2011 - SMS and Oral Tradition.pdf:pdf},
title = {{" A Misnomer of Sizeable Proportions " : SMS and Oral Tradition}}
}
@article{Ney1994a,
abstract = {We study the problem of stochastic language modelling from the viewpoint of introducing suitable structures into the conditional probability distributions. The task of these distributions is to predict the probability of a new word by looking at M or even all predecessor words. The conventional approach is to limit M to 1 or 2 and to interpolate the resulting bigram and trigram models with a unigram model in a linear fashion. However, there are many other structures that can be used to model the probabilistic dependences between the predecessor word and the word to be predicted. The structures considered in this paper are: nonlinear interpolation as an alternative to linear interpolation; equivalence classes for word histories and single words; cache memory and word associations. For the optimal estimation of nonlinear and linear interpolation parameters, the leaving-one-out method is systematically used. For the determination of word equivalence classes in a bigram model, an automatic clustering procedure has been adapted. To capture long-distance dependences, we consider various models for word-by-word dependences; the cache model may be viewed as a special type of self-association. Experimental results are presented for two text databases, a German database and an English database},
author = {Ney, Hermann and Essen, Ute and Kneser, Reinhard},
doi = {10.1006/csla.1994.1001},
issn = {08852308},
journal = {Computer Speech {\&} Language},
month = {jan},
number = {1},
pages = {1--38},
title = {{On Structuring Probabilistic Dependences in Stochastic Language Modelling}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0885230884710011},
volume = {8},
year = {1994}
}
@book{Harper2005,
abstract = {SMS or Text is one of the most popular forms of messaging. Yet, despite its immense popularity, SMS has remained unexamined by science. Not only that, but the commercial organisations, who have been forced to offer SMS by a demanding public, have had very little idea why it has been successful. Indeed, they have, until very recently, planned to replace SMS with other messaging services such as MMS. This book is the first to bring together scientific studies into the values that ‘texting' provides, examining both cultural variation in countries as different as the Philippines and Germany, as well as the differences between SMS and other communications channels like Instant Messaging and the traditional letter. It presents usability and design research which explores how SMS will evolve and what is likely to be the pattern of person-to-person messaging in the future. In short, Inside Text is a fundamental resource for anyone interested in mobile communications at the start of the 21st Century The book will be of interest to anyone in the CHI, CSCW and mobile communications research areas, as well as sociologists, anthropologists, communications scientists and policy makers.},
author = {Harper, R and Palen, L and Taylor, A},
editor = {Harper, Richard and Palen, Leysia and Taylor, Alex},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Harper, Palen, Taylor - Unknown - The Inside Text Social, Cultural and Design Perspectives on SMS.pdf:pdf},
isbn = {1402030606},
keywords = {no-tag},
publisher = {Springer},
title = {{The Inside Text : Social, Cultural and Design Perspectives on SMS (The Computer Supported Cooperative Work Series)}},
url = {http://www.amazon.fr/exec/obidos/ASIN/1402030592/citeulike04-21},
year = {2005}
}
@inproceedings{Komatsu2005a,
author = {Komatsu, Hiroyuki and Takabayashi, Satoru and Masui, Toshiyuki},
booktitle = {Proceedings of the 2005 International Conference on Active Media Technology, AMT 2005},
doi = {10.1109/AMT.2005.1505271},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Komatsu, Takabayashi, Masui - 2005 - Corpus-based predictive text input(2).pdf:pdf},
isbn = {0780390350},
title = {{Corpus-based predictive text input}},
year = {2005}
}
@article{Wandmacher2008a,
abstract = {Most current word prediction systems make use of n-gram language models (LM) to estimate the probability of the following word in a phrase. In the past years there have been many attempts to enrich such language models with further syntactic or semantic information. We want to explore the predictive powers of Latent Semantic Analysis (LSA), a method that has been shown to provide reliable information on long-distance semantic dependencies between words in a context. We present and evaluate here several methods that integrate LSA-based information with a standard language model: a semantic cache, partial reranking, and different forms of interpolation. We found that all methods show significant improvements, compared to the 4-gram baseline, and most of them to a simple cache model as well.},
archivePrefix = {arXiv},
arxivId = {0801.4716},
author = {Wandmacher, Tonio and Antoine, Jean-Yves},
eprint = {0801.4716},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wandmacher, Antoine - 2008 - Methods to integrate a language model with semantic information for a word prediction component.pdf:pdf},
journal = {Computational Linguistics},
number = {June},
pages = {10},
title = {{Methods to integrate a language model with semantic information for a word prediction component}},
url = {http://arxiv.org/abs/0801.4716},
year = {2008}
}
@misc{Johnson2016,
author = {Johnson, Bryan},
booktitle = {TechCrunch},
title = {{The combination of human and artificial intelligence will define humanity's future | TechCrunch}},
url = {https://techcrunch.com/2016/10/12/the-combination-of-human-and-artificial-intelligence-will-define-humanitys-future/},
urldate = {2016-12-31},
year = {2016}
}
@book{zipf32selective,
author = {Zipf, G K},
publisher = {Harvard University Press},
title = {{Selective Studies and the Principle of Relative Frequency in Language}},
year = {1932}
}
@misc{Benoit2016,
author = {Benoit, Kenneth and Nulty, Paul and Watanabe, Kohei and Lauderdale, Benjamin and Obeng, Adam and Barber{\'{a}}, Pablo and Lowe, Will},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Benoit et al. - 2016 - Package 'quanteda' Title Quantitative Analysis of Textual Data.pdf:pdf},
title = {{Package 'quanteda': Quantitative Analysis of Textual Data}},
year = {2016}
}
@article{Carlberger1997b,
abstract = {Prolet, a word prediction program, has been in use for the last ten years as a writing aid, and was designed to accelerate the writing process and minimize the writing effort for persons with motor dysfunction.},
author = {Carlberger, Alice and Carlberger, Johan and Magnuson, Tina and Hunnicutt, M Sharon and Palazuelos-cagigas, Sira E and Navarro, Santiago A and Electronica, Ingenieria},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Carlberger Johan Carlberger Tina Magnuson Sharon Hunnicutt, Palazuelos-Cagigas Santiago Aguilera Navarro - Unknown - Profet, A New Gener.pdf:pdf},
journal = {In Proceedings of the 2nd Workshop on NLP for Communication Aids},
keywords = {word-prediction},
pages = {23--28},
title = {{Profet, A New Generation of Word Prediction:}},
url = {http://ucrel.lancs.ac.uk/acl/W/W97/W97-0504.pdf http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.11.5847},
year = {1997}
}
@article{Katz1987,
abstract = {See, stats, and : https : / / www . researchgate . net / publication / 2572004 Estimation the recognizer Article Source : CiteSeer CITATIONS 37 READS 663 4 , including : Some : Computers (CHIL) project IARPA Lori French 342 , 023 SEE G . Adda French 161 , 609 SEE All . The . All - text and , letting .Abstract-Thedescriptionofanoveltypeofrn-gramlanguagemodelisgiven.Themodeloffers,viaanonlinearrecursiveprocedure,acom-putationandspaceefficientsolutiontotheproblemofestimatingprob-abilitiesfromsparsedata.Thissolutioncomparesfavorablytootherproposedmethods.Whilethemethodhasbeendevelopedforandsuc-cessfullyimplementedintheIBMRealTimeSpeechRecognizers,itsgeneralitymakesitapplicableinotherareaswheretheproblemofes-timatingprobabilitiesfromsparsedataarises.Sparsenessofdataisaninherentpropertyofanyrealtext,anditisaproblemthatonealwaysencounterswhilecollectingfre-quencystatisticsonwordsandwordsequences(m-grams)fromatextoffinitesize.Thismeansthatevenforaverylargedatacol-lection,themaximumlikelihoodestimationmethoddoesnotallowustoadequatelyestimateprobabilitiesofrarebutneverthelesspos-siblewordsequences-manysequencesoccuronlyonce("single-tons");manymoredonotoccuratall.Inadequacyofthemaximumlikelihoodestimatorandthenecessitytoestimatetheprobabilitiesofm-gramswhichdidnotoccurinthetextconstitutetheessenceoftheproblem.Themainideaoftheproposedsolutiontotheproblemistore-duceunreliableprobabilityestimatesgivenbytheobservedfre-quenciesandredistributethe"freed"probability"mass"amongm-gramswhichneveroccurredinthetext.Thereductionisachievedbyreplacingmaximumlikelihoodestimatesform-gramshavinglowcountswithrenormalizedTuring'sestimates[l],andthere-distributionisdoneviatherecursiveutilizationoflowerlevelcon-ditionaldistributions.WefoundTuring'smethodattractivebe-causeofitssimplicityanditscharacterizationastheoptimalempiricalBayes'estimatorofamultinomialprobability.Robbinsin[2]introducestheempiricalBayes'methodologyandNadasin[3]givesvariousderivationsoftheTuring'sformula.LetNbeasampletextsizeandletn,bethenumberofwords(m-grams)whichoccurredinthetextexactlyrtimes,sothatN=Crn,.},
author = {Katz, Slavam},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Katz - Unknown - EstimationofProbabilitiesfromSparseDataforthe LanguageModelComponentofaSpeechRecognizer.pdf:pdf},
number = {3},
pages = {35--1987},
title = {{Estimation of Probabilities from Sparse Data for the Language Model Component of a Speech Recognizer}},
url = {https://www.researchgate.net/profile/Lori{\_}Lamel/publication/2572004{\_}Estimation{\_}of{\_}probabilities{\_}from{\_}Sparse{\_}data{\_}for{\_}the{\_}language{\_}model{\_}component{\_}of{\_}a{\_}speech{\_}recognizer/links/5422cdc10cf26120b7a55d60.pdf},
year = {1987}
}
@article{Gimpel2011,
abstract = {We address the problem of part-of-speech tagging for English data from the popular micro-blogging service Twitter. We develop a tagset, annotate data, develop features, and report tagging results nearing 90{\%} accuracy. The data and tools have been made available to the research community with the goal of enabling richer text analysis of Twitter and related social media data sets.},
author = {Gimpel, Kevin and Schneider, Nathan and O'Connor, Brendan and Das, Dipanjan and Mills, Daniel and Eisenstein, Jacob and Heilman, Michael and Yogatama, Dani and Flanigan, Jeffrey and Smith, Noah A},
doi = {10.1.1.206.3224},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gimpel et al. - 2011 - Part-of-Speech Tagging for Twitter Annotation, Features, and Experiments.pdf:pdf},
isbn = {978-1-932432-88-6},
journal = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Shortpapers},
number = {2},
pages = {42--47},
title = {{Part-of-Speech Tagging for Twitter: Annotation, Features, and Experiments}},
year = {2011}
}
@article{Moore2010,
abstract = {We address the problem of selecting non- domain-specific language model training data to build auxiliary language models for use in tasks such as machine transla- tion. Our approach is based on comparing the cross-entropy, according to domain- specific and non-domain-specifc language models, for each sentence of the text source used to produce the latter language model. We show that this produces better language models, trained on less data, than both random data selection and two other previously proposed methods.},
author = {Moore, Robert C and Lewis, William},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Moore, Lewis - Unknown - Intelligent Selection of Language Model Training Data.pdf:pdf},
isbn = {9781617388088},
journal = {Proceedings of ACL},
number = {July},
pages = {220--224},
title = {{Intelligent Selection of Language Model Training Data}},
year = {2010}
}
@book{Guiraud1954a,
author = {Guiraud, Pierre},
pages = {116},
title = {{Les caract{\`{e}}res statistiques du vocabulaire}},
year = {1954}
}
@article{Wandmacher2008,
abstract = {Most current word prediction systems make use of n-gram language models (LM) to estimate the probability of the following word in a phrase. In the past years there have been many attempts to enrich such language models with further syntactic or semantic information. We want to explore the predictive powers of Latent Semantic Analysis (LSA), a method that has been shown to provide reliable information on long-distance semantic dependencies between words in a context. We present and evaluate here several methods that integrate LSA-based information with a standard language model: a semantic cache, partial reranking, and different forms of interpolation. We found that all methods show significant improvements, compared to the 4-gram baseline, and most of them to a simple cache model as well.},
archivePrefix = {arXiv},
arxivId = {0801.4716},
author = {Wandmacher, Tonio and Antoine, Jean-Yves},
eprint = {0801.4716},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wandmacher, Antoine - 2008 - Methods to integrate a language model with semantic information for a word prediction component.pdf:pdf},
journal = {Computational Linguistics},
number = {June},
pages = {10},
title = {{Methods to integrate a language model with semantic information for a word prediction component}},
url = {http://arxiv.org/abs/0801.4716},
year = {2008}
}
@article{Gagolewski2016,
abstract = {Allows for fast, correct, consistent, portable, as well as convenient character string/text processing in every locale and any native encoding. Owing to the use of the ICU library, the package provides R users with platform-independent functions known to Java, Perl, Python, PHP, and Ruby programmers. Among available
features there are: pattern searching (e.g., with ICU Java-like regular expressions or the Unicode Collation Algorithm), random string generation, case mapping, string transliteration, concatenation,
Unicode normalization, date-time formatting and parsing, etc.},
author = {Gagolewski, Marek},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gagolewski - 2016 - Package 'stringi' Character String Processing Facilities.pdf:pdf},
journal = {CRAN},
title = {{Package 'stringi': Character String Processing Facilities}},
year = {2016}
}
@inproceedings{Shahyad2011a,
abstract = {The purpose of the present research was to comparison of content, motivation and frequency of S.M.S Messages sent by boys versus girls. 288 high school students (125 girls and 138 boys) aged 14 to 18 participated in the study that have been chosen via cluster multistage sampling method and completed Type of SMS using Assessment Questionnaire (TSAQ; Shayad, 2010). Data was analyzed by using multivariate analysis of covariance (MANOVA). The results showed that there was a significant difference between boys and girls with regard to motivation, content and frequency of S.M.S messages. Girls send S.M.S more frequently than boys. In fact they send an average of 39 S.M.S a day versus 15 S.M.S sent of the average by a high school boy. Girls seek reassuring information while boys try to sending information for assurance and avoid of face-to-face relationship when they have recourse to S.M.S Boys also send more S.M.S with uncommon content, gibe content and impersonal information. ?? 2011 Published by Elsevier Ltd.},
author = {Shahyad, Shima and Pakdaman, Shahla and Hiedary, Mohamood and Miri, Mirnader and Asadi, Masoud and Nasri, Azad and Alipour, Asghar Shir},
booktitle = {Procedia - Social and Behavioral Sciences},
doi = {10.1016/j.sbspro.2011.03.207},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shahyad et al. - 2011 - A Comparison of motivation, frequency and content of S.M.S. messages sent in boys and girls high school stude(2).pdf:pdf},
issn = {18770428},
keywords = {Adolescent,Content,High school student,Motivation,Number of sent S.M.S},
title = {{A Comparison of motivation, frequency and content of S.M.S. messages sent in boys and girls high school student}},
year = {2011}
}
@article{Trnka2008,
abstract = {We present the development and tuning of a topic-adapted language model for word prediction, which improves keystroke savings over a comparable baseline. We outline our plans to develop and integrate style adaptations, building on our experience in topic modeling to dynamically tune the model to both topically and stylistically relevant texts.},
author = {Trnka, Keith},
doi = {10.3115/1564154.1564167},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Trnka - Unknown - Adaptive Language Modeling for Word Prediction(2).pdf:pdf},
journal = {HLT-SRWS '08 Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies},
number = {June},
pages = {61--66},
title = {{Adaptive language modeling for word prediction}},
url = {http://dl.acm.org/citation.cfm?id=1564167},
year = {2008}
}
@article{Lenhart2012,
abstract = {Texting volume is up while the frequency of voice calling is down. About one in four teens say they own smartphones. The volume of texting among teens has risen from 50 texts a day in 2009 to 60 texts for the median teen text user. Older teens, boys, and blacks are leading the increase. Texting is the dominant daily mode of communication between teens and all those with whom they communicate.},
author = {Lenhart, Amanda},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - full-text(2).pdf:pdf},
journal = {Pew Research Center},
pages = {34},
title = {{Teens , Smartphones {\&} Texting}},
url = {http://pewinternet.org/Reports/2012/Teens-and-smartphones.aspx},
year = {2012}
}
@article{Sundarkantham2011a,
abstract = {Language is a unique phenomenon that distinguishes man from other animals. It is our primary method of communication with each other, yet very little is understood about how language is acquired when we are infants. A greater understanding in this area would have the potential to improve man machine communication The problem that is attempted to be solved in this paper is that of programming a computer to play the Shannon Game. To play the Shannon game, one must predict which words are most likely to follow a given segment of English Text. Word Prediction would be most useful for writers with physical disabilities and severe spelling problems. The aim of this paper is to improve on existing results by writing a program that is capable of automatically inferring a grammar from a Natural Language Corpus, and applying this to the Shannon Game. To play the Shannon Game, a stochastic Grammar for an approximation to the target language must be inferred from a text sample, and as the quality of this grammar improves so too does the quality of the predictor that uses the inferred grammar. The proposed algorithm in the paper uses Support Vector Machine to perform the part of speech tagging which produces 97.6{\%} correct predictions.},
author = {Sundarkantham, K and Shalinie, S Mercy and Pushparathi, S.},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sundarkantham, Shalinie - Unknown - WORD PREDICTOR USING NATURAL LANGUAGE GRAMMAR INDUCTION TECHNIQUE(2).pdf:pdf},
issn = {18234690},
journal = {Journal of Engineering Science and Technology},
keywords = {K-means clustering,Natural language grammatical inference,Support vector machines},
number = {2},
pages = {204--215},
title = {{Word predictor using natural language grammar induction technique}},
url = {www.jatit.org},
volume = {6},
year = {2011}
}
@article{Copestake1997,
abstract = {Current communication devices designed$\backslash$nfor non-speaking users are inadequate to$\backslash$nsupport conversation because the speed$\backslash$nwith which a user can input information is typically very limited. We describe some practical work on word prediction, and discuss its limitations as a technique for speeding up free text entry. We then outline an alternative approach, currently under development, which combines prediction with a constrained technique for natural language generation.},
author = {Copestake, Ann},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Copestake - Unknown - Augmented and alternative NLP techniques for augmentative and alternative communication.pdf:pdf},
journal = {Proceedings of the ACL workshop on Natural Language Processing for Communication Aids},
pages = {37--42},
title = {{Augmented and alternative NLP techniques for augmentative and alternative communication}},
url = {http://anthology.aclweb.org/W/W97/W97-0506.pdf http://acl.ldc.upenn.edu/W/W97/W97-0506.pdf},
year = {1997}
}
@article{Ghayoomia,
abstract = {—The prediction task in national language processing means to guess the missing letter, word, phrase, or sentence that likely follow in a given segment of a text. Since 1980s many systems with different methods were developed for different languages. In this paper an overview of the existing prediction methods that have been used for more than two decades are described and a general classification of the approaches is presented. The three main categories of the classification are statistical modeling, knowledge-based modeling, and heuristic modeling (adaptive).},
author = {Ghayoomi, Masood and Momtazi, Saeedeh},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ghayoomi, Momtazi - Unknown - An Overview on the Existing Language Models for Prediction Systems as Writing Assistant Tools(2).pdf:pdf},
keywords = {Assistant technology,Index Terms—Word prediction,Lan-guage modeling},
title = {{An Overview on the Existing Language Models for Prediction Systems as Writing Assistant Tools}},
url = {http://hpsg.fu-berlin.de/{~}ghayoomi/Publishedpapers/ghayoomi-2009[3].pdf}
}
@misc{Dowle,
author = {Dowle, Matt},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dowle - Unknown - Package 'data.table'.pdf:pdf},
title = {{Package 'data.table'}},
url = {https://cran.r-project.org/web/packages/data.table/data.table.pdf}
}
@article{Biber2004,
abstract = {Multi-dimensional (MD) analysis is a methodological approach that applies multivariate statistical techniques (especially factor analysis and cluster analysis) to the investigation of register variation in a language. The approach was originally developed to analyze the full range of spoken and written registers in a language. Early studies focused on English register variation (Biber 1985, 1986 and 1988), while later studies have applied the same approach to Somali, Korean, Tuvaluan, Taiwanese, and Spanish. Surprisingly, these studies have found some striking similarities in the underlying 'dimensions' that distinguish among spoken and written registers in these diverse languages. It is even more surprising that MD studies of restricted discourse domains have also uncovered dimensions that are similar in linguistic form and function to the more general studies of register variation. The present study presents an MD analysis of a single register: conversation. Three primary dimensions of variation are identified, and then cluster analysis is used to distinguish among six conversation text types. The dimensions and text types are interpreted in linguistic and functional terms. The author's expectations were that a unique set of dimensions would emerge to characterize the variation among conversational texts. Instead, the three dimensions identified here turn out to be closely related to dimensions identified in previous analyses of general register variation. Taken together with previous studies, the present study of conversation raises the possibility of universal dimensions of variation.},
author = {Biber, Douglas},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Biber - 2004 - Conversation text types A multi-dimensional analysis.pdf:pdf},
journal = {JADT},
title = {{Conversation text types: A multi-dimensional analysis}},
year = {2004}
}
@article{Duggan2015,
abstract = {Soil functional stability is the capacity of soil functions to resist and recover from an environmental perturbation and can be used to evaluate soil health. It can be influenced by the presence of xenobiotics such as herbicides. The impact of a fresh 2,4-D contamination (36 mg kg-1 dry soil) on soil functional stability was evaluated by comparing the capacity of soil enzyme activities to resist and recover from a heat perturbation for both a clean and 2,4-D-contaminated soil. The functional stabilities of the soils (uniform sands, pH 6.9, 7{\%} (w/w) organic matter) were calculated using the relative soil stability index (RSSI). The RSSI scores indicate the proportion of potential enzyme activity the soil retains after a perturbation compared to the potential activity of an unperturbed soil. Six extra-cellular enzyme activities (acid and alkaline phosphatases, arylsulfatase, urease, protease and ??-glucosidase) were monitored in soil microcosms during a 15-day period. During this period, a 60 ??C heat perturbation was applied to the soil for 24 h. The activities of arylsulfatase and protease were found to be the most stable following heat perturbation obtaining the highest RSSI scores (87{\%} and 77{\%}, respectively). Urease activity showed the lowest RSSI score (38{\%}). Although all enzyme activities were inhibited by the presence of 2,4-D, the RSSI results indicated that contamination lowered the stability of only three enzyme activities (arylsulfatase, ??-glucosidase and urease). The RSSI adequately described resistance, recovery and recovery rate parameters and enabled differentiation between functional stabilities of clean and contaminated soil and between different soil types. ?? 2006 Elsevier Ltd. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {Accessed October 9, 2008},
author = {B{\'{e}}caert, Val{\'{e}}rie and Samson, R{\'{e}}jean and Desch{\^{e}}nes, Louise},
doi = {10.1016/j.chemosphere.2006.01.008},
eprint = {Accessed October 9, 2008},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/B{\'{e}}caert, Samson, Desch{\^{e}}nes - 2006 - Effect of 2,4-D contamination on soil functional stability evaluated using the relative soil stabili.pdf:pdf},
isbn = {1083610110836},
issn = {00456535},
journal = {Chemosphere},
keywords = {Enzyme activity,Functional recovery,Functional resistance,Heat perturbation,Soil health},
number = {10},
pages = {1713--1721},
pmid = {16476467},
title = {{Effect of 2,4-D contamination on soil functional stability evaluated using the relative soil stability index (RSSI)}},
url = {http://www.pewinternet.org/files/2015/08/Social-Media-Update-2015-FINAL2.pdf},
volume = {64},
year = {2006}
}
@manual{Xie2016a,
annote = {R package version 1.15.1},
author = {Xie, Yihui},
publisher = {https://github.com/yihui/knitr},
title = {{knitr: A General-Purpose Package for Dynamic Report Generation in R}},
url = {https://github.com/yihui/knitr},
year = {2016}
}
@book{Simpson1989,
abstract = {2nd ed. / prepared by J.A. Simpson and E.S.C. Weiner. Provides definitions of approximately 290,500 English words, arranged alphabetically in twenty volumes, with cross-references, etymologies, and pronunciation keys, and includes a bibliography. v. 1. A-Bazouki -- v. 2. B.B.C.-Chalypsography -- v. 3. Cham-Creeky -- v. 4. Creel-Duzepere -- v. 5. Dvandva-Follis -- v. 6. Follow-Haswed -- v. 7. Hat-Intervacuum -- v. 8. Interval-Looie -- v. 9. Look-Mouke -- v. 10. Moul-Ovum -- v. 11. Ow-Poisant -- v. 12. Poise-Quelt -- v. 13. Quemadero-Roaver -- v. 14. Rob-Sequyle -- v. 15. Ser-Soosy -- v. 16. Soot-Styx -- v. 17. Su-Thrivingly -- v. 18. Thro-Unelucidated -- v. 19. Unemancipated-Wau-wau -- v. 20. Wave-Zyxt. Bibliography.},
author = {Simpson, J. A. and Weiner, E. S. C. and {Oxford University Press.}},
edition = {2},
isbn = {0198611862},
publisher = {Clarendon Press},
title = {{The Oxford English Dictionary.}},
year = {1989}
}
@article{Heafield2011,
abstract = {We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and memory costs. The PROBING data structure uses linear probing hash tables and is de signed for speed. Compared with the widely-used SRILM, our PROBING model is 2.4 times as fast while using 57{\%} of the memory. The TRIE data structure is a trie with bit-level packing, sorted records, interpolation search, and optional quantization aimed at lower memory consumption. TRIE simultaneously uses less memory than the smallest lossless baseline and less CPU than the fastest baseline. Our code is open-source, thread-safe, and integrated into the Moses, cdec, and Joshua translation systems. This paper describes the several performance techniques used and presents benchmarks against alternative implementations.},
author = {Heafield, Kenneth},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Heafield - Unknown - KenLM Faster and Smaller Language Model Queries.pdf:pdf},
isbn = {978-1-937284-12-1},
journal = {Proceedings of the Sixth Workshop on Statistical Machine Translation},
number = {2009},
pages = {187--197},
title = {{KenLM : Faster and Smaller Language Model Queries}},
url = {http://www.kheafield.com/professional/avenue/kenlm.pdf http://www.aclweb.org/anthology/W11-2123{\%}5Cnhttp://kheafield.com/code/kenlm},
year = {2011}
}
@misc{WikimediaFoundation,
author = {{Wikimedia Foundation}},
booktitle = {Microsoft},
title = {{Wikipedia Corpus}},
url = {https://support.microsoft.com/en-us/help/14200/windows-compress-uncompress-zip-files},
urldate = {2017-03-17}
}
@misc{Rossenstock,
author = {Rosenstock, Jesse},
title = {{NGramQuickTour {\textless} GRM {\textless} TWiki}},
url = {http://openfst.cs.nyu.edu/twiki/bin/view/GRM/NGramQuickTour{\#}NgramCounting},
urldate = {2017-04-03}
}
@incollection{Mandelbrot61,
address = {New York},
author = {Mandelbrot, B B},
booktitle = {Structures of Language and its Mathematical Aspects},
editor = {Jacobsen, R},
keywords = {bibtex-import},
publisher = {American Mathematical Society},
title = {{On the theory of word frequencies and on related Markovian models of discourse}},
year = {1961}
}
@article{James2000a,
author = {James, Frankie},
file = {:C$\backslash$:/Users/John/Downloads/Modified{\_}Kneser-Ney{\_}Smoothing{\_}of{\_}n-gram{\_}Models.pdf:pdf},
journal = {October},
number = {October},
pages = {0--17},
title = {{Modified Kneser-Ney Smoothing of n-gram Models Modified Kneser-Ney Smoothing of n-gram Models}},
year = {2000}
}
@article{GloriaCorpasPastor2010,
author = {{Gloria Corpas Pastor} and {Miriam Seghiri}},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pastor, Seghiri - Unknown - Size matters A quantitative approach to corpus representativeness(2).pdf:pdf},
pages = {111--145},
title = {{Size matters: a quantative approach to corpus representativeness}},
volume = {1},
year = {2010}
}
@article{Gh2001,
author = {Gh, Derudwrulr and Gh, Hfqrorjtdv and Gh, S W R and Gh, Rolwpfqlfd and Navarro, Santiago Aguilera},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Palazuelos et al. - 2001 - Contribution to word prediction in Spanish and its integration in technical aids for people with physical dis.pdf:pdf},
title = {{Contribution to Word Prediction in Spanish and its Integration in Technial Aids for People with Physical Disabilities}},
url = {http://www.geintra-uah.org/system/files/PhDThesisEnglishSummary-SiraPalazuelos.pdf},
year = {2001}
}
@article{Landauer1997,
abstract = {Bellcore How do people know as much as they do with as little information as they get? The problem takes many forms; learning vocabulary from text is an especially dramatic and convenient case for research. A new general theory of acquired similarity and knowledge representation, latent semantic analysis (LSA), is presented and used to successfully simulate such learning and several other psycholinguistic phenomena. By inducing global knowledge indirectly from local co-occurrence data in a large body of representative text, LSA acquired knowledge about the full vocabulary of English at a comparable rate to schoolchildren. LSA uses no prior linguistic or perceptual similarity knowledge; it is based solely on a general mathematical learning method that achieves powerful inductive effects by ex-tracting the right number of dimensions (e.g., 300) to represent objects and contexts. Relations to other theories, phenomena, and problems are sketched. Prologue "How much do we know at any time? Much more, or so I believe, than we know we know!" —Agatha Christie, The Moving Finger A typical American seventh grader knows the meaning of 10-15 words today that she did not know yesterday. She must have acquired most of them as a result of reading because (a) the majority of English words are used only in print, (b) she already knew well almost all the words she would have encoun-tered in speech, and (c) she learned less than one word by direct instruction. Studies of children reading grade-school text find that about one word in every 20 paragraphs goes from wrong to right on a vocabulary test. The typical seventh grader would have read less than 50 paragraphs since yesterday, from which she should have learned less than three new words. Apparently, she mastered the meanings of many words that she did not encounter. Evidence for all these assertions is given in detail later. This phenomenon offers an ideal case in which to study a problem that has plagued philosophy and science since Plato We thank Karen Lochbaum for valuable help in analysis; George Furnas for early ideas and inspiration; Peter Foltz, Walter Kintsch, and Ernie Mross for unpublished data; and for helpful comments on the ideas and drafts, we thank, in alphabetic order, 24 centuries ago, the fact that people have much more knowl-edge than appears to be present in the information to which they have been exposed. Plato's solution, of course, was that people must come equipped with most of their knowledge and need only hints and contemplation to complete it. In this article we suggest a very different hypothesis to explain the mystery of excessive learning. It rests on the simple notion that some domains of knowledge contain vast numbers of weak interrelations that, if properly exploited, can greatly amplify learning by a process of inference. We have discovered that a very simple mechanism of induction, the choice of the correct dimensionality in which to represent similarity between objects and events, can sometimes, in particular in learning about the similarity of the meanings of words, produce sufficient enhance-ment of knowledge to bridge the gap between the information available in local contiguity and what people know after large amounts of experience. Overview},
author = {Landauer, Thomas K and Dutnais, Susan T and Anderson, Richard and Carroll, Doug and Fbltz, Peter and Pumas, George and Kintsch, Walter and Menn, Lise and Streeter, Lynn},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Landauer et al. - 1997 - A Solution to Plato's Problem The Latent Semantic Analysis Theory of Acquisition, Induction, and Representation.pdf:pdf},
journal = {Psychological Review},
number = {2},
pages = {211--240},
title = {{A Solution to Plato's Problem: The Latent Semantic Analysis Theory of Acquisition, Induction, and Representation of Knowledge}},
url = {http://www.indiana.edu/{~}clcl/Q550{\_}WWW/Papers/Landauer{\_}Dumais{\_}1997.pdf},
volume = {1},
year = {1997}
}
@misc{TheRFoundation2015,
author = {{The R Foundation}},
booktitle = {Mathematical Statistics with Applications in R},
doi = {10.1016/B978-0-12-417113-8.09994-X},
isbn = {0387947256},
issn = {1609-3631},
pages = {745},
pmid = {21196786},
title = {{What is R?}},
url = {https://www.r-project.org/about.html http://www.r-project.org/about.html http://linkinghub.elsevier.com/retrieve/pii/B978012417113809994X},
year = {2015}
}
@article{Begleiter2004,
abstract = {This paper is concerned with algorithms for prediction of discrete sequences over a finite alphabet, using variable order Markov models. The class of such algorithms is large and in principle includes any lossless compression algorithm. We focus on six prominent prediction algorithms, including Context Tree Weighting (CTW), Prediction by Partial Match (PPM) and Probabilistic Suffix Trees (PSTs). We discuss the properties of these algorithms and compare their performance using real life sequences from three domains: proteins, English text and music pieces. The comparison is made with respect to prediction quality as measured by the average log-loss. We also compare classification algorithms based on these predictors with respect to a number of large protein classification tasks. Our results indicate that a “decomposed ” CTW (a variant of the CTW algorithm) and PPM outperform all other algorithms in sequence prediction tasks. Somewhat surprisingly, a different algorithm, which is a modification of the Lempel-Ziv compression algorithm, significantly outperforms all algorithms on the protein classification problems. 1.},
author = {Begleiter, Ron and El-Yaniv, Ran and Yona, Golan},
doi = {10.1613/jair.1491},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Begleiter, El-Yaniv, Yona - 2004 - On Prediction Using Variable Order Markov Models.pdf:pdf},
issn = {10769757},
journal = {Journal of Artificial Intelligence Research},
pages = {385--421},
title = {{On prediction using variable order Markov models}},
volume = {22},
year = {2004}
}
@misc{Kottmann2016,
author = {Kottmann, J{\"{o}}rn and Ingersoll, Grant and Drost, Isabel and Kosin, James and Baldridge, Jason and Morton, Thomas and Silva, William and Agerri, Rodrigo and Autayeu, Aliaksandr and Galitsky, Boris and Giaconia, Mark and Teofili, Tommaso and Khuc, Vinh and Beylerian, Anthony and Bouazizi, Mondher and Mattmann, Chris and Mensikova, Anastasija},
title = {{openNLP: Apache OpenNLP Tools Interface}},
url = {https://opennlp.apache.org/},
year = {2016}
}
@article{Piantadosi2014,
abstract = {The frequency distribution of words has been a key object of study in statistical linguistics for the past 70 years. This distribution approximately follows a simple mathematical form known as Zipf 's law. This paper first shows that human language has highly complex, reliable structure in the frequency distribution over and above this classic law, though prior data visualization methods obscured this fact. A number of empirical phenomena related to word frequencies are then reviewed. These facts are chosen to be informative about the mechanisms giving rise to Zipf's law, and are then used to evaluate many of the theoretical explanations of Zipf's law in language. No prior account straightforwardly explains all the basic facts, nor is supported with independent evaluation of its underlying assumptions. To make progress at understanding why language obeys Zipf's law, studies must seek evidence beyond the law itself, testing assumptions and evaluating novel predictions with new, independent data.},
author = {Piantadosi, Steven T},
doi = {10.3758/s13423-014-0585-6},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Piantadosi - 2015 - Zipf's word frequency law in natural language a critical review and future directions.pdf:pdf},
isbn = {1342301405856},
issn = {1531-5320},
journal = {Psychonomic bulletin $\backslash${\&} review},
keywords = {frequency f,frequent word has a,known as zipf,language,obeying a power law,r,s law,statistics,that scales according to,the r th most,zipf},
month = {oct},
number = {5},
pages = {1112----1130},
pmid = {24664880},
title = {{Zipf's word frequency law in natural language: a critical review and future directions}},
url = {http://link.springer.com/10.3758/s13423-014-0585-6 http://www.ncbi.nlm.nih.gov/pubmed/24664880 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4176592},
volume = {21},
year = {2014}
}
@article{Chelba1998,
abstract = {The paper presents a language model that devel-ops syntactic structure and uses it to extract mean-ingful information from the word history, thus en-abling the use of long distance dependencies. The model assigns probability to every joint sequence of words–binary-parse-structure with headword an-notation and operates in a left-to-right manner — therefore usable for automatic speech recognition. The model, its probabilistic parameterization, and a set of experiments meant to evaluate its predictive power are presented; an improvement over standard trigram modeling is achieved.},
archivePrefix = {arXiv},
arxivId = {arXiv:cs/9811022v2},
author = {Chelba, Ciprian and Jelinek, Frederick},
eprint = {9811022v2},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chelba, Jelinek - 2000 - Exploiting Syntactic Structure for Language Modeling.pdf:pdf},
journal = {Proceedings of the Thirty-Sixth Annual Meeting of the {\{}A{\}}ssociation for {\{}C{\}}omputational {\{}L{\}}inguistics and Seventeenth International Conference on Computational Linguistics},
pages = {225--231},
primaryClass = {arXiv:cs},
title = {{Exploiting Syntactic Structure for Language Modeling}},
url = {https://arxiv.org/pdf/cs/9811022.pdf},
year = {1998}
}
@article{Reid2004a,
abstract = {The increasingly widespread use of text -messaging has led to the questioning of the social and psychological effects of this novel communication medium. A selection of findings from an online questionnaire that was developed by the author to answer this pertinent question are presented. McKenna's recent work on the way the Internet can help some people develop relationships is drawn upon and taken a step further by exploring the differences between those who prefer texting ('Texters') and those who prefer talking on their mobiles ('Talkers'). A large sample of 982 respondents completed the questionnaire. Results showed there was a clear distinction between Texters and Talkers in the way they used their mobiles and their underlying motivations. The key finding to emerge in the preliminary analyses was that Texters seemed to form close knit 'text circles' with their own social ecology, interconnecting with a close group of friends in perpetual text contact. Compared to Talkers, Texters were found to be more lonely and socially anxious, and more likely to disclose their 'real-self' through text than via face-to-face or voice call exchanges. S tructural equation modeling produced a model showing that where respondents located their real-self and whether they were a Texter or a Talker mediated between the loneliness and social anxiety measures and the impact of these on relational outcomes, in line with McKenna's theoretical framework. Thus it appears that there is something special about texting that allows some people to translate their loneliness and/or social anxiety into productive relationships whilst for others the mobile does not afford the same effect. .Applications and explorations for future research are discussed.},
author = {Reid, Donna and Reid, Fraser},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Reid, Reid - 2004 - Insights into the Social and Psychological Effects of SMS Text Messaging.pdf:pdf},
title = {{The social and psychological effects of Text Insights into the Social and Psychological Effects of SMS Text Messaging}},
url = {www.plymouth.ac.uk},
year = {2004}
}
@article{Magnuson2002,
author = {Magnuson, T and Hunnicutt, S},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Magnuson, Hunnicutt - Unknown - Dept. for Speech, Music and Hearing Quarterly Progress and Status Report Measuring the effectiveness of.pdf:pdf},
pages = {057--067},
title = {{Dept. for Speech, Music and Hearing Quarterly Progress and Status Report Measuring the effectiveness of word prediction: The advantage of long-term use}},
url = {http://www.speech.kth.se/qpsr},
year = {2002}
}
@article{Kneser1995,
abstract = {In stochastic language modeling, backing-off is a widely used$\backslash$nmethod to cope with the sparse data problem. In case of unseen events$\backslash$nthis method backs off to a less specific distribution. In this paper we$\backslash$npropose to use distributions which are especially optimized for the task$\backslash$nof backing-off. Two different theoretical derivations lead to$\backslash$ndistributions which are quite different from the probability$\backslash$ndistributions that are usually used for backing-off. Experiments show an$\backslash$nimprovement of about 10{\%} in terms of perplexity and 5{\%} in terms of word$\backslash$nerror rate},
author = {Kneser, R. and Ney, H.},
doi = {10.1109/ICASSP.1995.479394},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kneser - Unknown - IMPROVED BACKING-OFF FOR M-GRAM LANGUAGE MODELING.pdf:pdf},
isbn = {0-7803-2431-5},
issn = {1520-6149},
journal = {1995 International Conference on Acoustics, Speech, and Signal Processing},
pages = {181--184},
title = {{Improved backing-off for M-gram language modeling}},
volume = {1},
year = {1995}
}
@article{Hasan2012,
abstract = {-Smoothing techniques is the utmost possibility estimate of probabilities to produce more precise probabilities. Smoothing is one of the most significant techniques while constructing a language model with a limited number of training data. In this paper, our main aim to analyze the performance of different smoothing techniques on n-grams. For language modeling, we considered two most widely-used smoothing algorithms: Witten-Bell smoothing and Kneser-Ney smoothing. For the evaluation we use BLEU (Bilingual Evaluation Understudy) and NIST (National Institute of Standards and Technology) scoring techniques. We have done the evaluation of these models is performed by comparing the automatically produced word alignment. We use Moses Statistical Machine Translation System for our work. Our machine translation approach has been tested on German to English and English to German task. The obtain results are considerably better than those obtained with alternative approaches to machine translation. This paper addresses several aspects of Statistical Machine Translation (SMT).},
author = {Hasan, A S M Mahmudul and Islam, Saria and Rahman, M Arifur},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - 2012 - A Comparative Study of Witten Bell and Kneser-Ney Smoothing Methods for Statistical Machine Translation.pdf:pdf},
journal = {JU Journal of Information Technology},
number = {June},
title = {{A Comparative Study of Witten Bell and Kneser-Ney Smoothing Methods for Statistical Machine Translation}},
url = {http://www.juniv.edu/jujit/files/2012/09/1{\_}new.pdf},
volume = {1},
year = {2012}
}
@article{Tan2012,
abstract = {This paper presents an attempt at building a large scale distributed composite language model that is formed by seamlessly integrating an n-gram model, a structured language model, and probabilistic latent semantic analysis under a directed Markov random field paradigm to simultaneously account for local word lexical information, mid-range sentence syntactic structure, and long-span document semantic content. The composite language model has been trained by performing a convergent N-best list approximate EM algorithm and a follow-up EM algorithm to improve word prediction power on corpora with up to a billion tokens and stored on a supercomputer. The large scale distributed composite language model gives drastic perplexity reduction over n-grams and achieves significantly better translation quality measured by the Bleu score and "readability" of translations when applied to the task of re-ranking the N-best list from a state-of-the-art parsing-based machine translation system.},
author = {Tan, Ming and Zhou, Wenli and Zheng, Lei and Wang, Shaojun},
doi = {10.1162/COLI_a_00107},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tan et al. - Unknown - A Scalable Distributed Syntactic, Semantic, and Lexical Language Model.pdf:pdf},
isbn = {1530-9312},
issn = {0891-2017},
journal = {Computational Linguistics},
number = {3},
pages = {631--671},
title = {{A Scalable Distributed Syntactic, Semantic, and Lexical Language Model}},
volume = {38},
year = {2012}
}
@book{Guiraud1954,
address = {Paris},
author = {Guiraud, Pierre},
publisher = {Presses Universitaires de France},
title = {{Les caracteres statistique du vocabulaire}},
year = {1954}
}
@misc{MacFarlane2006,
author = {MacFarlane, John},
title = {{About Pandoc}},
url = {http://pandoc.org/ http://johnmacfarlane.net/pandoc/},
year = {2006}
}
@article{Zurhellen,
abstract = {This article is one of a series of short essays, collectively titled " Further Explorations, " published as part of a special issue of Oral Tradition in honor of John Miles Foley's 65 th birthday and 2011 retirement. The surprise Festschrift, guest-edited by Lori and Scott Garner entirely without his knowledge, celebrates John's tremendous impact on studies in oral tradition through a series of essays contributed by his students from the University of Missouri-Columbia (1979-present) and from NEH Summer Seminars that he has directed (1987-1996). http://journal.oraltradition.org/issues/26ii In The Pathways Project, John Miles Foley (2011-) discusses briefly the social role of SMS (Short Message Service), suggesting that " even so-called text messaging, a misnomer of sizeable proportions given that the activity really amounts to a long-distance emergent communication enacted virtually, knits people together into interactive groups and keeps them connected and 'present' to one another. " 1 In this essay, I propose a merger of current research on text messaging and the study of oral traditions in order to shed light on the relationship between this new mode of communication and the workings of consciousness being transformed by the eAgora. Focusing first on the limitations of text messaging as a medium that unexpectedly encouraged language innovation, we can explore how text messaging language merges effective communicative practices from both oral and written technologies in order to generate more efficient communication within a newly-limited, writing-based technology. Moreover, in addition to its efficiency, the kind of linguistic play found in text messaging can be viewed as a source of pleasure for those who engage in texting (" texters "). Thus, by employing the discourse of orality and literacy, we can explain how text messaging, while impossible to imagine without the myriad writing technologies mastered before it, actually encourages its literacy-obsessed users to practice communicative techniques more often found within oral cultures, or more precisely, communicative techniques found in cultures in the incipient stages of literacy. Such cultures are ripe for language innovation precisely because they have begun to record knowledge but have not yet standardized the recording procedure. Coincident with a perspective that sees text messaging as bridging a consciousness gap between oral and literate cultures, then, is the recognition that close study of the ways in which text messaging reworks language could lead to fruitful discoveries about the most current ways in which Computer-Mediated Communication (CMC) directs human life toward ever-emerging horizons of consciousness. When David Crystal (2008) hyperbolized the emergence of text messaging in the following passage, this form of communication was already a well-developed medium. Nevertheless, his humorous figuring of text messaging's inception, while not quite accurate, highlights precisely the form's limits that made it such an unlikely competitor in the tightly-wound market of twenty-first-century technologies (173-74): Oral Tradition, 26/2 (2011): 637-624},
annote = {From Duplicate 2 (" A Misnomer of Sizeable Proportions " : SMS and Oral Tradition - Zurhellen, Sarah)

Not primary research},
author = {Zurhellen, Sarah},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zurhellen - 2011 - SMS and Oral Tradition.pdf:pdf},
title = {{" A Misnomer of Sizeable Proportions " : SMS and Oral Tradition}}
}
@article{Garay-Vitoria2004,
abstract = {Prediction is one of the most extended techniques to enhance the rate of communication for people with motor and speech impairments who use Augmentative and Alternative Communication systems. There is an enormous diversity of prediction methods and techniques mentioned in the literature. Therefore, the designer finds tremendous difficulties in understanding and comparing them in order to decide the most convenient technique for a specific design. This paper presents a survey on prediction techniques applied to communicators with the intention of helping them to understand this field. Prediction applications and related features, such as block size, dictionary structure, prediction method, interface, special features, measurement and results, are detailed. Systems found in the literature are studied and described. Finally, a discussion is carried out on the possible comparison among the different methods. {\textcopyright} Springer-Verlag 2004.},
author = {Garay-Vitoria, N. and Abascal, J.},
doi = {10.1007/978-3-540-30111-0_35},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Garay-Vitoria, Abascal - 2004 - A comparison of prediction techniques to enhance the communication rate(2).pdf:pdf},
isbn = {978-3-540-23375-6},
issn = {03029743 16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {October},
pages = {400--417},
title = {{A comparison of prediction techniques to enhance the communication rate}},
volume = {3196},
year = {2004}
}
@book{Yule1944a,
author = {Yule, George Udny},
booktitle = {Cambridge University Press},
pages = {306},
publisher = {Cambridge University Press},
title = {{The Statistical Study of Literary Vocabulary}},
year = {1944}
}
@article{VandenBosch2008,
abstract = {Word completion is a basic technology for reducing the effort involved in text entry on mobile devices and in augmentative communication devices, where efficiency and ease of use are needed, but where a low memory footprint is also required. Standard solutions compress a lexicon into a suffix tree with a small memory footprint and high retrieval speed. Keystroke savings, a measurable correlate of text entry effort gain, typically improve when the algorithm would also take into account the previous word; however, this comes at the cost of a large footprint. We develop two word completion algorithms that encode the previous word in the input. The first algorithm utilizes a character buffer that includes a fixed number of recent keystrokes, including those belonging to previous words. The second algorithm includes the complete previous word as an extra input feature. In simulation studies, the first algorithm yields marked improvements in keystroke savings, but has a large memory footprint. The second algorithm can be tuned by frequency thresholding to have a small footprint, and be less than one order of magnitude slower than the baseline system, while its keystroke savings improve over the baseline.},
author = {van den Bosch, Antal and Bogers, Toine},
doi = {10.1145/1409240.1409315},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/van den Bosch, Bogers - 2008 - Efficient context-sensitive word completion for mobile devices(2).pdf:pdf},
isbn = {9781595939524},
journal = {Proceedings of the 10th international conference on Human computer interaction with mobile devices and services - MobileHCI '08},
keywords = {con-,ergonomics,mobile devices,predictive text processing,scaling,text sensitivity,word completion},
pages = {465},
title = {{Efficient context-sensitive word completion for mobile devices}},
url = {http://portal.acm.org/citation.cfm?doid=1409240.1409315},
year = {2008}
}
@article{Kurt2016,
author = {Kurt, Author and Karatzoglou, Alexandros and Meyer, David},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - 2016 - Package 'RWeka' RWeka Interface.pdf:pdf},
title = {{Package ‘ RWeka '}},
year = {2016}
}
@article{StefanEvert2015,
abstract = {Statistical models and utilities for the analysis of word frequency distributions. The utilities include functions for loading, manipulating and visualizing word frequency data and vocabulary growth curves. The package also implements several statistical models for the distribution of word frequencies in a population. (The name of this library derives from the most famous word frequency distribution, Zipf's law.)},
author = {Evert, Stefan},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Stefan Evert, Baroni, Stefan Evert - 2015 - Package 'zipfR' Title Statistical models for word frequency distributions.pdf:pdf},
journal = {R Package},
pages = {94},
title = {{Package "zipfR"}},
year = {2015}
}
@article{Garay-Vitoria2004a,
abstract = {Prediction is one of the most extended techniques to enhance the rate of communication for people with motor and speech impairments who use Augmentative and Alternative Communication systems. There is an enormous diversity of prediction methods and techniques mentioned in the literature. Therefore, the designer finds tremendous difficulties in understanding and comparing them in order to decide the most convenient technique for a specific design. This paper presents a survey on prediction techniques applied to communicators with the intention of helping them to understand this field. Prediction applications and related features, such as block size, dictionary structure, prediction method, interface, special features, measurement and results, are detailed. Systems found in the literature are studied and described. Finally, a discussion is carried out on the possible comparison among the different methods. {\textcopyright} Springer-Verlag 2004.},
author = {Garay-Vitoria, Nestor and Abascal, Julio},
doi = {10.1007/978-3-540-30111-0_35},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Garay-Vitoria, Abascal - 2004 - A comparison of prediction techniques to enhance the communication rate(2).pdf:pdf},
isbn = {978-3-540-23375-6},
issn = {03029743 16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {400--417},
title = {{A comparison of prediction techniques to enhance the communication rate}},
volume = {3196},
year = {2004}
}
@article{Baber2012,
abstract = {This communication study explores the relation between commitment, relational maintenance strategies, and text message use in romantic partners. This study examines how these three factors are connected through the use of surveys. It was hypothesized that romantic partners who were more committed to one another would use text messages to communicate about certain relational maintenance strategies. Results showed romantic partners who used more relational maintenance strategies did in fact use text messages to communicate about these issues more often. Also, couples who were more committed to their partner did use more positivity when communicating through text messages with their partner. It was also found that males use the relational maintenance strategy of openness more often than females when communicating through text messages.},
author = {Baber, Vashaun M and Shen, Sam and Tew, Michael and Stacey, Kathleen and Ypsilanti, Michigan},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Baber et al. - 2012 - Relational maintenance An examination of how gender, relational maintenance strategies, and commitment affect the.pdf:pdf},
title = {{Relational maintenance: An examination of how gender, relational maintenance strategies, and commitment affect the use of text messages in romantic relationships}},
year = {2012}
}
@article{Rastrow2012,
abstract = {Automatic Speech Recognition (ASR) and Statistical Machine Translation (SMT), among other natural language processing applications, rely on a language model (LM) to provide a strong linguistic prior over word sequences of the often prohibitively large and complex hypothesis space of these systems. The language models deployed in most state-of-the-art ASR and SMT systems are n-gram models. Several statistical frameworks have been proposed to build more complex models and " put (the syntactic structure of) language back into language modeling. " Yet, n-gram models, despite being linguistically nave, are still favored, because estimating them from text is well understood, they are computationally efficient, and integrating them into ASR and SMT systems is straightforward. This dissertation proposes novel algorithms and techniques that make it practical to estimate and apply more complex language models in ASR and SMT tasks, in particular syntactic modes for speech recognition. While yielding significantly better performance than n-gram models, the syntactic structured language models (SLM) can not be efficiently trained on a large amount of text data due to the impractical size of the resulting model. A general information-ii ABSTRACT theoretic pruning scheme is proposed to significantly reduce the size of the SLM while},
author = {Rastrow, Ariya},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rastrow - 2012 - Practical and Efficient Incorporation of Syntactic Features into Statistical Language Models.pdf:pdf},
title = {{Practical and Efficient Incorporation of Syntactic Features into Statistical Language Models}},
url = {https://www.cs.jhu.edu/{~}mdredze/publications/ariya{\_}rastrow{\_}thesis.pdf},
volume = {Ph.D.},
year = {2012}
}
@article{Zurhellen,
abstract = {This article is one of a series of short essays, collectively titled " Further Explorations, " published as part of a special issue of Oral Tradition in honor of John Miles Foley's 65 th birthday and 2011 retirement. The surprise Festschrift, guest-edited by Lori and Scott Garner entirely without his knowledge, celebrates John's tremendous impact on studies in oral tradition through a series of essays contributed by his students from the University of Missouri-Columbia (1979-present) and from NEH Summer Seminars that he has directed (1987-1996). http://journal.oraltradition.org/issues/26ii In The Pathways Project, John Miles Foley (2011-) discusses briefly the social role of SMS (Short Message Service), suggesting that " even so-called text messaging, a misnomer of sizeable proportions given that the activity really amounts to a long-distance emergent communication enacted virtually, knits people together into interactive groups and keeps them connected and 'present' to one another. " 1 In this essay, I propose a merger of current research on text messaging and the study of oral traditions in order to shed light on the relationship between this new mode of communication and the workings of consciousness being transformed by the eAgora. Focusing first on the limitations of text messaging as a medium that unexpectedly encouraged language innovation, we can explore how text messaging language merges effective communicative practices from both oral and written technologies in order to generate more efficient communication within a newly-limited, writing-based technology. Moreover, in addition to its efficiency, the kind of linguistic play found in text messaging can be viewed as a source of pleasure for those who engage in texting (" texters "). Thus, by employing the discourse of orality and literacy, we can explain how text messaging, while impossible to imagine without the myriad writing technologies mastered before it, actually encourages its literacy-obsessed users to practice communicative techniques more often found within oral cultures, or more precisely, communicative techniques found in cultures in the incipient stages of literacy. Such cultures are ripe for language innovation precisely because they have begun to record knowledge but have not yet standardized the recording procedure. Coincident with a perspective that sees text messaging as bridging a consciousness gap between oral and literate cultures, then, is the recognition that close study of the ways in which text messaging reworks language could lead to fruitful discoveries about the most current ways in which Computer-Mediated Communication (CMC) directs human life toward ever-emerging horizons of consciousness. When David Crystal (2008) hyperbolized the emergence of text messaging in the following passage, this form of communication was already a well-developed medium. Nevertheless, his humorous figuring of text messaging's inception, while not quite accurate, highlights precisely the form's limits that made it such an unlikely competitor in the tightly-wound market of twenty-first-century technologies (173-74): Oral Tradition, 26/2 (2011): 637-624},
annote = {Not primary research},
author = {Zurhellen, Sarah},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zurhellen - 2011 - SMS and Oral Tradition.pdf:pdf},
title = {{" A Misnomer of Sizeable Proportions " : SMS and Oral Tradition}}
}
@book{Herdan1960,
author = {Herdan, Gustav},
pages = {448},
publisher = {Mouton},
title = {{Type-token Mathematics}},
year = {1960}
}
@article{Sichel1986,
author = {Sichel, H},
journal = {Mathematical Scientist},
pages = {45--72},
title = {{Word frequency distributions and type-token characteristics}},
volume = {11},
year = {1986}
}
@article{Bilmes2003,
abstract = {We introduce factored language models (FLMs) and generalized parallel backoff (GPB). An FLM represents words as bundles of features (e.g., morphological classes, stems, data-driven clusters, etc.), and induces a probability model covering sequences of bundles rather than just words. GPB extends standard backoff to general conditional probability tables where variables might be heterogeneous types, where no obvious natural (temporal) backoff order exists, and where multiple dynamic backoff strategies are allowed. These methodologies were implemented during the JHU 2002 workshop as extensions to the SRI language modeling toolkit. This paper provides initial perplexity results on both CallHome Arabic and on Penn Treebank Wall Street Journal articles. Significantly, FLMs with GPB can produce bigrams with significantly lower perplexity, sometimes lower than highly-optimized baseline trigrams. In a multi-pass speech recognition context, where bigrams are used to create first-pass bigram lattices or N-best lists, these results are highly relevant.},
author = {Bilmes, Jeff A and Kirchhoff, Katrin},
doi = {10.3115/1073483.1073485},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bilmes, Kirchhoff - 2003 - Factored Language Models and Generalized Parallel Backoff.pdf:pdf},
journal = {Naacl-2003},
pages = {4--6},
title = {{Factored Language Models and Generalized Parallel Backoff}},
url = {http://www.aclweb.org/anthology/N03-2002 http://portal.acm.org/citation.cfm?doid=1073483.1073485{\%}5Cnhttp://acl.ldc.upenn.edu/N/N03/N03-2002.pdf},
volume = {2},
year = {2003}
}
@article{Good1953,
abstract = {A random sample is drawn from a population of animals of various species. (The theory may also be applied to studies of literary vocabulary, for example.) If a particular species is represented r times in the sample of size N, then r/N is not a good estimate of the population frequency, p, when r is small. Methods are given for estimating p, assuming virtually nothing about the underlying population. The estimates are expressed in terms of smoothed values of the numbers nr (r= 1, 2, 3, ...), where nr is the number of distinct species that are each represented r times in the sample. (nr may be described as ‘the frequency of the frequency r'.) Turing is acknowledged for the most interesting formula in this part of the work. An estimate of the proportion of the population represented by the species occurring in the sample is an immediate corollary. Estimates are made of measures of heterogeneity of the population, including Yule's ‘characteristic' and Shannon's ‘entropy'. Methods are then discussed that do depend on assumptions about the underlying population. It is here that most work has been done by other writers. It is pointed out that a hypothesis can give a good fit to the numbers nr but can give quite the wrong value for Yule's characteristic. An example of this is Fisher's fit to some data of Williams's on Macrolepidoptera.},
author = {Good, I.J.},
doi = {10.1093/biomet/40.3-4.237},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Good - 1953 - The Population Frequencies of Species and the Estimation of Population Parameters.pdf:pdf},
issn = {0006-3444},
journal = {Biometrika},
number = {3-4},
pages = {237--264},
title = {{The population frequencies of spiecies and the estimation of population parameters}},
url = {http://links.jstor.org/sici?sici=0006-3444{\%}28195312{\%}2940{\%}3A3{\%}2F4{\%}3C237{\%}3ATPFOSA{\%}3E2.0.CO{\%}3B2-K https://academic.oup.com/biomet/article-lookup/doi/10.1093/biomet/40.3-4.237 http://biomet.oxfordjournals.org/content/40/3-4/237.abstract},
volume = {40},
year = {1953}
}
@misc{Wickham2016,
author = {Wickham, Hadley},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - R Package 'ggplot2' Create Elegant Data Visualisations Using the Grammar of Graphics.pdf:pdf},
title = {{R Package 'ggplot2' | Create Elegant Data Visualisations Using the Grammar of Graphics}},
year = {2016}
}
@book{Barton1994,
author = {Barton, David},
pages = {96},
publisher = {Blackwell Publishing},
title = {{Literacy: An Introduction to the Ecology of Written Language}},
year = {1994}
}
@article{Piantadosi2014,
abstract = {The frequency distribution of words has been a key object of study in statistical linguistics for the past 70 years. This distribution approximately follows a simple mathematical form known as Zipf 's law. This paper first shows that human language has highly complex, reliable structure in the frequency distribution over and above this classic law, though prior data visualization methods obscured this fact. A number of empirical phenomena related to word frequencies are then reviewed. These facts are chosen to be informative about the mechanisms giving rise to Zipf's law, and are then used to evaluate many of the theoretical explanations of Zipf's law in language. No prior account straightforwardly explains all the basic facts, nor is supported with independent evaluation of its underlying assumptions. To make progress at understanding why language obeys Zipf's law, studies must seek evidence beyond the law itself, testing assumptions and evaluating novel predictions with new, independent data.},
author = {Piantadosi, Steven T},
doi = {10.3758/s13423-014-0585-6},
isbn = {1342301405856},
issn = {1531-5320},
journal = {Psychonomic bulletin $\backslash${\&} review},
keywords = {frequency f,frequent word has a,known as zipf,language,obeying a power law,r,s law,statistics,that scales according to,the r th most,zipf},
month = {oct},
number = {5},
pages = {1112----1130},
pmid = {24664880},
title = {{Zipf's word frequency law in natural language: a critical review and future directions}},
url = {http://link.springer.com/10.3758/s13423-014-0585-6 http://www.ncbi.nlm.nih.gov/pubmed/24664880 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4176592},
volume = {21},
year = {2014}
}
@misc{Kennon2015,
author = {Kennon, Joshua},
title = {{Blog Demographics 2015 Edition: If Life Were a Game, You All Would Be Champions}},
url = {http://www.joshuakennon.com/blog-demographics-2015-edition-if-life-were-a-game-you-would-be-champions/},
urldate = {2016-12-13},
year = {2015}
}
@article{Evert2005,
abstract = {You shall know a word by the company it keeps! With this slogan, Firth (1957) drew attention to a fact that language scholars had intuitively known for a long time: In natural language, words are not combined randomly into phrases and sentences, con- strained only by the rules of syntax. The particular ways in which they go together are a rich and important source of information both about language and about the world we live in. In the 1930s, J. R. Firth coined the term collocations for such char- acteristic, or habitual word combinations (as he called them). While Firth used to be lamentably vague about his precise understanding of this concept (cf. Lehr 1996, 21), the term itself and the general idea behind it that collocations correspond to some conventional way of saying things (Manning and Sch{\"{u}}tze 1999, 151) were eagerly taken up by researchers in various fields, leading to the serious terminolog- ical confusion that surrounds the concept of collocations today. As Choueka puts it: even though any two lexicographers would agree that once upon a time, hit the road and similar idioms are collocations, they would most certainly disagree on al- most anything else (Choueka 1988, 4). Feel free to replace lexicographers with any profession that is concerned with language data.},
author = {Evert, Stefan},
doi = {10.1073/pnas.141413598},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Evert - 2005 - The Statistics of Word Cooccurrences Word Pairs and Collocations.pdf:pdf},
isbn = {0027-8424 (Print)},
issn = {00278424},
journal = {Unpublished doctoral dissertation Institut fur maschinelle Sprachverarbeitung Universitat Stuttgart},
number = {August 2004},
pages = {353},
pmid = {11447261},
title = {{The Statistics of Word Cooccurrences Word Pairs and Collocations}},
url = {http://en.scientificcommons.org/19948039},
volume = {98},
year = {2005}
}
@article{Matiasek2002a,
abstract = {Communication and information exchange is a vital factor in human society. Communication disorders severely influence the quality of life. Whereas experienced typists will produce some 300 keystrokes per minute, persons with motor impairments achieve only much lower rates. Predictive typing systems for English speaking areas have proven useful and efficient, but for all other European languages there exist no predic-tive typing programs powerful enough to substantially improve the com-munication rate and the IT access for disabled persons. FASTY aims at offering a communication support system significantly increasing typing speed, adaptable to users with different language and strongly varying needs. In this way the large group of non-English-speaking disabled citi-zens will be supported in living a more independent and self determined life.},
author = {Matiasek, Johannes and Baroni, Marco and Trost, Harald},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Matiasek, Baroni, Trost - 2002 - FASTY—A multi-lingual approach to text prediction.pdf:pdf},
isbn = {3-540-43904-8},
journal = {Computers Helping People with Special {\ldots}},
number = {c},
pages = {243--250},
title = {{FASTY—A multi-lingual approach to text prediction}},
url = {http://link.springer.com/chapter/10.1007/3-540-45491-8{\_}51},
volume = {2398},
year = {2002}
}
@misc{Hornik2016,
abstract = {Description An interface to the Apache OpenNLP tools (version 1.5.3). The Apache OpenNLP library is a machine learning based toolkit for the processing of natural language text written in Java. It supports the most common NLP tasks, such as tokenization, sentence segmentation, part-of-speech tagging, named entity extraction, chunking, parsing, and coreference resolution. See {\textless}http://opennlp.apache.org/{\textgreater} for more information. Imports NLP ({\textgreater}= 0.1-6.3), openNLPdata ({\textgreater}= 1.5.3-1), rJava ({\textgreater}= 0.6-3)},
author = {Hornik, Kurt},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hornik, Maintainer - 2016 - Title Apache OpenNLP Tools Interface.pdf:pdf},
title = {{Apache OpenNLP Tools Interface}},
year = {2016}
}
@book{Bauer1983,
abstract = {Interest in word-formation is probably as old as interest in language itself. As Dr. Bauer points out in his Introduction, many of the questions that scholars are asking now were also being asked in the seventeenth, eighteenth and nineteenth centuries. However, there is still little agreement on methodology in the study of word-formation or theoretical approaches to it; even the kind of data relevant to its study is open to debate. Dr. Bauer here provides students and general linguists alike with a new perspective on what is a confused and often controversial field of study, providing a resolution to the terminological confusion which currently reigns in this area. In doing so, he clearly demonstrates the challenge and intrinsic fascination of the study of word-formation. Linguists have recently become increasingly aware of the relevance of word-formation to work in syntax and semantics, phonology and morphology, and Dr Bauer discusses - within a largely synchronic and transformational framework - the theoretical issues involved. He considers topics where word-formation has a contribution to make to other areas of linguistics and, without pretending to provide a fully-fledged theory of word-formation, develops those points which he sees as being central to its study. -- Publisher description. 1. Introduction --- 2. Some basic concepts --- 3. Lexicalization --- 4. Productivity --- 5. Phonological issues in word-formation --- 6. Syntactic and semantic issues in word-formation --- 7. An outline of English word-formation --- 8. Theory and practice --- 9. Conclusion.},
address = {Cambridge},
author = {Bauer, Laurie},
isbn = {0521284929},
pages = {311},
publisher = {Cambridge University Press},
title = {{English word-formation}},
year = {1983}
}
@article{Lidstone1920,
author = {Lidstone, George James},
journal = {Transactions of the Faculty of Actuaries},
keywords = {mathematics},
pages = {182--192},
title = {{Note on the general case of the bayes-laplace formula for inductive or a posteriori probabilities}},
volume = {8},
year = {1920}
}
@inproceedings{Paperno2016a,
abstract = {We introduce LAMBADA, a dataset to evaluate the capabilities of computational models for text understanding by means of a word prediction task. LAMBADA is a collection of narrative passages shar- ing the characteristic that human subjects are able to guess their last word if they are exposed to the whole passage, but not if they only see the last sentence preced- ing the target word. To succeed on LAM- BADA, computational models cannot sim- ply rely on local context, but must be able to keep track of information in the broader discourse. We show that LAMBADA ex- emplifies a wide range of linguistic phe- nomena, and that none of several state-of- the-art language models reaches accuracy above 1{\%} on this novel benchmark. We thus propose LAMBADA as a challenging test set, meant to encourage the develop- ment of new models capable of genuine understanding of broad context in natural language text.},
archivePrefix = {arXiv},
arxivId = {1606.06031},
author = {Paperno, Denis and Lazaridou, Angeliki and Pham, Quan Ngoc and Bernardi, Raffaella and Pezzelle, Sandro and Baroni, Marco and Boleda, Gemma and Fern, Raquel},
booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, Berlin, Germany, August 7-12, 2016},
eprint = {1606.06031},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Paperno et al. - 2016 - The LAMBADA dataset Word prediction requiring a broad discourse context.pdf:pdf},
keywords = {Distributional semantics},
pages = {1525--1534},
title = {{The LAMBADA dataset : Word prediction requiring a broad discourse context}},
year = {2016}
}
@article{Kelly2012,
author = {Kelly, Lynne and Keaten, James A. and Becker, Bonnie and Cole, Jodi and Littleford, Lea and Rothe, Barrett},
doi = {10.1080/17459435.2012.719203},
issn = {1745-9435},
journal = {Qualitative Research Reports in Communication},
month = {jan},
number = {1},
pages = {1--9},
title = {{“It's the American Lifestyle!”: An Investigation of Text Messaging by College Students}},
url = {http://www.tandfonline.com/doi/abs/10.1080/17459435.2012.719203},
volume = {13},
year = {2012}
}
@article{Hsu2008,
abstract = {Despite the availability of better performing techniques, most language models are trained using popular toolkits that do not support perplexity optimization. In this work, we present an efficient data structure and optimized algorithms specifically designed for iterative parameter tuning. With the resulting implementation, we demonstrate the feasibility and effectiveness of such iterative techniques in language model estimation. Index Terms: language modeling, smoothing, interpolation 1.},
author = {Hsu, Bo-June and Glass, James},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hsu, Glass - 2008 - Iterative language model estimation efficient data structure {\&} algorithms.pdf:pdf},
issn = {19909772},
journal = {Proceedings of Interspeech},
keywords = {Index Terms,interpolation,language modeling,smoothing},
pages = {1--4},
title = {{Iterative language model estimation: efficient data structure {\&} algorithms}},
url = {http://people.csail.mit.edu/jrg/2008/paul-interspeech08.pdf},
volume = {8},
year = {2008}
}
@article{Garay-Vitoria1997,
author = {Garay-Vitoria, Nestor and Gonz{\'{a}}lez-Abascal, Julio},
doi = {10.1145/238218.238333},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Garay-Vitoria, Gonz{\'{a}}lez-Abascal - 1997 - Intelligent word-prediction to enhance text input rate (a syntactic analysis-based word-predict.pdf:pdf},
isbn = {0897918398},
journal = {Proceedings of the 2nd international conference on Intelligent user interfaces  - IUI '97},
number = {December},
pages = {241--244},
title = {{Intelligent word-prediction to enhance text input rate (a syntactic analysis-based word-prediction aid for people with severe motor and speech disability)}},
url = {http://portal.acm.org/citation.cfm?doid=238218.238333},
year = {1997}
}
@article{Thurlow2011,
abstract = {On the day we first started putting this chapter together, soon-to-be-President-of-the-United-States Barack Obama announced his choice of vice-presidential running mate by sending a text message to journalists and Democratic Party senators and supporters. One not-so-restrained New York Times journalist characterized the event: 'Mr. Obama's use of the newfound medium is the widest use of texting by a presidential candidate in history.' The following morning, again in the USA, a National Public Radio journalist talked about 'the most highly anticipated text message in human history'. This already newsworthy event was evidently being given an added mediatized spin thanks to texting. No doubt like many readers of the current volume, we are not convinced of the historic proportions of the Obama campaign's text message. This was, however, certainly a communicative event loaded with pragmatic – and metapragmatic – force. Why choose to use texting to deliver this public message? (After all, supporters could just as easily have been notified by the ancient technology of email.) What did the choice of text message mean to voters? Why should it warrant such media interest? Why make so much fuss about a text message which bore so little resemblance to the millions of text messages sent every day by ordinary people around the world? In this chapter, we consider these matters by stepping back from the hyperbolic commentary on text messaging by journalists and the entertaining observations of popular writers. To this end, we start with a comprehensive but potted review of the scholarly, research-driven literature on text messaging; this work highlights the range of applications to which texting has been put as well as the ways in which sociolinguists, discourse analysts and other communication scholars have been attending to language in texting messaging. Shifting next to a more specifically pragmatic and metapragmatic focus, we present some of our own empirical research as a way to illustrate general phenomena covered in the wider scholarly literature and to ground text messaging as a pragmatic phenomenon. We close our chapter with some brief thoughts about gaps in the academic literature and possible directions for future research on the language of text messaging. Before we go any further, however, we offer the following brief account of text messaging as a digital technology.},
author = {Thurlow, Crispin and Poff, Michele},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Thurlow, Poff - 2011 - Text Messaging.pdf:pdf},
publisher = {Mouton de Gruyter},
title = {{Text Messaging}},
year = {2011}
}
@article{Luyckx,
abstract = {Current advances in shallow parsing and machine learning allow us to use results from these fields in a methodology for Authorship Attribution. We report on experiments with a corpus that consists of newspaper articles about national current affairs by different journalists from the Belgian newspaper De Standaard. Because the documents are in a similar genre, register, and range of topics, token-based (e.g., sentence length) and lexical features (e.g., vocabulary richness) can be kept roughly constant over the different authors. This allows us to focus on the use of syntax-based features as possible predictors for an author's style, as well as on those token-based features that are predictive to author style more than to topic or register. These style characteristics are not under the author's conscious control and therefore good clues for Authorship Attribution. Machine Learning methods (TiMBL and the WEKA software package) are used to select informative combinations of syntactic, token-based and lexical features and to predict authorship of unseen documents. The combination of these features can be considered an implicit profile that characterizes the style of an author.},
author = {Luyckx, Kim and Daelemans, Walter},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Luyckx, Daelemans - Unknown - Shallow Text Analysis and Machine Learning for Authorship Attri- bution.pdf:pdf},
title = {{Shallow Text Analysis and Machine Learning for Authorship Attri- bution}}
}
@article{Tagg2009a,
author = {Tagg, Caroline},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tagg - 2009 - by.pdf:pdf},
title = {{A CORPUS LINGUISTICS STUDY OF SMS TEXT MESSAGING}},
year = {2009}
}
@article{Kasesniemi2002,
author = {Kasesniemi, Eija-Liisa and Rautianen, Pirjo},
title = {{Mobile culture of children and teenagers in Finland}},
year = {2002}
}
@article{Faulkner2005a,
abstract = {SMS or text messaging is an area of growth in the communications field. The studies described below consisted of a questionnaire and a diary study. The questionnaire was designed to examine texting activities in 565 users of the mobile phone. The diary study was carried out by 24 subjects over a period of 2 weeks. The findings suggest that text messaging is being used by a wide range of people for all kinds of activities and that for some people it is the preferred means of communication. These studies should prove interesting for those examining the use and impact of SMS. {\textcopyright} 2004 Elsevier B.V. All rights reserved.},
author = {Faulkner, Xristine and Culwin, Fintan},
doi = {10.1016/j.intcom.2004.11.002},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Faulkner, Culwin - 2005 - When fingers do the talking A study of text messaging(2).pdf:pdf},
isbn = {0953-5438},
issn = {09535438},
journal = {Interacting with Computers},
keywords = {Communication,E-mail,SMS,Text messaging},
title = {{When fingers do the talking: A study of text messaging}},
year = {2005}
}
@misc{RstudioTeam2016a,
author = {{RStudio and Inc.}},
publisher = {github.com/rstudio/rmarkdown},
title = {{rmarkdown: R Markdown Document Conversion}},
url = {http://rmarkdown.rstudio.com/},
year = {2016}
}
@article{Ghayoomi,
abstract = {Word prediction is the problem of guessing the words which are likely to follow in a given text segment by displaying a list of the most probable words that could appear in that position. In this research, we designed and implemented three word predictors for Persian. Our baseline is a statistical-based system which uses language models. The first system uses word statistics; in the second one we use the main syntactic categories of a Persian POS tagged corpus; and the last one uses the main syntactic categories along with their morphological, syntactic and semantic subcategories. Using KeyStroke Saving (KSS) as the most important metrics to evaluate systems' performance, the primary word-based statistical system achieved 37{\%} KSS, and the second system that used only the main syntactic categories with word-statistics achieved 38.95{\%} KSS. Our last system which used all of the available information to the words get the best result by 42.45{\%} KSS.},
author = {Ghayoomi, Masood and Daroodi, Ehsan},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ghayoomi - 2008 - A POS-Based Word Prediction System for.pdf:pdf},
keywords = {POS tagging,statistical language modeling,word prediction},
title = {{A POS-based Word Prediction System for the Persian Language}}
}
@article{Neunerdt,
abstract = {—Using social media tools such as blogs and forums have become more and more popular in recent years. Hence, a huge collection of social media texts from different communities is available for accessing user opinions, e.g., for marketing studies or acceptance research. Typically, methods from Natural Language Processing are applied to social media texts to automatically recognize user opinions. A fundamental component of the linguistic pipeline in Natural Language Processing is Part-of-Speech tagging. Most state-of-the-art Part-of-Speech taggers are trained on newspaper corpora, which differ in many ways from non-standardized social media text. Hence, applying common taggers to such texts results in performance degradation. In this paper, we present extensions to a basic Markov model tagger for the annotation of social media texts. Considering the German standard Stuttgart/ ubinger TagSet (STTS), we distinguish 54 tag classes. Applying our approach improves the tagging accuracy for social media texts considerably, when we train our model on a combination of annotated texts from newspapers and Web comments.},
author = {Neunerdt, Melanie and Reyer, Michael and Mathar, Rudolf},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Neunerdt, Reyer, Mathar - Unknown - A POS Tagger for Social Media Texts Trained on Web Comments(2).pdf:pdf},
keywords = {German,Index Terms—Natural language processing,opinion mining,part-of-speech tagging},
title = {{A POS Tagger for Social Media Texts Trained on Web Comments}}
}
@article{Pauls2011,
abstract = {N-gram language models are a major resource bottleneck in machine translation. In this paper, we present several language model implementations that are both highly compact and fast to query. Our fastest implementation is as fast as the widely used SRILM while requiring only 25{\%} of the storage. Our most compact representation can store all 4 billion n-grams and associated counts for the Google n-gram corpus in 23 bits per n-gram, the most compact lossless representation to date, and even more compact than recent lossy compression techniques. We also discuss techniques for improving query speed during decoding, including a simple but novel language model caching technique that improves the query speed of our language models (and SRILM) by up to 300{\%}.},
author = {Pauls, Adam and Klein, Dan},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pauls, Klein - Unknown - Faster and Smaller N -Gram Language Models(2).pdf:pdf},
isbn = {978-1-932432-87-9},
journal = {Computational Linguistics},
pages = {258--267},
title = {{Faster and Smaller N-Gram Language Models}},
url = {http://www.aclweb.org/anthology/P11-1027},
year = {2011}
}
@article{Katz1987,
abstract = {See, stats, and : https : / / www . researchgate . net / publication / 2572004 Estimation the recognizer Article Source : CiteSeer CITATIONS 37 READS 653 4 , including : Some : Callsurf Quaero Lori French 342 , 990 SEE G . Adda French 161 , 604 SEE All . The . All - text and , letting .Abstract-Thedescriptionofanoveltypeofrn-gramlanguagemodelisgiven.Themodeloffers,viaanonlinearrecursiveprocedure,acom-putationandspaceefficientsolutiontotheproblemofestimatingprob-abilitiesfromsparsedata.Thissolutioncomparesfavorablytootherproposedmethods.Whilethemethodhasbeendevelopedforandsuc-cessfullyimplementedintheIBMRealTimeSpeechRecognizers,itsgeneralitymakesitapplicableinotherareaswheretheproblemofes-timatingprobabilitiesfromsparsedataarises.Sparsenessofdataisaninherentpropertyofanyrealtext,anditisaproblemthatonealwaysencounterswhilecollectingfre-quencystatisticsonwordsandwordsequences(m-grams)fromatextoffinitesize.Thismeansthatevenforaverylargedatacol-lection,themaximumlikelihoodestimationmethoddoesnotallowustoadequatelyestimateprobabilitiesofrarebutneverthelesspos-siblewordsequences-manysequencesoccuronlyonce("single-tons");manymoredonotoccuratall.Inadequacyofthemaximumlikelihoodestimatorandthenecessitytoestimatetheprobabilitiesofm-gramswhichdidnotoccurinthetextconstitutetheessenceoftheproblem.Themainideaoftheproposedsolutiontotheproblemistore-duceunreliableprobabilityestimatesgivenbytheobservedfre-quenciesandredistributethe"freed"probability"mass"amongm-gramswhichneveroccurredinthetext.Thereductionisachievedbyreplacingmaximumlikelihoodestimatesform-gramshavinglowcountswithrenormalizedTuring'sestimates[l],andthere-distributionisdoneviatherecursiveutilizationoflowerlevelcon-ditionaldistributions.WefoundTuring'smethodattractivebe-causeofitssimplicityanditscharacterizationastheoptimalempiricalBayes'estimatorofamultinomialprobability.Robbinsin[2]introducestheempiricalBayes'methodologyandNadasin[3]givesvariousderivationsoftheTuring'sformula.LetNbeasampletextsizeandletn,bethenumberofwords(m-grams)whichoccurredinthetextexactlyrtimes,sothatN=Crn,.},
author = {Katz, Slavam and Lamel, Lori and Adda, G},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Katz, Lamel, Adda - 1987 - Estimation of probabilities from sparse data for the language model component of a speech recognizer.pdf:pdf},
issn = {0096-3518},
journal = {IEEE transactions on acoustics, speech, and signal processing},
number = {3},
pages = {400--401},
title = {{Estimation of probabilities from sparse data for the language model component of a speech recognizer}},
url = {https://www.researchgate.net/profile/Lori{\_}Lamel/publication/2572004{\_}Estimation{\_}of{\_}probabilities{\_}from{\_}Sparse{\_}data{\_}for{\_}the{\_}language{\_}model{\_}component{\_}of{\_}a{\_}speech{\_}recognizer/links/5422cdc10cf26120b7a55d60.pdf},
volume = {35},
year = {1987}
}
@misc{RstudioTeam2016,
author = {{Rstudio Team}},
booktitle = {RStudio},
title = {{RStudio – Open source and enterprise-ready professional software for R}},
url = {https://www.rstudio.com/},
year = {2016}
}
@article{Francis1979,
author = {Francis, W N and Kucera, H},
journal = {Letters to the Editor},
number = {2},
pages = {7},
title = {{Brown corpus manual}},
url = {http://clu.uni.no/icame/manuals/BROWN/INDEX.HTM},
volume = {5},
year = {1979}
}
@article{Agarwal2007a,
abstract = {The use of digital mobile phones has led to a tremendous increase in communication using SMS. On a phone keypad, multiple words are mapped to same numeric code. We propose a Context Based Word Prediction system for SMS messaging in which context is used to predict the most appropriate word for a given code. We extend this system to allow informal words (short forms for proper English words). The mapping from informal word to its proper English words is done using Double Metaphone Encoding based on their phonetic similarity. The results show 31{\%} improvement over the traditional frequency based word estimation. Introduction The growth of wireless technology has provided us with many new ways of communication such as SMS (Short Message Service). SMS messaging can also be used to interact with automated systems or participating in contests. With tremendous increase in Mobile Text Messaging, there is a need for an efficient text input system. With limited keys on the mobile phone, multiple letters are mapped to same number (8 keys, 2 to 9, for 26 alphabets). The many to one mapping of alphabets to numbers gives us same numeric code for multiple words.},
author = {Agarwal, Sachin},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Agarwal, Arora - Unknown - Context Based Word Prediction for Texting 1 Language(2).pdf:pdf},
journal = {Large Scale Semantic Access to Content (Text,},
pages = {360--368},
title = {{Context based word prediction for texting language}},
url = {http://dl.acm.org/citation.cfm?id=1931426},
year = {2007}
}
@article{Crystal,
author = {Crystal, David and Clark, Rykia and Timmons, Taylor and Kanski, Alison and Campbell, Jeanna and Reese, Hanna and Murphy, Katey},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Crystal - 2009 - Txtng the Gr8 Db8.pdf:pdf},
title = {{Txting: The gr8 db8}}
}
@article{Aliprandi2007,
abstract = {We present FastType, an innovative system for word and letter prediction for an inflected language, namely the Italian language. The system is based on combined statistical and lexical methods and it uses robust language resources. Word prediction is particularly useful to minimise keystrokes for users with special needs, and to reduce misspellings for users having limited Italian proficiency. Word prediction can be effectively used in language learning, by suggesting correct and well-formed words to non-native users. This is significant, and particularly difficult to cope with, for inflected languages such as Italian, where the correct word form depends on the context. After describing the system, we evaluate its performances and, besides the high Keystrokes Saving, we show that FastType outclasses typical word prediction limitations getting outstanding results even over a very large dictionary of words.},
author = {Aliprandi, Carlo and Carmignani, Nicola and Mancarella, Paolo},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Aliprandi, Carmignani, Mancarella - 2007 - An Inflected-Sensitive Letter and Word Prediction System(2).pdf:pdf},
journal = {System},
keywords = {2006,2007,accepted,alternative and,and particularly difficult to,assistive technology,augmentative communication,august 5,computer aided language learning,december 30,june 10,natural language processing,nlp,received,revised,this is significant,users,word prediction},
number = {2},
pages = {79 -- 85},
title = {{An Inflected-Sensitive Letter and Word Prediction System}},
volume = {5},
year = {2007}
}
@article{Trnka2008,
abstract = {We present the development and tuning of a topic-adapted language model for word prediction, which improves keystroke savings over a comparable baseline. We outline our plans to develop and integrate style adaptations, building on our experience in topic modeling to dynamically tune the model to both topically and stylistically relevant texts.},
author = {Trnka, Keith},
doi = {10.3115/1564154.1564167},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Trnka - Unknown - Adaptive Language Modeling for Word Prediction(2).pdf:pdf},
journal = {HLT-SRWS '08 Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies},
number = {June},
pages = {61--66},
title = {{Adaptive language modeling for word prediction}},
url = {http://dl.acm.org/citation.cfm?id=1564167},
year = {2008}
}
@article{Cavalieri2015,
abstract = {This paper concentrates on improving a text-based human-machine interface integrated into a robotic wheelchair. Since word prediction is one of the most common methods used in such systems, the goal of this work is to improve the results using this specific module. For this, an expo‐ nential interpolation language model (LM) is considered. First, a model based on partial differential equations is proposed; with the appropriate initial conditions, we are able to design a interpolation language model that merges a word-based n-gram language model and a part-of-speech-based language model. Improvements in keystroke saving (KSS) and perplexity (PP) over the word-based n-gram language model and two other traditional interpola‐ tion models are obtained, considering two different task domains and three different languages. The proposed interpolation model also provides additional improve‐ ments over the hit rate (HR) parameter.},
author = {Cavalieri, DC and Bastos-Filho, Teodiano and Palazuelos-Cagigas, SE},
doi = {10.5772/61753},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cruz Cavalieri et al. - Unknown - On Combining Language Models to Improve a Text-based Human-machine Interface Regular Paper.pdf:pdf},
journal = {International Journal of Advanced Robotic Systems},
keywords = {Communication Aid,Human-machine Interfaces,Language Modelling,Word Prediction Systems},
title = {{On Combining Language Models to Improve a Text-based Human-machine Interface}},
url = {http://journals.sagepub.com/doi/pdf/10.5772/61753 http://www.intechopen.com/journals/international{\_}journal{\_}of{\_}advanced{\_}robotic{\_}systems/on-combining-language-models-to-improve-a-text-based-human-machine-interface},
year = {2015}
}
@incollection{Carroll1967,
address = {Providence},
author = {Carroll, J.B.},
booktitle = {Computational Analysis of Present-Day American English},
editor = {Kucera, H and Francis, W.N.},
pages = {406--424},
publisher = {Brown University Press},
title = {{On sampling from a lognormal model of frequency distribution}},
year = {1967}
}
@misc{Norvig2013,
author = {Norvig, Peter},
title = {{English Letter Frequency Counts: Mayzner Revisited or ETAOIN SRHLDCU}},
url = {http://norvig.com/mayzner.html http://norvig.com/mayzner.html (Archived at: http://www.webcitation.org/6b56XqsfK)},
year = {2013}
}
@misc{Duggan2015,
author = {Duggan, Maeve},
title = {{Mobile Messaging and Social Media 2015 | Pew Research Center}},
url = {http://www.pewinternet.org/2015/08/19/mobile-messaging-and-social-media-2015/},
urldate = {2016-12-14},
year = {2015}
}
@techreport{PortioResearch2015,
abstract = {This report explains how three key trends are shaping the future of communications and technology: ? A rapid shift to a mobile first mindset ? The unprecedented reach of SMS ? The emphasis on content personalisation These three trends have already fundamentally changed how people purchase goods and services, and they will continue to change the way consumers interact with companies and services. Throughout this report, we bring your attention to the ubiquity of SMS. 6.1 billion people, out of a total human population of 7.3 billion, use an SMS-capable mobile phone. SMS can be used to reach 84{\%} of the human race alive today. SMS is a true mobile first technology, and SMS offers enterprises an opportunity to communicate with their customers, staff and suppliers in a highly personalised and extremely responsive way. The future of the telecoms, media, technology and consumer electronics industries are merging and changing faster now than ever before, and we believe the trends discussed in this report will continue to be profoundly important over the next decade.},
author = {PortioResearch},
booktitle = {Megatrends in consumer technology},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - 2015 - SMS the language of 6 billion people Megatrends in consumer technology.pdf:pdf},
pages = {1--49},
title = {{SMS: The language of 6 billion people}},
url = {www.portioresearch.com{\%}5Cnportioresearch.com},
year = {2015}
}
@book{R.H.Baayen2008,
abstract = {Statistical analysis is a useful skill for linguists and psycholinguists, allowing them to understand the quantitative structure of their data. This textbook provides a straightforward introduction to the statistical analysis of language. Designed for linguists with a non-mathematical background, it clearly introduces the basic principles and methods of statistical analysis, using 'R', the leading computational statistics programme. The reader is guided step-by-step through a range of real data sets, allowing them to analyse acoustic data, construct grammatical trees for a variety of languages, quantify register variation in corpus linguistics, and measure experimental data using state-of-the-art models. The visualization of data plays a key role, both in the initial stages of data exploration and later on when the reader is encouraged to criticize various models. Containing over 40 exercises with model answers, this book will be welcomed by all linguists wishing to learn more about working with and presenting quantitative data.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Baayen, R H and {R. H. Baayen}},
doi = {10.1558/sols.v2i3.471},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Baayen - Unknown - Analyzing Linguistic Data A practical introduction to statistics.pdf:pdf},
isbn = {9780521882590},
issn = {17508657},
keywords = {R,linguistics,psycholinguistics,statistics},
number = {3},
pages = {368},
pmid = {25246403},
publisher = {Cambridge University Press},
title = {{Analyzing Linguistic Data: A Practical Introduction to Statistics using R}},
volume = {2},
year = {2008}
}
@inproceedings{Talbot2007,
abstract = {A Bloom filter (BF) is a randomised data structure for set membership queries. Its space requirements fall significantly below lossless information-theoretic lower bounds but it produces false positives with some quantifiable probability. Here we present a general framework for deriving smoothed language model probabilities from BFs. We investigate how a BF containing n-gram statistics can be used as a direct replacement for a conventional n-gram model. Recent work has demonstrated that corpus statistics can be stored efficiently within a BF, here we consider how smoothed language model probabilities can be derived efficiently from this randomised representation. Our proposal takes advantage of the one-sided error guarantees of the BF and simple inequalities that hold between related n-gram statistics in order to further reduce the BF storage requirements and the error rate of the derived probabilities. We use these models as replacements for a conventional language model in machine translation experiments.},
author = {Talbot, David and Osborne, Miles},
booktitle = {EMNLP-2007},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Talbot, Osborne - Unknown - Smoothed Bloom filter language models Tera-Scale LMs on the Cheap.pdf:pdf},
number = {June},
pages = {468--476},
title = {{Smoothed Bloom filter language models: Tera-scale LMs on the cheap}},
url = {http://homepages.inf.ed.ac.uk/miles/papers/emnlp07.pdf papers2://publication/uuid/6DB50926-4247-433E-9B2D-1C1D3B9B4442},
year = {2007}
}
@article{Pecina,
author = {Pecina, Pavel},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pecina - 2009 - Collocation Extraction AND THEORETICAL LINGUISTICS.pdf:pdf},
title = {{LEXICAL ASSOCIATION MEASURES Collocation Extraction}}
}
@article{Mandelbrot1953,
author = {Mandelbrot, Benoit},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mandelbrot - 1953 - An Informational Theory of the Statistical Structure of Language.pdf:pdf},
journal = {Communication Theory},
number = {2},
pages = {486--502},
title = {{An Informational Theory of the Statistical Structure of Language}},
url = {http://www.uvm.edu/pdodds/files/papers/others/1953/mandelbrot1953a.pdf},
year = {1953}
}
@misc{Xie2013,
abstract = {Suitable for both beginners and advanced users, this book shows you how to write reports in simple languages such as Markdown. The reports range from homework, projects, exams, books, blogs, and web pages to any documents related to statistical graphics, computing, and data analysis. While familiarity with LaTeX and HTML is helpful, the book requires no prior experience with advanced programs or languages. For beginners, the text provides enough features to get started on basic applications. For power users, the last several chapters enable an understanding of the extensibility of the knitr package.},
author = {Xie, Yihui},
isbn = {1482203537},
title = {{knitr: Elegant, flexible and fast dynamic report generation with R | knitr}},
url = {http://yihui.name/knitr/ http://yihui.name/knitr/{\%}5Cnpapers2://publication/uuid/21E89689-A1A0-4422-A0C8-45D31FA20C5B},
year = {2013}
}
@article{Even-zohar1995a,
abstract = {The eventual goal of a language model is to accu-rately predict the value of a missing word given its context. We present an approach to word prediction that is based on learning a representation for each word as a function of words and linguistics pred-icates in its context. This approach raises a few new questions that we address. First, in order to learn good word representations it is necessary to use an expressive representation of the context. We present a way that uses external knowledge to gener-ate expressive context representations, along with a learning method capable of handling the large num-ber of features generated this way that can, poten-tially, contribute to each prediction. Second, since the number of words "competing" for each predic-tion is large, there is a need to "focus the attention" on a smaller subset of these. We exhibit the contri-bution of a "focus of attention" mechanism to the performance of the word predictor. Finally, we de-scribe a large scale experimental study in which the approach presented is shown to yield significant im-provements in word prediction tasks.},
annote = {Not relevant to current task. Considers features of sentense and context that is not available to the mobile phone prediction problem.},
author = {Even-zohar, Yair},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Even-zohar - 1995 - to Word Prediction.pdf:pdf},
pages = {124--131},
title = {{to Word Prediction *}},
year = {1995}
}
@misc{LawlerJohn1999,
author = {LawlerJohn},
title = {{English Word List}},
url = {http://www-personal.umich.edu/{~}jlawler/wordlist},
year = {1999}
}
@article{Mezei2012,
abstract = {Many students with physical disabilities have difficulty with writing fluency due to motor limitations. One type of assistive technology that has been developed to improve writing speed and accuracy is word prediction software, although there is a paucity of research supporting its use for individuals with physical disabilities. This study used an alternating treatment design between word prediction versus word pro- cessing to examine fluency, accuracy, and passage length on writing draft papers by individuals who have physical disabilities. Results indicated that word prediction had little to no effectiveness in increasing writing speed for all of the students in this study, but it shows promise in decreasing spelling and typographical errors.},
author = {Mezei, Peter J and {Wolff Heller}, Kathryn},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mezei, Heller, Mezei - Unknown - EFFECTS OF WORD PREDICTION ON WRITING FLUENCY FOR STUDENTS WITH PHYSICAL DISABILITIES.pdf:pdf},
isbn = {978-1-124-04823-9},
journal = {Physical Disabilities: Education and Related Services},
number = {1},
pages = {3--26},
title = {{Effects of Word Prediction on Writing Fluency for Students With Physical Disabilities}},
url = {http://files.eric.ed.gov/fulltext/EJ986388.pdf},
volume = {31},
year = {2012}
}
@incollection{Biber1993a,
address = {New York},
author = {Biber, Douglas and Finegan, E.},
booktitle = {Sociolinguistic Perspectives on Register},
chapter = {2},
pages = {31--58},
publisher = {Oxford University Press},
title = {{An Analytical Framework for Register Studies}},
year = {1993}
}
@inproceedings{Ghayoomi2009,
abstract = {—The prediction task in national language processing means to guess the missing letter, word, phrase, or sentence that likely follow in a given segment of a text. Since 1980s many systems with different methods were developed for different languages. In this paper an overview of the existing prediction methods that have been used for more than two decades are described and a general classification of the approaches is presented. The three main categories of the classification are statistical modeling, knowledge-based modeling, and heuristic modeling (adaptive). Index Terms—Word prediction, Assistant technology, Lan-guage modeling.},
author = {Ghayoomi, Masood and Momtazi, Saeedeh},
booktitle = {Conference Proceedings - IEEE International Conference on Systems, Man and Cybernetics},
doi = {10.1109/ICSMC.2009.5346027},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ghayoomi, Momtazi - Unknown - An Overview on the Existing Language Models for Prediction Systems as Writing Assistant Tools.pdf:pdf},
isbn = {9781424427949},
issn = {1062922X},
keywords = {Assistant technology,Language modeling,Word prediction},
pages = {5083--5087},
title = {{An overview on the existing language models for prediction systems as writing assistant tools}},
year = {2009}
}
@misc{Xie2014,
author = {Xie, Yihui},
title = {{The printr Package | Automatically Print R Objects According to knitr Output Format}},
url = {http://yihui.name/printr/},
year = {2014}
}
@article{Hasselgren2003,
abstract = {Due to the emergence of SMS messages, the significance of effective text entry on limited-size keyboards has increased. In this paper, we describe and discuss a new method to enter text more efficiently using a mobile telephone keyboard. This method, which we called HMS, predicts words from a sequence of keystrokes using a dictionary and a function combining bigram frequencies and word length. We implemented the HMS text entry method on a software-simulated mobile telephone keyboard and we...},
author = {Hasselgren, Jon and Montnemery, E},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hasselgren, Montnemery - 2003 - Hms A predictive text entry method using bigrams.pdf:pdf},
journal = {{\ldots} for Text Entry Methods},
pages = {43--50},
title = {{Hms: A predictive text entry method using bigrams}},
url = {http://dl.acm.org/citation.cfm?id=1628201},
year = {2003}
}
@article{Garay-Vitoria2004,
abstract = {Prediction is one of the most extended techniques to enhance the rate of communication for people with motor and speech impairments who use Augmentative and Alternative Communication systems. There is an enormous diversity of prediction methods and techniques mentioned in the literature. Therefore, the designer finds tremendous difficulties in understanding and comparing them in order to decide the most convenient technique for a specific design. This paper presents a survey on prediction techniques applied to communicators with the intention of helping them to understand this field. Prediction applications and related features, such as block size, dictionary structure, prediction method, interface, special features, measurement and results, are detailed. Systems found in the literature are studied and described. Finally, a discussion is carried out on the possible comparison among the different methods. {\textcopyright} Springer-Verlag 2004.},
author = {Garay-Vitoria, Nestor and Abascal, Julio},
doi = {10.1007/978-3-540-30111-0_35},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Garay-Vitoria, Abascal - 2004 - A comparison of prediction techniques to enhance the communication rate(2).pdf:pdf},
isbn = {978-3-540-23375-6},
issn = {03029743 16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {October},
pages = {400--417},
title = {{A comparison of prediction techniques to enhance the communication rate}},
volume = {3196},
year = {2004}
}
@article{Wickham2016b,
author = {Wickham, Hadley},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wickham - 2016 - Package ‘stringr'.pdf:pdf},
title = {{Package ‘stringr'}},
year = {2016}
}
@misc{Cool-Smileys2010,
author = {Cool-Smileys},
title = {{List of Text Emoticons: The Ultimate Resource}},
url = {http://cool-smileys.com/text-emoticons},
year = {2010}
}

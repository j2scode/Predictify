# Related Work
In this section, several approaches to word prediction are surveyed. This is not intended to be an exhaustive review, rather, the focus is on novel approaches and those considered state-of-the-art in word prediction. First, statistical n-gram based approaches and language models (LMs) are examined. Then, language models that incorporate syntactice and semantic knowledge are considered.

## Statistical Word Prediction 

### CMU-Cambridge Statistical Language Modeling Toolkit
Originally released in 1994, by Philip Clarkson of Cambridge University and Ronald Rosenfeld of Carnegie Melon University, the CMU-Cambridge Statistical Language Model Toolkit is a collection of Unix software tools for developing and testing traditional bigram and trigram language models [Clarkson1997]. It now supports $N$-gram models for any value of $N$, Good-Turing [10.2307/2333344] discounting with backoff, linear discounting, absolute discounting, and Witten-Bell [Witten:2006:ZPE:2263404.2271157] discounting. Both closed and open vocabulary models are supported.  The closed vocabulary model makes no accommodation for OOV tokens, which cause the system to error.  The open vocabulary model maps all OOV words in the training set to a single symbol and allows OOV tokens to be defined in a test set. Multiple models may be combined with linear interpolation and evaluated using the perplexity statistic  [Clarkson1997].

### MIT Language Model Toolkit
The MIT Language Model Toolkit (MITLM) supports the estimation of statistical N-gram language models involving iterative estimation of smoothing, interpolation, and N-gram weighting parameters. It's default model is the trigram with modified Kneser-Ney smoothing, although traditional Kneser-Ney with discount tuning, and other N-gram orders are also supported. Linear interpolation is supported with tunable weights. Closed and open vocabulary models are supported. 

### VariKN
The VariKN [Silvola] language model is a toolkit for training n-gram language models using absolute discounting, interpolated Kneser-Ney smoothing, revised Kneser pruning and Kneser-Ney growing which enables very high-order N-grams. Absolute discounting and interpolated Kneser-Ney smoothing was introduced in the prior section.  In the **revised Kneser Pruning technique**, the model is revised after each N-gram has been pruned, the back-off coefficients and distributions are updated, and the impact on the log probability *by the N-gram count* (rather than the probability estimated *by the model*), is assessed [Siivola2007a]. If the decrease in log probability is greater than the pruning threshold, the N-gram is restored. Analogously, the **Kneser-Ney Growing algorithm incrementally creates a variable length model, rather than computing all N-grams up to a certain order, then pruning. The algorithm starts with an interpolated 1-g Kneser-Ney model, adding each N-gram $w$ at order $k=1$, then adds each $k+1$ N-gram $w_1$,$w$ to the model only if it they meet a cost criterion. Since the model is grown one distribution at a time, individual unnecessary N-grams are pruned. The costs criterion consists of the cost of encoding the training data - the log probability of the training data, and the cost of coding the N-gram model - the number of bits required to store each float with given precision. Experiments on Finnish and English text corpora have shown that the revised Kneser-Ney algorithm gives significantly lower cross-entropies when compared to the previous pruning algorithms and the Kneser-Key Growing algorithm further reduces perplexity [Siivola2007a].

### Web-Based Language Models
Several language language model toolkits have been developed for very large web-based corpora. The **IRST Language Modeling Toolkit**, features algorithms and data structures for estimating, storing, and accessing very large N-gram language models with billions of N-grams on conventional machines. Stored in the conventional ARPA format, IRSTLM performs N-gram collection and storage using efficient data structures and distributed processing. Standard interpolation as well as Witten-Bell [Witten:2006:ZPE:2263404.2271157] and improved Kneser-Ney smoothing algorithms are supported [Federico2008]. Their model data structure implements a forward trie, in which each node contains a sorted array of entries and words are looked up in their natural left-to-right order. This with a binary file format with memory mapping offers lower memory consumption and caching to increase speed and quantization [Heafield2011].

The **Berkeley LM** [Pauls2011] is a Java-based library for the efficient storage and estimation of very large N-Gram language models in memory. Encoding techniques, direct address caching, and changes to the language model interface, enables the system to store the 4 billion N-grams of the Google WebIT [Brants2006] corpus with associated counts in 10 GB of memory. Implicit tries [Whittaker2001], a more compact implementation of traditional tries structures, encoded nodes in tries into entries in arrays with enough bits to index all the words in the model. Context encoding, based upon the work of [Hsu2008], encodes an N-gram with its context such that the pair can be stored in a single 64-bit integer. Experiments have shown improvements in query speed over SRILM and compression rates against state-of-the-art lossy compression [Pauls2011].

**RandLM** is a space-efficient N-gram based language model built using Bloom filters (BF), a randomized data structure fall with space requirements that are significantly below lossless information-theoretic lower bounds [Talbot2007]. Standard as well as Witten-Bell [Witten:2006:ZPE:2263404.2271157] and Kneser-Ney smoothing algorithms are implemented. Experiments have demonstrated the RandLM to substantially reduce memory requirements of very large N-gram models.

The **KenLM** [Heafield2011], lead by Kenneth Heafield of Carnegie Mellon University, is one of the fastest open-source language model toolkits used in available.  Estimation is based upon modified Kneser-Ney smoothing without pruning.  It supports multi-core processing and features two data structures for efficient language model queries. The PROBING data structure, designed for speed, uses linear probing hash tables. The TRIE data structure reduces memory consumption with with bit-level packing, sorted records, interpolation search, and optional quantization. Experimenets have shown that it is faster and uses less memory than SRILM and IRSTLM.

The **OpenGrm** NGram library is used to make and modify n-gram language models encoded as weighted finite-state transducers (FSTs). Functionality includes counting, smoothing, pruning, applying, and evaluating models.  It supports 1-gram through 5-gram counts, smoothing using Katz [Katz1987] Kneser-Ney [Kneser1995], and (default) [Witten-Bell Witten:2006:ZPE:2263404.2271157]. It also provides support for distributed computation [Rossenstock].

## Syntactic Word Prediction
Word-based probabilistic models, augmented with syntactic models that use part-of-speech information to predict more syntactically approrpiate words, have been shown to outperform baseline word-based models in terms of key-stroke savings and reduced perplexity. This section compiles some of the syntactic word prediction research and systems that implement them.

### Syntactic Word Prediction Research
[@Fazly2003] introduced two algorithms - Tags-and-Words, and Linear Combination. Tag-of-Words combined a POS-tag trigram model and a word bigram model into a single model.  The POS-tag trigram model attempted to find the most likley POS tag for the current postition given the two previous POS tags.  The bigram model identified words that have the highest probability of being in the current position, given the most likely POS tag for this position and the previous word.  The Linear Combintation was a linear interpolation of the two models. The linear combination model significantly outperformed the baseline bigram model. The Tags-and-Words model performed only slightly better than baseline. [@Gh2001] implemented bigram and trigram POS models that estimated the probability of the previous word $w_{t-1}$ belonging to the previous words' POS category  $c_{t-1}$ , the probability that category $c_t^i$ follows category $c_{t-1}^j$, and multiplied that by the probability of predicting $w_t$, given category $c_t^i$. [@Copestake1997]  Profet [@Carlberger1997b], a word prediction system used as a writing aid for people with motor dysfunction incorporates POS tag unigrams, bigrams, and trigrams. [@Ghayoomi] implemented a POS based prediction system for the Persian language using word and POS quadgrams.

## Semantic Word Prediction 
[@Wandmacher2008] used Latent Semantic Analysis and Singular Value Decomposition (SVD) to integrate semantic similarity to language models.  Probabilities were based upon the normalized cosine similarity of each word vector and the vector of the current context. Semantic cache, partial reranking and standard interpolation model integration techniques were evaluated. Semantic cache assigned a higher probability to those words in the cache with a higher cosine similarity with the word being predicted. Partial reranking filtered the best $n$ candidates from the basic language model in order to prevent the LSA model from making implausible predictions. Standard interpolation simply added the weighted probabilities of the two models. Their experiment showed that all measures outperformed baseline in terms of keystroke savings rate (ksr), and perpelexity.


## Integrated Word Prediction 
### SRI Language Model Toolkit
The SRI Language Modeling Toolkit [Stolcke2002], by SRI International, Menlo Park, CA, is a collection of C++ libraries, executables, and scripts that support language model estimation and evaluation for us in speech recognition, statistical tagging and segmentation, and machine translation applications. Since most language models in SRILM are N-gram based, it supports "theoretically" any order N-gram model. SRILM supports an array of standard smoothing and discounting algorithms such as Good-Turing, absolute, Witten-Bell, Jelinek-Mercer smoothing, and original and modified Kneser-Ney. Users can select an optional predefined vocabulary to limit or augment the words from the training data. Out-of-vocabulary words can be ignored or treated as a special "unknown word" token. 

In addition to the standard word-based N-gram models, SRILM implements and integrates several other LMs.  **Class-based models** incorporate domain knowledge such as word classes. Word classes can be manually created or learned using bigram statistics. Dynamic programming is used to disambiguate words that belong to multiple classes.  **Cache models** assign non-zero probabilities to recent words to capture the tendency of words to reoccur over short time spans [DeMori1996]. These models are typically interpolated with standard n-gram models to improve predictive performance. **Skip language models** probabilistically skip words in the LM history to include more distant words. The skipping probabilities associated with each word are estimated using expectation minimalization. SRILM has implemented **factored language models** (FLMs) proposed by Bilmes and Kirchoff [Bilmes2003] which represents words as discrete features. This technique has applicability to languages with complex morphology such as Arabic [Vergyri2004]. 

Several methods for N-gram model integration have been implemented [Stolcke2011].  Two or more **dynamically interpolated LMs** can be combined such that the interpolation weights reflect the likelihood of the model given n-gram history. **Unigram marginal adaptation** [Kneser1997] integrates an existing Ngram language model with unigram statistics from an adaptation set by scaling the N-gram probabilities to conform to the unigram distribution. **Log-linear interpolation** of LMs [Klakow1998a] is "...similar to standard linear interpolation except that probabilities are combined multiplicatively rather than additively." [Stolcke2011]

### FASTY - A Multi-Lingual Word Prediction System
Fasty [@Matiasek2002a], a multi-lingual word prediction system integrates its n-gram model with POS based statistics,  morphological processing, grammar-based and collocation-based prediction.  Unigram and bigram-based statistics, combined with a trigram-based POS tag model forms the baseline.  Morphological processing, implemented via finite state-tranducers is used where the other components run out of predictions. Grammar-based prediction analyzes predictions in a wide syntactic contexts and provides syntactic criteria for ranking predictions. Collocation processing assigns higher probabilities to words that have been observed together in the contexet.  

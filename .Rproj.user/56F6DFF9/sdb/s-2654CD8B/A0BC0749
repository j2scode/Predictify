{
    "collab_server" : "",
    "contents" : "## ---- mkn_evaluate\n#==============================================================================#\n#                                 mknEvaluate                                  #\n#==============================================================================#\n#'  mknEvaluate\n#' \n#' This function evaluates a test set on the modified kneser-ney model, \n#' returning a data frame containing model size data, processing time, and \n#' perplexity scores. \n#' \n#' @param mkn - the meta data for the language model\n#' @param training the meta data for the training data \n#' @param test - the meta data for the validation or test corpus\n#' @param sents - the number of sentences from the test data to evaluate, \n#'                NULL = all sentences\n#' @param directories - the project directory structure\n#' @return estimates - the probability estimates for each word in the test data\n#' @author John James\n#' @export\nmknEvaluate <- function(mkn, training, test, sents = NULL, directories) {\n  \n  memory.limit(20000)\n  startTime <- Sys.time()\n  message(paste(\"\\nEvaluating performance on \", mkn$mDesc, 'at', startTime))\n  \n  \n  # Function that performs the quadgram probability estimate \n  getScore <- function(ngram, n) {\n    \n    ngramType <- list('Quadgram', 'Trigram', 'Bigram', 'Unigram')\n    res <- list()\n    \n    # Check if n-gram exists in training, and obtain probability\n    res$prob <- model[[n]][ nGram == ngram][, Pmkn]\n    res$type <- ngramType[[n]]\n    \n    if (length(res$prob) != 0) {\n      return(res)\n    } else if (n > 1) {\n      ngram <- paste0(unlist(strsplit(ngram, ' '))[-1], collapse = ' ')\n      return(getScore(ngram, n-1))\n    } else {\n      res$prob <- model[[n]][ nGram == 'UNK'][, Pmkn]\n      return(res)\n    }\n  }\n    \n   \n  scoreSentence <- function(sentence) {\n    tokens <- unlist(quanteda::tokenize(sentence, what = 'word'))\n    rbindlist(lapply(seq_along(tokens[1:(length(tokens)-3)]), function(x) {\n      ngram <- paste0(tokens[x:(x+3)], collapse = ' ')\n      score <- getScore(ngram, mkn$mOrder)\n      sentScore <- list()\n      sentScore$quadgram <- ngram\n      sentScore$ngramType <- score$type\n      sentScore$logProb <- score$prob\n      sentScore\n    }))\n  }\n  \n  message('...loading training data')\n  train <- readFile(training$processed[[mkn$mOrder]])\n  df <- quanteda::dfm(train, tolower = FALSE, remove = 'BOS')\n  V <- length(featnames(df))\n  trainTokens <- sum(ntoken(df))\n\n  message(paste('...loading language model'))\n  model <- lapply(seq_along(mkn$model), function(x) {\n    loadObject(mkn$model[[x]])\n  })\n\n  message(paste('...loading test data'))\n  #filePath <- test$processed[[mkn$mOrder]]\n  filePath <- test\n  document <- readFile(filePath)\n  if (!(is.null(sents))) {\n    document <- sampleData(document, numChunks = sents, chunkSize = 1, format = 'v')\n  }\n  # Compute number of sentences and tokens w/o 'BOS'\n  M <- length(document) # M = number of sentences\n\n  message('...evaluating sentence probabilities')\n  scores <- rbindlist(lapply(seq_along(document), function(x) {\n    s <- scoreSentence(document[x])\n    if (x %in% c(100, 200, 500, 1000, 5000, 10000, 20000)) { \n      elapsed <- round(difftime(Sys.time(), startTime,  units = 'mins'))\n      elapsed <- as.numeric(elapsed) + 1\n      rate <- x / elapsed \n      remaining <- length(document) - x\n      timeMin <- round(remaining / rate, digits = 1)\n      timeHrs <- round(timeMin / 60, digits = 1)\n      message(paste('......',x,'out of',length(document), 'sentences processed in', \n                    elapsed, 'minutes.', timeMin,'minutes remaining (', timeHrs, 'hours)'))\n    }\n    s\n  }))\n  \n\n  # Compute perplexity\n  N <- length(scores$logProb)\n  pp <- 2^-(sum(scores$logProb) / N)\n  \n  # Note the time\n  endTime <- Sys.time()\n  duration <- round(difftime(endTime, startTime,  units = 'auto'), 2)\n  \n  # Summarize results\n  evaluation <- list(\n    summary = list(\n      date = startTime,\n      end = endTime,\n      duration = duration,\n      model = mkn$mDesc,\n      trainingSet = training$corpusName,\n      trainingSents = length(train),\n      trainingTokens = trainTokens,\n      trainingSize = object.size(train),\n      modelSize = object.size(model),\n      testSents = M,\n      testWords = N,\n      perplexity = pp\n    ),\n    detail = scores\n  )\n  \n  # Save Analysis\n  output <- list()\n  output$directory <- directories$analysisDir\n  output$fileName  <- paste0(sub('\\\\..*', '', paste0('MKN-evaluation-')),\n                             mkn$mName,\n                             format(Sys.time(),'_%Y%m%d_%H%M%S'), '.Rdata')\n  output$objName   <- 'evaluation'\n  output$data  <- evaluation\n  saveObject(output)\n  \n  # Log and Return results\n  logR('mknEvaluate', startTime, '', ' ')\n\n  # Alert User\n  message(paste('MKN model evaluated at', endTime))\n  message(paste('Elapsed time is', duration))\n\n  return(evaluation)\n}\n## ---- end\n#ppd <- mknEvaluate(lm$mkn$epsilon, corpora$training$epsilon,  corpora$validation$epsilon, sents = NULL, directories)",
    "created" : 1498522411093.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "52610961",
    "id" : "A0BC0749",
    "lastKnownWriteTime" : 1498547741,
    "last_content_update" : 1498547741066,
    "path" : "~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM0A.mknEvaluate.R",
    "project_path" : "src/LM0A.mknEvaluate.R",
    "properties" : {
    },
    "relative_order" : 5,
    "source_on_save" : true,
    "source_window" : "",
    "type" : "r_source"
}
{
    "collab_server" : "",
    "contents" : "## ---- analysis_data\n#==============================================================================#\n#                               getFeatures                                    #\n#==============================================================================#\n#'  getFeatures\n#' \n#' This function takes as its parameters, the meta data for the corpus file,\n#' the regex patterns and reference data. The function then returns a \n#' featurevector containing various feature measures.\n#' \n#' @param type - type of analysis 'full' or 'fast'\n#' @param korpus - the meta data for the corpus\n#' @param registers - the meta data for the corpus registers\n#' @param docId - the document sequence number (1=blog, 2=news, 3=twitter)\n#' @param regex  - the regex patterns\n#' @param referenceData - abbreviations, contractions, emoticons, etc...\n#' @return analysis - a list containing a vector of feature measures \n#'                    and a list of samples\n#' @author John James\n#' @export\ngetFeatures <- function(type, korpus, registers, docId, regex, referenceData) {\n  \n  message(paste(\"...analyzing\", registers[[docId]]$fileDesc,\n                'at', Sys.time()))\n  \n  # Initialize inspection objects and key variables\n  featureVector  <- list()\n  featureSamples <- list()\n  featureTables  <- list()\n  wordLengths    <- list()\n  \n  \n  # Read Data\n  message('......loading the text')\n  document <- list()\n  document$directory <- korpus$directory\n  document$fileName <- registers[[docId]]$fileName\n  sents <- readFile(document)\n  tokens <- unlist(quanteda::tokenize(unlist(sents), what = 'word'))\n  \n  # Extract Words\n  message(\"......extracting words\")\n  words <- grep(regex$words, tokens, value = TRUE, perl = TRUE)\n  words <- gsub(regex$punct, '', words, perl = TRUE)\n  words <- words[nchar(words) > 0]\n  \n  #===========================================================================#\n  #                     Extract Descriptive Statistics                        #        \n  #===========================================================================#\n  message(\"......extracting basic features\")\n  featureVector$category   <- registers[[docId]]$fileDesc\n  featureVector$objectSize <- as.numeric(round((object.size(tokens) / 1000000), 1))\n  featureVector$sentences  <- length(sents)\n  featureVector$tokens     <- length(tokens)\n  featureVector$words      <- length(words)\n  featureVector$wordTypes  <- length(unique(words))\n  featureVector$chars      <- sum(nchar(words))\n  featureVector$wordsPerSent    <- featureVector$words / featureVector$sentences\n  featureVector$minWordLength   <- min(nchar(words))\n  featureVector$maxWordLength   <- max(nchar(words))\n  featureVector$meanWordLength  <- mean(nchar(words))\n  \n  if (type == 'full') {  \n    #=========================================================================#\n    #                 Extract Noisy Character Classes                         #        \n    #=========================================================================#\n    message(\"......extracting tokens containing control characters\")\n    control <- grep(regex$control, tokens, value = TRUE, perl = TRUE)\n    featureVector$control <- length(control)\n    controlsTable <- as.data.frame(table(control))\n    featureSamples$control <- head(controlsTable[order(-controlsTable$Freq),], 20)\n    \n    \n    message(\"......extracting tokens containing non-ASCII characters\")\n    nonAscii <- grep(regex$nonAscii, tokens, value = TRUE, perl = TRUE)\n    featureVector$nonAscii <- length(nonAscii)\n    nonAscii <- nonAscii[nchar(nonAscii) == 1]\n    nonAsciiTable  <- as.data.frame(table(nonAscii))\n    featureSamples$nonAscii  <- nonAsciiTable[order(-nonAsciiTable$Freq),]\n    \n    message(\"......extracting tokens containing non-Printable characters\")\n    nonPrint <- grep(regex$nonPrintable, tokens, value = TRUE, perl = TRUE)\n    featureVector$nonPrint <- length(nonPrint)\n    nonPrintTable  <- as.data.frame(table(nonPrint))\n    featureSamples$nonPrint  <- nonPrintTable[order(-nonPrintTable$Freq),]\n    \n    message(\"......extracting tokens containing non-UTF8 characters\")\n    nonUTF8i  <- grep(\"tokens\", iconv(tokens, \"latin1\", \"ASCII\", sub=\"tokens\"))\n    nonUTF8   <- tokens[nonUTF8i]\n    featureVector$nonUTF8 <- length(nonUTF8)\n    nonUTF8Table  <- as.data.frame(table(nonUTF8))\n    featureSamples$nonUTF8 <- nonUTF8Table[order(-nonUTF8Table$Freq),]\n    \n    #=========================================================================#\n    #                      Extract Linguistic Features                        #        \n    #=========================================================================#\n    message(\"......extracting tokens containing contractions\")\n    featureSamples$contractions <- tokens[tokens %in% referenceData$contractions$key]\n    featureVector$contractions\t<- length(featureSamples$contractions)\n    featureTables$contractions  <- as.data.frame(table(featureSamples$contractions))\n    featureTables$contractions <- featureTables$contractions[\n      order(-featureTables$contractions$Freq),]\n    featureTables$contractions$category <- registers[[docId]]$fileDesc\n    \n    message(\"......extracting tokens containing abbreviations\")\n    featureSamples$abbreviations <- tokens[tokens %in% referenceData$abbreviations$key]\n    featureVector$abbreviations\t <- length(featureSamples$abbreviations)\n    featureTables$abbreviations  <- as.data.frame(table(featureSamples$abbreviations))\n    featureTables$abbreviations <- featureTables$abbreviations[\n      order(-featureTables$abbreviations$Freq),]\n    featureTables$abbreviations$category  <- registers[[docId]]$fileDesc\n    \n    message(\"......extracting tokens containing profanity\")\n    featureSamples$badWords <- tokens[tokens %in% referenceData$badWordsFile$key]\n    featureVector$badWords\t<- length(featureSamples$badWords)\n    featureTables$badWords  <- as.data.frame(table(featureSamples$badWords))\n    featureTables$badWords <- featureTables$badWords[\n      order(-featureTables$badWords$Freq),]\n    featureTables$badWords$category  <- registers[[docId]]$fileDesc\n    \n    message(\"......extracting tokens requiring normalization\")\n    featureSamples$corrections <- tokens[tokens %in% referenceData$corrections$key]\n    featureVector$corrections\t<- length(featureSamples$corrections)\n    featureTables$corrections  <- as.data.frame(table(featureSamples$corrections))\n    featureTables$corrections <- featureTables$corrections[\n      order(-featureTables$corrections$Freq),]\n    if (featureVector$corrections > 0) { \n      featureTables$corrections$category  <- registers[[docId]]$fileDesc\n    }\n  }  \n  analysis <- list(featureVector  = featureVector,\n                   featureTables = featureTables,\n                   featureSamples = featureSamples,\n                   wordLengths = wordLengths)\n  \n  message(paste(\"...analysis for \", registers[[docId]]$fileDesc,\n                'complete at', Sys.time()))\n  return(analysis)\n}\n\n#==============================================================================#\n#                             summarizeAnalysis                                #\n#==============================================================================#\n#'  summarizeAnalysis\n#' \n#' This function takes as its parameter, the korpus meta data, the featureMatrix\n#' feature samples and the regex patterns, then produces a list of corpus total \n#' figures for the featureMatrix.\n#' \n#' @param type - indicates type of analysis 'full', or 'fast'\n#' @param korpus - the corpus meta data\n#' @param registers - the meta data for the corpus registers\n#' @param featureMatrix - the feature matrix for the 3 registers\n#' @param regex - the regex patterns\n#' @return totals - a data frame containing total feature counts for the corpus\n#' @author John James\n#' @export\nsummarizeAnalysis <- function(type, korpus, registers, featureMatrix, \n                              featureSamples, regex) {\n  \n  message(\"...summarizing corpus feature totals\")\n  gc()\n  \n  # Initialize Structures\n  featureTables <- list()\n  wordLengths   <- list()\n  \n  # Obtain word types across the combined corpus\n  message(\"......reading combined tokenized corpus\")\n  \n  sents <- unlist(lapply(seq_along(registers), function(x) {\n    document <- list()\n    document$directory <- korpus$directory\n    document$fileName <- registers[[x]]$fileName\n    readFile(document)\n  }))\n  tokens <- unlist(quanteda::tokenize(sents, what = 'word'))\n  \n  # Extract Word Types\n  message(\"......extracting total word \")\n  words <- grep(regex$words, tokens, value = TRUE, perl = TRUE)\n  words <- gsub(regex$punct, '', words, perl = TRUE)\n  words <- words[nchar(words) > 0]\n  \n  #===========================================================================#\n  #                     Summarize Descriptive Statistics                      #        \n  #===========================================================================#\n  message(\"......summarizing Descriptive Statistics\")\n  featureVector <- list()\n  featureVector$category\t  <- \t\"Corpus\"\n  featureVector$objectSize <- sum(featureMatrix$objectSize)\n  featureVector$sentences  <- \tsum(featureMatrix$sentences)\n  featureVector$tokens\t  <- \tsum(featureMatrix$tokens)\n  featureVector$words\t  <- \tsum(featureMatrix$words)\n  featureVector$wordTypes\t  <- \tlength(unique(words))\n  featureVector$chars \t  <- \tsum(featureMatrix$chars)\n  featureVector$wordsPerSent    <- length(words) / sum(featureMatrix$sentences)\n  featureVector$minWordLength\t  <- min(nchar(words))\n  featureVector$maxWordLength\t  <- max(nchar(words))\n  featureVector$meanWordLength\t<- mean(nchar(words))\n  \n  if (type == 'full') {\n    #=========================================================================#\n    #                        Summarize Noisy Features                         #        \n    #=========================================================================#\n    message(\"......summarizing noisy features\")\n    featureVector$nonAscii      <- sum(featureMatrix$nonAscii)\n    featureVector$control      <- sum(featureMatrix$control)\n    featureVector$nonPrint      <- sum(featureMatrix$nonPrint)\n    featureVector$nonUTF8      <- sum(featureMatrix$nonUTF8)\n    \n    #=========================================================================#\n    #                    Summarize Linguistic Features                        #        \n    #=========================================================================#\n    message(\"......summarizing linguistic features\")\n    \n    featureVector$contractions  <- \tsum(featureMatrix$contractions)\n    allContractions <- unlist(lapply(seq_along(featureSamples), function(x) {\n      featureSamples[[x]]$contractions\n    }))\n    featureTables$contractions  <- as.data.frame(table(allContractions))\n    if (nrow(featureTables$contractions) > 0) {\n      featureTables$contractions$category  <- 'Corpus'\n    }\n    \n    featureVector$abbreviations  <- \tsum(featureMatrix$abbreviations)\n    allabbreviations <- unlist(lapply(seq_along(featureSamples), function(x) {\n      featureSamples[[x]]$abbreviations\n    }))\n    featureTables$abbreviations  <- as.data.frame(table(allabbreviations))\n    if (nrow(featureTables$abbreviations) > 0) {\n      featureTables$abbreviations$category  <- 'Corpus'\n    }  \n    \n    featureVector$badWords  <- \tsum(featureMatrix$badWords)\n    allBadWords <- unlist(lapply(seq_along(featureSamples), function(x) {\n      featureSamples[[x]]$badWords\n    }))\n    featureTables$badWords  <- as.data.frame(table(allBadWords))\n    if (nrow(featureTables$badWords) > 0) {\n      featureTables$badWords$category  <- 'Corpus'\n    }  \n    \n    featureVector$corrections  <- \tsum(featureMatrix$corrections)\n    allcorrections <- unlist(lapply(seq_along(featureSamples), function(x) {\n      featureSamples[[x]]$corrections\n    }))\n    featureTables$corrections  <- as.data.frame(table(allcorrections))\n    if (nrow(featureTables$corrections) > 0) {\n      featureTables$corrections$category  <- 'Corpus'\n    }  \n    \n  }\n  featureVector <- as.data.frame(featureVector)\n  \n  totals <- list(featureVector = featureVector,\n                 featureTables = featureTables,\n                 wordLengths = wordLengths)\n  return(totals)\n  \n}\n#==============================================================================#\n#                              analyzeData                                     #\n#==============================================================================#\n#'  analyzeData\n#' \n#' This function takes as its parameter, meta data initiates a series of \n#' vectorized calls to a function that analyzes each file and returns a list \n#' of feature measures and feature samples.  The feature measures are combined \n#' into a single data frame, summary corpus level feature measures are calculated \n#' and the list, containing the feature measure data frame and the selected \n#' samples is returned to the calling environment.\n#' \n#' @param type - 'full' analysis or 'fast' (abbreviated) analysis\n#' @param korpus - the meta data for the corpora to be analyzed\n#' @param registers - the meta data for the registers to be analyzed\n#' @param reference - the meta data for the reference files which contain\n#'                    lists of profane words, abbreviations, emoticons\n#'                    and contractions.\n#' @param directories - the directory structure \n#' @param regex - the regex patterns\n#' @return analysis - a list containing a matrix of feature counts and \n#'          measures, and a list of selected sample features from each document.\n#' @author John James\n#' @export\nanalyzeData <- function(type, korpus, registers, reference, directories, regex) {\n  \n  startTime <- Sys.time()\n  \n  message(paste(\"\\nConducting a Analysis of \"), korpus$corpusName, \" at \",  startTime)\n  \n  # Load reference data and address regex patterns\n  referenceData <- loadReferenceData(reference)\n  \n  # Inspect Individual Corpus Files\n  featureVectors <- lapply(seq_along(registers),function(x) {\n    getFeatures(type, korpus, registers, x, regex, referenceData)})\n  \n  # Combine vectors into feature matrix\n  featureMatrix <- rbindlist(lapply(seq_along(featureVectors), function(x) {\n    featureVectors[[x]]$featureVector}))\n  \n  # Combine feature samples into list\n  featureSamples <- lapply(seq_along(featureVectors), function(x) {\n    featureVectors[[x]]$featureSamples})\n  \n  # Combine word length distributions into list of data frames\n  wordLengths <- lapply(seq_along(featureVectors), function(x) {\n    featureVectors[[x]]$wordLengths})\n  \n  # Combine feature tables into list of data frames\n  featureTables <- lapply(seq_along(featureVectors), function(x) {\n    featureVectors[[x]]$featureTables})\n  \n  # Get totals for corpus\n  totals <- summarizeAnalysis(type, korpus, registers, featureMatrix, featureSamples, regex)\n  featureMatrix <- rbind(featureMatrix, totals$featureVector)\n  totalWordLengths <- totals$wordLengths\n  totalFeatureTables <- totals$featureTables\n  \n  # Format Summary Data\n  metaData <- list(corpus = korpus$corpusName,\n                   objName = korpus$objName,\n                   fileName = korpus$fileName,\n                   analysisDate = Sys.time()) \n  \n  analysis <- list(metaData = metaData,\n                   featureMatrix = featureMatrix,\n                   featureSamples = featureSamples,\n                   featureTables = featureTables,\n                   totalWordLengths = totalWordLengths,\n                   totalFeatureTables = totalFeatureTables)\n  \n  # Save Results\n  output <- list()\n  output$directory <- directories$analysisDir\n  output$fileName  <- paste0(sub('\\\\..*', '', paste0(type, '-analysis-of-')),\n                             korpus$fileName, \n                             format(Sys.time(),'_%Y%m%d_%H%M%S'), '.Rdata')\n  output$objName   <- paste0('analysis', korpus$objName)\n  output$data  <- analysis\n  saveObject(output)\n  \n  # Log Results\n  logR('analyzeData', startTime, output$directory, output$fileName)\n  \n  # Alert User\n  endTime <- Sys.time()\n  message(paste('Analysis Complete at', endTime))\n  message(paste('Elapsed time is', round(difftime(endTime, startTime,  units = 'auto'), 2)))\n  \n  return(analysis)\n}\n## ---- end\n#rawCorpusAnalysis <- analyzeData('full', schema$corpora$raw, schema$referenceFiles, schema$directories, schema$regexPatterns)\n",
    "created" : 1494909268807.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "4255389979",
    "id" : "6C4FE61C",
    "lastKnownWriteTime" : 1494858203,
    "last_content_update" : 1494858203,
    "path" : "~/Data Science/Data Science Projects/PredictifyR-1.0/src/A2.analyzeData.R",
    "project_path" : "src/A2.analyzeData.R",
    "properties" : {
    },
    "relative_order" : 5,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}
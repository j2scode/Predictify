{
    "collab_server" : "",
    "contents" : "## ---- design_pilot\n#------------------------------------------------------------------------------#\n#                              designPilot                                     #\n#------------------------------------------------------------------------------#\n#'  designPilot \n#' \n#' This function takes the vocabulary and feature based estimates, the \n#' sampling unit analysis, and the clean data analysis and produces a \n#' list containing a data frame, comparing the two estimates and a data frame \n#' summaring the chosen estimate as the pilot corpus.\n#' \n#' @param vocabularyEstimate - the vocabulary-based estimate\n#' @param featureEstimate - the lexical feature based estimate \n#' @param samplingUnit - the sampling unit \n#' @param analysis - the clean data analysis\n#' @return alpha - the corpus design for alpha training set.\n#' @author John James\n#' @export\ndesignPilot <- function(vocabularyEstimate, featureEstimate, \n                        samplingUnit, analysis) {\n  \n  startTime <- Sys.time()\n  \n  message(paste('\\nDesigning alpha corpus at', startTime))\n  \n  #---------------------------------------------------------------------------#\n  #                  Compute Estimated Pilot Corpus Size                      #\n  #---------------------------------------------------------------------------#\n  register <- c(as.character(featureEstimate$Register[1:3]), 'Corpus')\n  hcCorpus <- analysis$featureMatrix$tokens\n  diversity <- as.vector(t(vocabularyEstimate[1:4,3]))\n  syntactic <- round(featureEstimate$'Sample Size'[1:4], 0)\n  tokens <- pmax(diversity, syntactic, na.rm = TRUE)\n  sentences <-round(tokens[1:3] / analysis$featureMatrix$wordsPerSent[1:3], 0)\n  sentences <- c(sentences, sum(sentences))              \n  pctTotal <- round(tokens / analysis$featureMatrix$tokens[1:4] * 100, 0)\n  proportion <- round(tokens[1:3] / sum(tokens[1:3]) * 100, 0)\n  proportion <- c(proportion, 100)\n  \n\n  comparison <- data.frame(register = register, hcCorpus = hcCorpus,\n                       diversity = diversity,\n                       syntactic = syntactic, tokens = tokens,\n                       sentences = sentences, pctTotal = pctTotal,\n                       proportion = proportion)\n  \n  names(comparison) <- c('Register', 'HC Corpus', 'Diversity-Based Estimate',\n                     'Lexical Feature-Based Estimate', \n                     'Tokens', 'Sentences', '% Total', \n                     'Proportion')\n  #---------------------------------------------------------------------------#\n  #                  Compute Extrapolated Pilot Corpus Size                   #\n  #---------------------------------------------------------------------------#\n  register <- c(as.character(featureEstimate$Register[1:3]), 'Corpus')\n  hcTokens <- analysis$featureMatrix$tokens\n  extrapolated <- pctTotal / pctTotal[4] * 5\n  extrapolatedTokens <- hcTokens * extrapolated / 100\n  extrapolatedSents <- ceiling(extrapolatedTokens / analysis$featureMatrix$wordsPerSent)\n  chunkSize <- rep(samplingUnit[[length(samplingUnit)]]$size, 4)\n  sentsPerChunk <- ceiling(samplingUnit[[length(samplingUnit)]]$size / \n                    analysis$featureMatrix$wordsPerSent)\n  chunks <- ceiling(extrapolatedSents / sentsPerChunk)\n  sampleSize <- chunks * sentsPerChunk\n  \n  pilot <- data.frame(register = register, hcTokens = hcTokens, \n                       extrapolated = extrapolated,\n                       extrapolatedTokens = extrapolatedTokens,\n                       extrapolatedSents = extrapolatedSents,\n                       chunkSize = chunkSize,\n                       sentsPerChunk = sentsPerChunk,\n                       chunks = chunks,\n                       sampleSize = sampleSize)\n  names(pilot) <- c('Register', 'HC Corpus (Tokens)',\n                     '% Total', 'Tokens', 'Sentences', \n                     'Chunk Size (Tokens)',\n                     'Sentences per Chunk', '# Chunks', \n                     'Sample Size (Sentences)')\n  \n  #---------------------------------------------------------------------------#\n  #           Compute Model Corpora (Train, Validation, Test) Sizes           #\n  #---------------------------------------------------------------------------#\n  # Compute training, validation and test set sizes\n  validationSet <- pilot$`Sample Size (Sentences)`[1:3]\n  testSet <- pilot$`Sample Size (Sentences)`[1:3]\n  multipliers <- c(2,4,7,10)\n  trainingSets <- tcrossprod(pilot$`Sample Size (Sentences)`[1:3], multipliers)\n  \n  # Format Corpus Design\n  corporaDesign <- as.data.frame(cbind(trainingSets, validationSet, testSet))\n  corporaDesign <- rbind(corporaDesign, colSums(corporaDesign))\n  names(corporaDesign) <- c('Alpha', 'Beta', 'Gamma', 'Delta', 'Validation', 'Test')\n  \n  # Correct for over allocation\n  hcCorpus <- comparison$`HC Corpus`\n  for (i in 1:(nrow(corporaDesign)-1)) {\n    for (j in 1:ncol(corporaDesign)) {\n      tokens <- corporaDesign[i,j] * round(analysis$featureMatrix$wordsPerSent[i],0)\n      if (tokens >= hcCorpus[i]) {\n        corporaDesign[i,j] = floor(hcCorpus[i] / chunkSize[i] * sentsPerChunk[i])\n      }\n    }\n  }\n  \n  # Reallocate shortfalls\n  blogTwitterTotal <- comparison$Proportion[1]  + comparison$Proportion[3]\n  blogProportion <- comparison$Proportion[1] / blogTwitterTotal\n  totals <- colSums(corporaDesign[1:3,])\n  shortfall <- corporaDesign[4,] - totals\n  shortfallBlogs <- shortfall * blogProportion\n  shortfallTwitter <- shortfall - shortfallBlogs\n  blogAdjustment <- floor(shortfallBlogs /  sentsPerChunk[1] * sentsPerChunk[1])\n  twitAdjustment <- floor(shortfallTwitter / sentsPerChunk[3] * sentsPerChunk[3])\n  corporaDesign[1,3] <- corporaDesign[1,3] + blogAdjustment$Gamma\n  corporaDesign[1,3] <- corporaDesign[1,3] + blogAdjustment$Delta\n  corporaDesign[3,3] <- corporaDesign[3,3] + twitAdjustment$Gamma\n  corporaDesign[3,3] <- corporaDesign[3,3] + blogAdjustment$Delta\n  Registers <- c('Blogs Register', 'News Register', 'Twitter Register', 'Corpus')\n  corporaDesign <- cbind(Registers, corporaDesign)\n  \n  \n  # Format results\n  design <- list(\n    comparison = comparison,\n    pilot = pilot,\n    corporaDesign = corporaDesign\n  )\n  \n  # Save Analysis\n  output <- list()\n  output$directory <- directories$analysisDir\n  output$fileName  <- paste0(sub('\\\\..*', '', paste0('')),\n                             'pilot-corpus-design',\n                             format(Sys.time(),'_%Y%m%d_%H%M%S'), '.Rdata')\n  output$objName   <- 'pilotDesign'\n  output$data  <- design\n  saveObject(output)\n  \n  # Log Results\n  logR('pilotDesign', startTime, output$directory, output$fileName)\n  \n  # Alert User\n  endTime <- Sys.time()\n  message(paste('Pilot corpus designed at', endTime))\n  message(paste('Elapsed time is', round(difftime(endTime, startTime,  units = 'auto'), 2)))\n  \n  return(design)\n}\n## ---- end\n#css <- estimatefeatureEstimate(corpora$clean, posTags, directories)",
    "created" : 1494917577564.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3728564439",
    "id" : "3D75FE74",
    "lastKnownWriteTime" : 1494925983,
    "last_content_update" : 1494925983899,
    "path" : "~/Data Science/Data Science Projects/PredictifyR-1.0/src/C0.designPilot.R",
    "project_path" : "src/C0.designPilot.R",
    "properties" : {
    },
    "relative_order" : 10,
    "source_on_save" : true,
    "source_window" : "",
    "type" : "r_source"
}
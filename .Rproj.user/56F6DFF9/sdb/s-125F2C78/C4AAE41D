{
    "collab_server" : "",
    "contents" : "## ---- mkn_evaluate\n#==============================================================================#\n#                                 mknEstimate                                  #\n#==============================================================================#\n#'  mknEstimate\n#' \n#' This function takes as it parameter, the language model to evaluate and\n#' the meta data for the training and test sets, the number of sentences from\n#' the test set to evaluate and the project directory structure and returns \n#' a data frame containing model size data, processing time, and perplexity\n#' scores. \n#' \n#' @param mkn - the meta data for the language model\n#' @param training the meta data for the training data \n#' @param test - the meta data for the validation or test corpus\n#' @param sents - the number of sentences from the test data to evaluate, \n#'                NULL = all sentences\n#' @param directories - the project directory structure\n#' @return estimates - the probability estimates for each word in the test data\n#' @author John James\n#' @export\nmknEstimate <- function(mkn, training, test, sents = NULL, directories) {\n  \n  memory.limit(20000)\n  startTime <- Sys.time()\n  message(paste(\"\\nEvaluating performance on \", test$fileDesc, 'at', startTime))\n  \n  \n  # Function that performs the quadgram probability estimate \n  score <- function(quadgram) {\n      \n    # Calculate unigram probability, p1\n    s <- quadgram[4]\n    n1p <- model[[1]][ nGram == s][,c(cKN, norm)]\n    p1 <- as.numeric(n1p[1] / n1p[2]) + 1 / V\n    \n    # Calculate bigram probability, p2\n    sfx <- paste0(quadgram[3:4], collapse = ' ')\n    cntx <- quadgram[3]\n    alpha <- max(0,model[[2]][ nGram == sfx][, alpha])\n    lambda <- model[[1]][ nGram == cntx][, lambda]\n    if (length(lambda) == 0) {\n      lambda <- 0.8 * as.numeric(discounts[2,4] / model[[1]][, norm])\n    }\n    p2 <- alpha + lambda * p1\n    \n    # Calculate trigram probability, p3\n    sfx <- paste0(quadgram[2:4], collapse = ' ')\n    cntx <- paste0(quadgram[2:3], collapse = ' ')\n    alpha <- max(0,model[[3]][ nGram == sfx][, alpha])\n    lambda <- model[[2]][ nGram == cntx][, lambda]\n    if (length(lambda) == 0) {\n      lambda <- 0.8 * as.numeric(discounts[3,4] / model[[2]][, norm])\n    }\n    p3 <- alpha + lambda * p2\n    \n    # Calculate quadgram probability, p4\n    sfx <- paste0(quadgram[1:4], collapse = ' ')\n    cntx <- paste0(quadgram[1:3], collapse = ' ')\n    alpha <- max(0,model[[4]][ nGram == sfx][, alpha])\n    lambda <- model[[3]][ nGram == cntx][, lambda]\n    if (length(lambda) == 0) {\n      lambda <- 0.8 * as.numeric(discounts[4,4])\n    }\n    p4 <- alpha + lambda * p3\n    \n    return(log(p4))\n  }\n  \n  scoreSentence <- function(sentence) {\n    tokens <- unlist(quanteda::tokenize(sentence, what = 'word'))\n    rbindlist(lapply(seq_along(tokens[1:(length(tokens)-4)]), function(x) {\n      nGram <- list()\n      nGram$quadgram <- paste0(tokens[x:(x+4)], collapse = ' ')\n      nGram$logProb <- score(tokens[x:(x+4)])\n      nGram\n    }))\n  }\n  \n  message('...loading training data')\n  train <- readFile(training)\n  df <- quanteda::dfm(train, tolower = FALSE, remove = 'BOS')\n  V <- length(featnames(df))\n\n  message(paste('...loading language model'))\n  model <- lapply(seq_along(mkn$counts), function(x) {\n    loadObject(mkn$counts[[x]])\n  })\n\n  message(paste('...loading test data'))\n  document <- readFile(test)\n  if (!(is.null(sents))) {\n    document <- sampleData(document, numChunks = sents, chunkSize = 1, format = 'v')\n  }\n  # Compute number of sentences and tokens w/o 'BOS'\n  M <- length(document) # M = number of sentences\n\n  message('...loading discounts')\n  discounts <- loadObject(mkn$discounts)\n  \n  message('...evaluating sentence probabilities')\n  scores <- rbindlist(lapply(seq_along(document), function(x) {\n    s <- scoreSentence(document[x])\n    if (x %in% c(100, 200, 500, 1000, 5000, 10000, 20000)) { \n      elapsed <- round(difftime(Sys.time(), startTime,  units = 'mins'))\n      elapsed <- as.numeric(elapsed) + 1\n      rate <- x / elapsed \n      remaining <- length(document) - x\n      timeMin <- round(remaining / rate, digits = 1)\n      timeHrs <- round(timeMin / 60, digits = 1)\n      message(paste('......',x,'out of',length(document), 'sentences processed in', \n                    elapsed, 'minutes.', timeMin,'minutes remaining (', timeHrs, 'hours)'))\n    }\n    s\n  }))\n  \n\n  # Compute perplexity\n  N <- length(scores$logProb)\n  pp <- 2^-(sum(scores$logProb) / N)\n  \n  # Note the time\n  endTime <- Sys.time()\n  duration <- round(difftime(endTime, startTime,  units = 'auto'), 2)\n  \n  # Summarize results\n  evaluation <- list(\n    summary = list(\n      date = startTime,\n      end = endTime,\n      trainingSet = training$corpusName,\n      size = object.size(model),\n      duration = duration,\n      testSents = M,\n      words = N,\n      perplexity = pp\n    ),\n    detail = scores\n  )\n  \n  # Save Analysis\n  output <- list()\n  output$directory <- directories$analysisDir\n  output$fileName  <- paste0(sub('\\\\..*', '', paste0('')),\n                             paste0(training$fileName,'-estimates'),\n                             format(Sys.time(),'_%Y%m%d_%H%M%S'), '.Rdata')\n  output$objName   <- 'evaluation'\n  output$data  <- evaluation\n  saveObject(output)\n  \n  # Log and Return results\n  logR('mknEstimate', startTime, test$directory, test$fileName)\n\n  # Alert User\n  message(paste('MKN model evaluated at', endTime))\n  message(paste('Elapsed time is', duration))\n\n  return(evaluation)\n}\n## ---- end\n#est <- mknEstimate(lm$mkn, corpora$training$processed$text$quadgram,  corpora$validation$processed$text$quadgram, directories)",
    "created" : 1495825553390.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "161946913",
    "id" : "C4AAE41D",
    "lastKnownWriteTime" : 1495837926,
    "last_content_update" : 1495837926088,
    "path" : "~/Data Science/Data Science Projects/PredictifyR-1.0/src/LM09.mknEstimate.R",
    "project_path" : "src/LM09.mknEstimate.R",
    "properties" : {
    },
    "relative_order" : 3,
    "source_on_save" : true,
    "source_window" : "",
    "type" : "r_source"
}